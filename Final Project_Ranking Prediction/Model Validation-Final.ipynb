{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fb656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from IPython import display\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, StratifiedGroupKFold, cross_val_score, GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.metrics import ndcg_score as nd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import optuna\n",
    "\n",
    "import pytorchltr\n",
    "from pytorchltr.loss import LambdaARPLoss1, LambdaARPLoss2, LambdaNDCGLoss1, LambdaNDCGLoss2, PairwiseDCGHingeLoss, PairwiseHingeLoss, PairwiseLogisticLoss\n",
    "from pytorchltr.evaluation import ndcg\n",
    "\n",
    "import operator\n",
    "import statistics\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ebbb8",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50ba63",
   "metadata": {},
   "source": [
    "Read pointwise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0c159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_data = pd.read_excel('processed_data.xlsx') # the dataset is mostly retrieved manually by Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75119e58",
   "metadata": {},
   "source": [
    "Drop rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6edf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_data = pointwise_data[pointwise_data.keyword != 'hastings property for sale']\n",
    "pointwise_data = pointwise_data[pointwise_data.keyword != 'property for rent hertfordshire']\n",
    "pointwise_data = pointwise_data[pointwise_data.keyword != 'property for sale bexhill']\n",
    "pointwise_data = pointwise_data[pointwise_data.keyword != 'property for sale ceredigion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9839ee7",
   "metadata": {},
   "source": [
    "Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a343df18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_date', 'keyword', 'gg_srp', 'result_type', 'srp_title',\n",
       "       'LP_link', 'domain_name', 'rank', 'srp_title_length',\n",
       "       'keyword_in_srp_title', 'any_kw_in_srp_title', 'domain_age',\n",
       "       'LP_pagesize', 'total_links', 'internal_links', 'external_links',\n",
       "       'meta_tag', 'is_secure', 'content_download_time', 'response_time',\n",
       "       'content_size', 'html_size', 'overall_score', 'speed_index',\n",
       "       'first_meaningful_paint_index', 'first_contentful_paint_index',\n",
       "       'time_to_interactive_index', 'total_blocking_time_index',\n",
       "       'largest_contentful_paint_index', 'first_input_delay_index',\n",
       "       'cumulative_layout_shift_index', 'uses_long_cache_ttl_index',\n",
       "       'unused_javascript_index', 'speed', 'time_to_interactive',\n",
       "       'total_byte_weight', 'largest_contentful_paint', 'first_input_delay',\n",
       "       'cumulative_layout_shift', 'first_meaningful_paint',\n",
       "       'first_contentful_paint', 'total_blocking_time', 'server_response_time',\n",
       "       'numTasks', 'maxRtt', 'mainDocumentTransferSize', 'numScripts',\n",
       "       'totalTaskTime', 'numTasksOver500ms', 'numTasksOver100ms',\n",
       "       'numTasksOver50ms', 'numTasksOver25ms', 'numTasksOver10ms',\n",
       "       'numRequests', 'numStylesheets', 'uses_long_cache_ttl',\n",
       "       'unused_javascript', 'amount_of_text', 'total_heading',\n",
       "       'total_heading_length', 'keyword_in_total_heading', 'heading1',\n",
       "       'heading1_length', 'keyword_in_heading1', 'heading2', 'heading2_length',\n",
       "       'keyword_in_heading2', 'heading3', 'heading3_length',\n",
       "       'keyword_in_heading3', 'total_img', 'keyword_in_img_alt',\n",
       "       'keyword_in_anchor', 'any_kw_in_anchor', 'keyword_in_footer',\n",
       "       'any_kw_in_footer', 'keyword_in_url', 'any_kw_in_url',\n",
       "       'keyword_in_body', 'meta_length', 'keyword_in_meta',\n",
       "       'is_title_tag_used', 'domain_pagerank', 'selfcalc_pagerank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data columns to select the columns to normalize\n",
    "pointwise_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43abb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>gg_srp</th>\n",
       "      <th>result_type</th>\n",
       "      <th>srp_title</th>\n",
       "      <th>LP_link</th>\n",
       "      <th>domain_name</th>\n",
       "      <th>rank</th>\n",
       "      <th>srp_title_length</th>\n",
       "      <th>keyword_in_srp_title</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword_in_footer</th>\n",
       "      <th>any_kw_in_footer</th>\n",
       "      <th>keyword_in_url</th>\n",
       "      <th>any_kw_in_url</th>\n",
       "      <th>keyword_in_body</th>\n",
       "      <th>meta_length</th>\n",
       "      <th>keyword_in_meta</th>\n",
       "      <th>is_title_tag_used</th>\n",
       "      <th>domain_pagerank</th>\n",
       "      <th>selfcalc_pagerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230503</td>\n",
       "      <td>1 bedroom house for rent</td>\n",
       "      <td>https://www.google.com/search?q=1%20bedroom%20...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>1 Bedroom Houses To Rent in Greater London</td>\n",
       "      <td>https://www.rightmove.co.uk/property-to-rent/L...</td>\n",
       "      <td>https://www.rightmove.co.uk</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260214</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.276974</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>1.012955</td>\n",
       "      <td>-0.008267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230503</td>\n",
       "      <td>1 bedroom house for rent</td>\n",
       "      <td>https://www.google.com/search?q=1%20bedroom%20...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>1 Bedroom houses to rent in London</td>\n",
       "      <td>https://www.zoopla.co.uk/to-rent/houses/1-bedr...</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.509871</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.025236</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>0.070154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230503</td>\n",
       "      <td>1 bedroom house for rent</td>\n",
       "      <td>https://www.google.com/search?q=1%20bedroom%20...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>1 Bedroom houses to rent in North London</td>\n",
       "      <td>https://www.zoopla.co.uk/to-rent/houses/1-bedr...</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>3</td>\n",
       "      <td>0.067693</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.029841</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>-0.058558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230503</td>\n",
       "      <td>1 bedroom house for rent</td>\n",
       "      <td>https://www.google.com/search?q=1%20bedroom%20...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>Search 1+ Bed Houses To Rent In London</td>\n",
       "      <td>https://www.onthemarket.com/to-rent/1-bed-hous...</td>\n",
       "      <td>https://www.onthemarket.com</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.124828</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>2.699498</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.660976</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.324308</td>\n",
       "      <td>-0.075753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230503</td>\n",
       "      <td>1 bedroom house for rent</td>\n",
       "      <td>https://www.google.com/search?q=1%20bedroom%20...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>1 Bedroom Flats and Houses to Rent in London</td>\n",
       "      <td>https://www.gumtree.com/flats-houses/property-...</td>\n",
       "      <td>https://www.gumtree.com</td>\n",
       "      <td>5</td>\n",
       "      <td>0.452735</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.338885</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>-0.094151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8313</th>\n",
       "      <td>20230505</td>\n",
       "      <td>zoopla property value</td>\n",
       "      <td>https://www.google.com/search?q=zoopla%20prope...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>House prices in London - sold prices and estim...</td>\n",
       "      <td>https://www.zoopla.co.uk/house-prices/london/</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>6</td>\n",
       "      <td>1.030299</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>-0.064560</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>0.420911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8314</th>\n",
       "      <td>20230505</td>\n",
       "      <td>zoopla property value</td>\n",
       "      <td>https://www.google.com/search?q=zoopla%20prope...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>Zoopla &gt; Search Property to Buy, Rent, House P...</td>\n",
       "      <td>https://www.zoopla.co.uk/</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>7</td>\n",
       "      <td>2.281686</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>0.137802</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>8.426398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8315</th>\n",
       "      <td>20230505</td>\n",
       "      <td>zoopla property value</td>\n",
       "      <td>https://www.google.com/search?q=zoopla%20prope...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>House prices in South London, London</td>\n",
       "      <td>https://www.zoopla.co.uk/house-prices/south-lo...</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.317349</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>-0.060979</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>-0.046721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8316</th>\n",
       "      <td>20230505</td>\n",
       "      <td>zoopla property value</td>\n",
       "      <td>https://www.google.com/search?q=zoopla%20prope...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>House prices in Street - sold prices and estim...</td>\n",
       "      <td>https://www.zoopla.co.uk/house-prices/street/</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>9</td>\n",
       "      <td>1.030299</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>-0.412491</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>-0.058509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>20230505</td>\n",
       "      <td>zoopla property value</td>\n",
       "      <td>https://www.google.com/search?q=zoopla%20prope...</td>\n",
       "      <td>Organic</td>\n",
       "      <td>House prices in Central London, London</td>\n",
       "      <td>https://www.zoopla.co.uk/house-prices/central-...</td>\n",
       "      <td>https://www.zoopla.co.uk</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.124828</td>\n",
       "      <td>-0.219636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036475</td>\n",
       "      <td>-0.370439</td>\n",
       "      <td>-0.032989</td>\n",
       "      <td>0.173155</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>-0.407886</td>\n",
       "      <td>-0.147237</td>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.768304</td>\n",
       "      <td>-0.058509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8279 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_date                   keyword  \\\n",
       "0       20230503  1 bedroom house for rent   \n",
       "1       20230503  1 bedroom house for rent   \n",
       "2       20230503  1 bedroom house for rent   \n",
       "3       20230503  1 bedroom house for rent   \n",
       "4       20230503  1 bedroom house for rent   \n",
       "...          ...                       ...   \n",
       "8313    20230505     zoopla property value   \n",
       "8314    20230505     zoopla property value   \n",
       "8315    20230505     zoopla property value   \n",
       "8316    20230505     zoopla property value   \n",
       "8317    20230505     zoopla property value   \n",
       "\n",
       "                                                 gg_srp result_type  \\\n",
       "0     https://www.google.com/search?q=1%20bedroom%20...     Organic   \n",
       "1     https://www.google.com/search?q=1%20bedroom%20...     Organic   \n",
       "2     https://www.google.com/search?q=1%20bedroom%20...     Organic   \n",
       "3     https://www.google.com/search?q=1%20bedroom%20...     Organic   \n",
       "4     https://www.google.com/search?q=1%20bedroom%20...     Organic   \n",
       "...                                                 ...         ...   \n",
       "8313  https://www.google.com/search?q=zoopla%20prope...     Organic   \n",
       "8314  https://www.google.com/search?q=zoopla%20prope...     Organic   \n",
       "8315  https://www.google.com/search?q=zoopla%20prope...     Organic   \n",
       "8316  https://www.google.com/search?q=zoopla%20prope...     Organic   \n",
       "8317  https://www.google.com/search?q=zoopla%20prope...     Organic   \n",
       "\n",
       "                                              srp_title  \\\n",
       "0            1 Bedroom Houses To Rent in Greater London   \n",
       "1                    1 Bedroom houses to rent in London   \n",
       "2              1 Bedroom houses to rent in North London   \n",
       "3                Search 1+ Bed Houses To Rent In London   \n",
       "4          1 Bedroom Flats and Houses to Rent in London   \n",
       "...                                                 ...   \n",
       "8313  House prices in London - sold prices and estim...   \n",
       "8314  Zoopla > Search Property to Buy, Rent, House P...   \n",
       "8315               House prices in South London, London   \n",
       "8316  House prices in Street - sold prices and estim...   \n",
       "8317             House prices in Central London, London   \n",
       "\n",
       "                                                LP_link  \\\n",
       "0     https://www.rightmove.co.uk/property-to-rent/L...   \n",
       "1     https://www.zoopla.co.uk/to-rent/houses/1-bedr...   \n",
       "2     https://www.zoopla.co.uk/to-rent/houses/1-bedr...   \n",
       "3     https://www.onthemarket.com/to-rent/1-bed-hous...   \n",
       "4     https://www.gumtree.com/flats-houses/property-...   \n",
       "...                                                 ...   \n",
       "8313      https://www.zoopla.co.uk/house-prices/london/   \n",
       "8314                          https://www.zoopla.co.uk/   \n",
       "8315  https://www.zoopla.co.uk/house-prices/south-lo...   \n",
       "8316      https://www.zoopla.co.uk/house-prices/street/   \n",
       "8317  https://www.zoopla.co.uk/house-prices/central-...   \n",
       "\n",
       "                      domain_name  rank  srp_title_length  \\\n",
       "0     https://www.rightmove.co.uk     1          0.260214   \n",
       "1        https://www.zoopla.co.uk     2         -0.509871   \n",
       "2        https://www.zoopla.co.uk     3          0.067693   \n",
       "3     https://www.onthemarket.com     4         -0.124828   \n",
       "4         https://www.gumtree.com     5          0.452735   \n",
       "...                           ...   ...               ...   \n",
       "8313     https://www.zoopla.co.uk     6          1.030299   \n",
       "8314     https://www.zoopla.co.uk     7          2.281686   \n",
       "8315     https://www.zoopla.co.uk     8         -0.317349   \n",
       "8316     https://www.zoopla.co.uk     9          1.030299   \n",
       "8317     https://www.zoopla.co.uk    10         -0.124828   \n",
       "\n",
       "      keyword_in_srp_title  ...  keyword_in_footer  any_kw_in_footer  \\\n",
       "0                -0.219636  ...          -0.036475         -0.370439   \n",
       "1                -0.219636  ...          -0.036475         -0.370439   \n",
       "2                -0.219636  ...          -0.036475         -0.370439   \n",
       "3                -0.219636  ...          -0.036475          2.699498   \n",
       "4                -0.219636  ...          -0.036475         -0.370439   \n",
       "...                    ...  ...                ...               ...   \n",
       "8313             -0.219636  ...          -0.036475         -0.370439   \n",
       "8314             -0.219636  ...          -0.036475         -0.370439   \n",
       "8315             -0.219636  ...          -0.036475         -0.370439   \n",
       "8316             -0.219636  ...          -0.036475         -0.370439   \n",
       "8317             -0.219636  ...          -0.036475         -0.370439   \n",
       "\n",
       "      keyword_in_url  any_kw_in_url  keyword_in_body  meta_length  \\\n",
       "0          -0.032989       0.173155        -0.084735     0.276974   \n",
       "1          -0.032989       0.173155        -0.084735     0.025236   \n",
       "2          -0.032989       0.173155        -0.084735     0.029841   \n",
       "3          -0.032989       0.173155        -0.084735     0.660976   \n",
       "4          -0.032989       0.173155        -0.084735     0.338885   \n",
       "...              ...            ...              ...          ...   \n",
       "8313       -0.032989       0.173155        -0.084735    -0.064560   \n",
       "8314       -0.032989       0.173155        -0.084735     0.137802   \n",
       "8315       -0.032989       0.173155        -0.084735    -0.060979   \n",
       "8316       -0.032989       0.173155        -0.084735    -0.412491   \n",
       "8317       -0.032989       0.173155        -0.084735    -0.407886   \n",
       "\n",
       "      keyword_in_meta  is_title_tag_used  domain_pagerank  selfcalc_pagerank  \n",
       "0           -0.147237           0.032989         1.012955          -0.008267  \n",
       "1           -0.147237           0.032989         0.768304           0.070154  \n",
       "2           -0.147237           0.032989         0.768304          -0.058558  \n",
       "3           -0.147237           0.032989         0.324308          -0.075753  \n",
       "4           -0.147237           0.032989         0.722998          -0.094151  \n",
       "...               ...                ...              ...                ...  \n",
       "8313        -0.147237           0.032989         0.768304           0.420911  \n",
       "8314        -0.147237           0.032989         0.768304           8.426398  \n",
       "8315        -0.147237           0.032989         0.768304          -0.046721  \n",
       "8316        -0.147237           0.032989         0.768304          -0.058509  \n",
       "8317        -0.147237           0.032989         0.768304          -0.058509  \n",
       "\n",
       "[8279 rows x 84 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the suitable columns\n",
    "scaler = StandardScaler() #Initialize z-score normalization\n",
    "columns_to_normalize = ['srp_title_length',\n",
    "       'keyword_in_srp_title', 'any_kw_in_srp_title', 'domain_age',\n",
    "       'LP_pagesize', 'total_links', 'internal_links', 'external_links',\n",
    "       'meta_tag', 'is_secure', 'content_download_time', 'response_time',\n",
    "       'content_size', 'html_size', 'overall_score', 'speed_index',\n",
    "       'first_meaningful_paint_index', 'first_contentful_paint_index',\n",
    "       'time_to_interactive_index', 'total_blocking_time_index',\n",
    "       'largest_contentful_paint_index', 'first_input_delay_index',\n",
    "       'cumulative_layout_shift_index', 'uses_long_cache_ttl_index',\n",
    "       'unused_javascript_index', 'speed', 'time_to_interactive',\n",
    "       'total_byte_weight', 'largest_contentful_paint', 'first_input_delay',\n",
    "       'cumulative_layout_shift', 'first_meaningful_paint',\n",
    "       'first_contentful_paint', 'total_blocking_time', 'server_response_time',\n",
    "       'numTasks', 'maxRtt', 'mainDocumentTransferSize', 'numScripts',\n",
    "       'totalTaskTime', 'numTasksOver500ms', 'numTasksOver100ms',\n",
    "       'numTasksOver50ms', 'numTasksOver25ms', 'numTasksOver10ms',\n",
    "       'numRequests', 'numStylesheets', 'uses_long_cache_ttl',\n",
    "       'unused_javascript', 'amount_of_text', 'total_heading',\n",
    "       'total_heading_length', 'keyword_in_total_heading', 'heading1',\n",
    "       'heading1_length', 'keyword_in_heading1', 'heading2', 'heading2_length',\n",
    "       'keyword_in_heading2', 'heading3', 'heading3_length',\n",
    "       'keyword_in_heading3', 'total_img', 'keyword_in_img_alt',\n",
    "       'keyword_in_anchor', 'any_kw_in_anchor', 'keyword_in_footer',\n",
    "       'any_kw_in_footer', 'keyword_in_url', 'any_kw_in_url',\n",
    "       'keyword_in_body', 'meta_length', 'keyword_in_meta',\n",
    "       'is_title_tag_used', 'domain_pagerank', 'selfcalc_pagerank']\n",
    "pointwise_data[columns_to_normalize] = scaler.fit_transform(pointwise_data[columns_to_normalize]) #Normalize\n",
    "\n",
    "#Display the standardized data\n",
    "pointwise_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac4b05",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18488d",
   "metadata": {},
   "source": [
    "## Pointwise Learning-To-Rank Model\n",
    "- Loss function = MSELoss\n",
    "- Using XGBoost Model for ranking task\n",
    "- Evaluation metric: NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6615cc7",
   "metadata": {},
   "source": [
    "Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3076a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        target = self.targets[index]\n",
    "        return feature, target\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear: # by checking the type we can init different layers in different ways\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "def data(data, columns_to_normalize):\n",
    "    grouped_data = data.groupby('keyword')\n",
    "    query_features = []  # List to store query features\n",
    "    query_urls = []  # List to store URLs for each query\n",
    "    query_ranks = []  # List to store ranks for each URL\n",
    "\n",
    "    for query, group in grouped_data:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "\n",
    "        query_urls.extend(urls)\n",
    "        query_ranks.extend(ranks)\n",
    "        query_features.extend(features)\n",
    "       \n",
    "    input_data = torch.tensor(query_features, dtype=torch.float32)\n",
    "    target_data = torch.tensor(query_ranks, dtype=torch.long)\n",
    "    \n",
    "    return input_data, target_data\n",
    "\n",
    "def train_ltr(net, train_iter, test_iter, traingroup, testgroup, loss, num_epochs, optimizer):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "        \n",
    "    loss_ = []\n",
    "    train_ndcg = []\n",
    "    test_ndcg = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        total_ndcg = 0\n",
    "        total_test_ndcg = 0\n",
    "        test_batches = 0\n",
    "        out_train = torch.tensor([])\n",
    "        out_test = torch.tensor([])\n",
    "        rank_train = torch.tensor([])\n",
    "        rank_test = torch.tensor([])\n",
    "        for i, v in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(i)\n",
    "            out_train = torch.cat((out_train, output), dim=0)\n",
    "            rank_train = torch.cat((rank_train, v), dim=0)\n",
    "            v=v.float().view(-1,1)\n",
    "            loss = l(output, v)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        out_train = torch.split(out_train, train_groups)\n",
    "        rank_train = torch.split(rank_train, train_groups)\n",
    "        for t1, t2 in zip(out_train, rank_train):\n",
    "            if len(t1)==1:\n",
    "                pass\n",
    "            else:\n",
    "                ndcg_score = nd(np.asarray([t2.tolist()]), np.asarray([t1.view(-1).tolist()])).mean()\n",
    "            total_ndcg += ndcg_score\n",
    "\n",
    "        for a, b in test_dataloader:\n",
    "            test_output = model(a)\n",
    "            out_test = torch.cat((out_test, test_output), dim=0)\n",
    "            rank_test = torch.cat((rank_test, b), dim=0)\n",
    "            test_batches += 1\n",
    "            \n",
    "        out_test = torch.split(out_test, test_groups)\n",
    "        rank_test = torch.split(rank_test, test_groups)\n",
    "        for t3, t4 in zip(out_test, rank_test):\n",
    "            if len(t3)==1:\n",
    "                pass\n",
    "            else:\n",
    "                test_ndcg_score = nd(np.asarray([t4.tolist()]), np.asarray([t3.view(-1).tolist()])).mean()\n",
    "            total_test_ndcg += test_ndcg_score\n",
    "\n",
    "        average_loss = total_loss/num_batches\n",
    "        average_ndcg = total_ndcg/len(train_groups)\n",
    "        average_test_ndcg = total_test_ndcg/len(test_groups)\n",
    "        \n",
    "        loss_.append(average_loss)\n",
    "        train_ndcg.append(average_ndcg)\n",
    "        test_ndcg.append(average_test_ndcg)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Train-NDCG: {average_ndcg}, Test-NDCG: {average_test_ndcg}\")\n",
    "    return loss_, train_ndcg, test_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288be66",
   "metadata": {},
   "source": [
    "Seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eed0b284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 6.851382741749575, Train-NDCG: 0.9199806376054878, Test-NDCG: 0.9191672668006446\n",
      "Epoch 2/100, Loss: 4.4804657555144765, Train-NDCG: 0.9303137579581954, Test-NDCG: 0.925346570146256\n",
      "Epoch 3/100, Loss: 3.933751363541193, Train-NDCG: 0.9366379040637169, Test-NDCG: 0.9216137989382397\n",
      "Epoch 4/100, Loss: 3.73891394711347, Train-NDCG: 0.941809677448353, Test-NDCG: 0.926323583543156\n",
      "Epoch 5/100, Loss: 3.6443136592130156, Train-NDCG: 0.9434941181063902, Test-NDCG: 0.9241746781042692\n",
      "Epoch 6/100, Loss: 3.329009004837073, Train-NDCG: 0.9470901836694132, Test-NDCG: 0.9259200323708844\n",
      "Epoch 7/100, Loss: 3.166568173734462, Train-NDCG: 0.9505355939518061, Test-NDCG: 0.9260998150727121\n",
      "Epoch 8/100, Loss: 3.0674015482266745, Train-NDCG: 0.9534439581948514, Test-NDCG: 0.9286845818331463\n",
      "Epoch 9/100, Loss: 2.9865080424840897, Train-NDCG: 0.9549913343772893, Test-NDCG: 0.9274858717016617\n",
      "Epoch 10/100, Loss: 2.8627739262177747, Train-NDCG: 0.9569837375384594, Test-NDCG: 0.9266287843257411\n",
      "Epoch 11/100, Loss: 2.7701909520487855, Train-NDCG: 0.9583549419374049, Test-NDCG: 0.9271846324524758\n",
      "Epoch 12/100, Loss: 2.71010985740141, Train-NDCG: 0.9599165880176002, Test-NDCG: 0.9261412160608307\n",
      "Epoch 13/100, Loss: 2.657536307322806, Train-NDCG: 0.9603918847686984, Test-NDCG: 0.9286178905420411\n",
      "Epoch 14/100, Loss: 2.6224026528821476, Train-NDCG: 0.9615060201211303, Test-NDCG: 0.9271476011348095\n",
      "Epoch 15/100, Loss: 2.641985236590611, Train-NDCG: 0.9617995648310939, Test-NDCG: 0.9282128935307555\n",
      "Epoch 16/100, Loss: 2.6595808957941864, Train-NDCG: 0.9620011106271534, Test-NDCG: 0.926419884670126\n",
      "Epoch 17/100, Loss: 2.4803263365070602, Train-NDCG: 0.9637977696643176, Test-NDCG: 0.9275833003648765\n",
      "Epoch 18/100, Loss: 2.459602647141558, Train-NDCG: 0.9641786279690847, Test-NDCG: 0.9288359881259148\n",
      "Epoch 19/100, Loss: 2.408229892164613, Train-NDCG: 0.9658839799334157, Test-NDCG: 0.9304985660158117\n",
      "Epoch 20/100, Loss: 2.309686555090734, Train-NDCG: 0.9671830813686317, Test-NDCG: 0.9266512596905007\n",
      "Epoch 21/100, Loss: 2.251061930100699, Train-NDCG: 0.9677992323762808, Test-NDCG: 0.9294242390384746\n",
      "Epoch 22/100, Loss: 2.224264030079335, Train-NDCG: 0.9691422641334652, Test-NDCG: 0.9263972616556344\n",
      "Epoch 23/100, Loss: 2.188822537300667, Train-NDCG: 0.9689831976924239, Test-NDCG: 0.9280584999702025\n",
      "Epoch 24/100, Loss: 2.1862592879268856, Train-NDCG: 0.9692121275628516, Test-NDCG: 0.9284323084639569\n",
      "Epoch 25/100, Loss: 2.2139061207068713, Train-NDCG: 0.969304266984922, Test-NDCG: 0.9280510283084761\n",
      "Epoch 26/100, Loss: 2.1921653440897018, Train-NDCG: 0.9701966923217786, Test-NDCG: 0.9290998476000447\n",
      "Epoch 27/100, Loss: 2.1128720461170456, Train-NDCG: 0.9703869828531805, Test-NDCG: 0.929243307925766\n",
      "Epoch 28/100, Loss: 2.0859938734683436, Train-NDCG: 0.9715614813467176, Test-NDCG: 0.9297172137134893\n",
      "Epoch 29/100, Loss: 2.058917499610767, Train-NDCG: 0.9719733382349737, Test-NDCG: 0.9303278275688687\n",
      "Epoch 30/100, Loss: 2.0197305645487735, Train-NDCG: 0.9721112050606887, Test-NDCG: 0.9290181490844034\n",
      "Epoch 31/100, Loss: 2.0016340198724167, Train-NDCG: 0.9720223524541262, Test-NDCG: 0.9242927564640725\n",
      "Epoch 32/100, Loss: 1.9926275005519103, Train-NDCG: 0.9722559972039704, Test-NDCG: 0.9284550727304158\n",
      "Epoch 33/100, Loss: 1.9359859119074931, Train-NDCG: 0.973528657540388, Test-NDCG: 0.9289154151685879\n",
      "Epoch 34/100, Loss: 1.9477998882820065, Train-NDCG: 0.9734198511693711, Test-NDCG: 0.9260619859429521\n",
      "Epoch 35/100, Loss: 1.907792899703634, Train-NDCG: 0.9736642905565589, Test-NDCG: 0.9274467021352995\n",
      "Epoch 36/100, Loss: 2.0441059614869133, Train-NDCG: 0.9719600306218026, Test-NDCG: 0.9287620068144907\n",
      "Epoch 37/100, Loss: 1.9204021836968437, Train-NDCG: 0.9741935242484818, Test-NDCG: 0.9257830656278501\n",
      "Epoch 38/100, Loss: 2.0377722625139256, Train-NDCG: 0.9734602182712861, Test-NDCG: 0.9276543062294289\n",
      "Epoch 39/100, Loss: 1.9460117224477915, Train-NDCG: 0.973600830828751, Test-NDCG: 0.9288155655739029\n",
      "Epoch 40/100, Loss: 1.9185259643816142, Train-NDCG: 0.9739750521090866, Test-NDCG: 0.9259736004955393\n",
      "Epoch 41/100, Loss: 1.9108205275691075, Train-NDCG: 0.9741423739175822, Test-NDCG: 0.9263468163529345\n",
      "Epoch 42/100, Loss: 1.8873051376878351, Train-NDCG: 0.9748611822691502, Test-NDCG: 0.9283831476621808\n",
      "Epoch 43/100, Loss: 1.8316329134187261, Train-NDCG: 0.9746809326737299, Test-NDCG: 0.927864265608006\n",
      "Epoch 44/100, Loss: 1.8372988234035634, Train-NDCG: 0.9759305470184486, Test-NDCG: 0.9290976218817834\n",
      "Epoch 45/100, Loss: 1.7967732053088106, Train-NDCG: 0.9758581414705169, Test-NDCG: 0.9305153562793834\n",
      "Epoch 46/100, Loss: 1.7716859118374073, Train-NDCG: 0.976245921764148, Test-NDCG: 0.9315944393656602\n",
      "Epoch 47/100, Loss: 1.7658687166617688, Train-NDCG: 0.9766396852670326, Test-NDCG: 0.9289086604306439\n",
      "Epoch 48/100, Loss: 1.7664989167869378, Train-NDCG: 0.975980350149838, Test-NDCG: 0.9279023421998144\n",
      "Epoch 49/100, Loss: 1.776974095058614, Train-NDCG: 0.9767281222702929, Test-NDCG: 0.9291122840926039\n",
      "Epoch 50/100, Loss: 1.7507274452110995, Train-NDCG: 0.9774049803961137, Test-NDCG: 0.9313557662201298\n",
      "Epoch 51/100, Loss: 1.7603513623637277, Train-NDCG: 0.9763472491702987, Test-NDCG: 0.9285027778151598\n",
      "Epoch 52/100, Loss: 1.7275901160738318, Train-NDCG: 0.9774017420653363, Test-NDCG: 0.9282513586597239\n",
      "Epoch 53/100, Loss: 1.709647427542486, Train-NDCG: 0.9773092396950903, Test-NDCG: 0.9321767852706083\n",
      "Epoch 54/100, Loss: 1.699969501742994, Train-NDCG: 0.9778556349908963, Test-NDCG: 0.9282265221071546\n",
      "Epoch 55/100, Loss: 1.7056098440347087, Train-NDCG: 0.9770247621413133, Test-NDCG: 0.9282881899720303\n",
      "Epoch 56/100, Loss: 1.6706232694180116, Train-NDCG: 0.9783400901589357, Test-NDCG: 0.928952488215917\n",
      "Epoch 57/100, Loss: 1.6667791680221395, Train-NDCG: 0.978904356012723, Test-NDCG: 0.9299602525395331\n",
      "Epoch 58/100, Loss: 1.7149237516709572, Train-NDCG: 0.9776248740029021, Test-NDCG: 0.9307306932021923\n",
      "Epoch 59/100, Loss: 1.6506549575622531, Train-NDCG: 0.9790088028007102, Test-NDCG: 0.9284719087064222\n",
      "Epoch 60/100, Loss: 1.6391768351485188, Train-NDCG: 0.9785767350773574, Test-NDCG: 0.9291505373751744\n",
      "Epoch 61/100, Loss: 1.7531325676734897, Train-NDCG: 0.9786023859732743, Test-NDCG: 0.9254684600478429\n",
      "Epoch 62/100, Loss: 1.7106850607959545, Train-NDCG: 0.9778709673537856, Test-NDCG: 0.9291303170696479\n",
      "Epoch 63/100, Loss: 1.7848293049899853, Train-NDCG: 0.9769221005475056, Test-NDCG: 0.9273358040050215\n",
      "Epoch 64/100, Loss: 1.7134310988268415, Train-NDCG: 0.9767215722215784, Test-NDCG: 0.9291023205702624\n",
      "Epoch 65/100, Loss: 1.6188515573740005, Train-NDCG: 0.9784843064093406, Test-NDCG: 0.9280859908063303\n",
      "Epoch 66/100, Loss: 1.5949629227572306, Train-NDCG: 0.97888506571997, Test-NDCG: 0.9295244443016011\n",
      "Epoch 67/100, Loss: 1.5627559581816484, Train-NDCG: 0.9791675217384757, Test-NDCG: 0.9310035496611158\n",
      "Epoch 68/100, Loss: 1.605176317303077, Train-NDCG: 0.9794512443453125, Test-NDCG: 0.9309505909741003\n",
      "Epoch 69/100, Loss: 1.5907088124377715, Train-NDCG: 0.9789467141189736, Test-NDCG: 0.9290376187395188\n",
      "Epoch 70/100, Loss: 1.5852687750821528, Train-NDCG: 0.9796836973571182, Test-NDCG: 0.9288149419242899\n",
      "Epoch 71/100, Loss: 1.590176184648189, Train-NDCG: 0.9786940073117059, Test-NDCG: 0.927199174104367\n",
      "Epoch 72/100, Loss: 1.6178505217323556, Train-NDCG: 0.9780614292886654, Test-NDCG: 0.9288438027138349\n",
      "Epoch 73/100, Loss: 1.6234533868838048, Train-NDCG: 0.9785375357269156, Test-NDCG: 0.9310165536711297\n",
      "Epoch 74/100, Loss: 1.565659076443329, Train-NDCG: 0.9789342969681528, Test-NDCG: 0.9302542834411281\n",
      "Epoch 75/100, Loss: 1.5716907483512077, Train-NDCG: 0.9804341530660373, Test-NDCG: 0.930025428447114\n",
      "Epoch 76/100, Loss: 1.5509362140499452, Train-NDCG: 0.9791157706762913, Test-NDCG: 0.9280602767910787\n",
      "Epoch 77/100, Loss: 1.5538820979963754, Train-NDCG: 0.9798487867145469, Test-NDCG: 0.9297827940728399\n",
      "Epoch 78/100, Loss: 1.5597270933758234, Train-NDCG: 0.9798867577522427, Test-NDCG: 0.9266844219274786\n",
      "Epoch 79/100, Loss: 1.5411582581950847, Train-NDCG: 0.9803584340740173, Test-NDCG: 0.9296256442756071\n",
      "Epoch 80/100, Loss: 1.5221072464173542, Train-NDCG: 0.9804188539195362, Test-NDCG: 0.9299583964899347\n",
      "Epoch 81/100, Loss: 1.5124114439755245, Train-NDCG: 0.9795669870401703, Test-NDCG: 0.9289603380917538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 1.5342854032121995, Train-NDCG: 0.9800217495468154, Test-NDCG: 0.9278871142350232\n",
      "Epoch 83/100, Loss: 1.5259594457304997, Train-NDCG: 0.9798892969716868, Test-NDCG: 0.9294028750117942\n",
      "Epoch 84/100, Loss: 1.5507627894049105, Train-NDCG: 0.9793107477933297, Test-NDCG: 0.9285367510028117\n",
      "Epoch 85/100, Loss: 1.5102004036307335, Train-NDCG: 0.9805278325360504, Test-NDCG: 0.9295991157714327\n",
      "Epoch 86/100, Loss: 1.502436808809854, Train-NDCG: 0.980134057179497, Test-NDCG: 0.9289186949095675\n",
      "Epoch 87/100, Loss: 1.4857463573200116, Train-NDCG: 0.9811249698403253, Test-NDCG: 0.9268530475945959\n",
      "Epoch 88/100, Loss: 1.4860902725210512, Train-NDCG: 0.9803657955662092, Test-NDCG: 0.9280152077040779\n",
      "Epoch 89/100, Loss: 1.596059288163692, Train-NDCG: 0.9795937251583244, Test-NDCG: 0.9236738070037024\n",
      "Epoch 90/100, Loss: 1.5223400697903933, Train-NDCG: 0.9794892755138207, Test-NDCG: 0.9271929409393503\n",
      "Epoch 91/100, Loss: 1.4701715330570793, Train-NDCG: 0.9807080579224041, Test-NDCG: 0.92927184705011\n",
      "Epoch 92/100, Loss: 1.4354954955082584, Train-NDCG: 0.9812437553860248, Test-NDCG: 0.9290770003672745\n",
      "Epoch 93/100, Loss: 1.4335750897509465, Train-NDCG: 0.9810017404170935, Test-NDCG: 0.9291489182684828\n",
      "Epoch 94/100, Loss: 1.4712072562804257, Train-NDCG: 0.9813852220235119, Test-NDCG: 0.9263104544637569\n",
      "Epoch 95/100, Loss: 1.4651689243993322, Train-NDCG: 0.9805605072762968, Test-NDCG: 0.9267198995765143\n",
      "Epoch 96/100, Loss: 1.436293083688487, Train-NDCG: 0.9814402148396785, Test-NDCG: 0.9245406264271231\n",
      "Epoch 97/100, Loss: 1.4606906679517404, Train-NDCG: 0.9814144746427725, Test-NDCG: 0.9287996121135079\n",
      "Epoch 98/100, Loss: 1.492412387460902, Train-NDCG: 0.9807487322337454, Test-NDCG: 0.9285529683501282\n",
      "Epoch 99/100, Loss: 1.490273199327614, Train-NDCG: 0.9800809284074506, Test-NDCG: 0.926049157543704\n",
      "Epoch 100/100, Loss: 1.5080638859726956, Train-NDCG: 0.9801514247369187, Test-NDCG: 0.9284905167840618\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 7).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "train_groups = list(train_data.groupby('keyword').size().to_numpy())\n",
    "test_groups = list(test_data.groupby('keyword').size().to_numpy())\n",
    "\n",
    "train_input, train_target = torch.tensor(train_data[columns_to_normalize].values), torch.tensor(train_data['rank'].values)\n",
    "test_input, test_target = torch.tensor(test_data[columns_to_normalize].values), torch.tensor(test_data['rank'].values)\n",
    "\n",
    "train_dataset = MyDataset(train_input.float(), train_target.float())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "test_dataset = MyDataset(test_input.float(), test_target.float())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize the ListNet model\n",
    "input_dim = train_input.size()[1]\n",
    "hidden_dim = 512  # Define the desired number of hidden units\n",
    "output_dim = 1  # Assuming you are predicting a single value per query\n",
    "\n",
    "# Initialize the ListNet model\n",
    "model = Net(input_dim, hidden_dim, output_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader, test_dataloader, train_groups, test_groups, l, 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e9d639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9284342466723683, 0.9430032670711095, 1.0, 0.7356209471245447)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1a = []\n",
    "out_test = torch.tensor([])\n",
    "rank_test = torch.tensor([])\n",
    "for a, b in test_dataloader:\n",
    "    test_output = model(a)\n",
    "    out_test = torch.cat((out_test, test_output), dim=0)\n",
    "    rank_test = torch.cat((rank_test, b), dim=0)\n",
    "            \n",
    "out_test = torch.split(out_test, test_groups)\n",
    "rank_test = torch.split(rank_test, test_groups)\n",
    "for t3, t4 in zip(out_test, rank_test):\n",
    "    if len(t3)==1:\n",
    "        pass\n",
    "    else:\n",
    "        scores1a.append(nd(np.asarray([t4.tolist()]), np.asarray([t3.view(-1).tolist()])))\n",
    "np.mean(scores1a), np.median(scores1a), max(scores1a), min(scores1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e500a",
   "metadata": {},
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76eda848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 6.319937277414713, Train-NDCG: 0.9162911326592287, Test-NDCG: 0.9239505269518824\n",
      "Epoch 2/100, Loss: 4.626631846485368, Train-NDCG: 0.9266115493046795, Test-NDCG: 0.9271824147285648\n",
      "Epoch 3/100, Loss: 4.222707372401134, Train-NDCG: 0.9328725777298476, Test-NDCG: 0.9247911633613489\n",
      "Epoch 4/100, Loss: 3.7852162635470012, Train-NDCG: 0.9386014007778882, Test-NDCG: 0.9257441040926552\n",
      "Epoch 5/100, Loss: 3.562610466221729, Train-NDCG: 0.9416282797917425, Test-NDCG: 0.9268876673788555\n",
      "Epoch 6/100, Loss: 3.4186729689678512, Train-NDCG: 0.9464223344505561, Test-NDCG: 0.9287019730913147\n",
      "Epoch 7/100, Loss: 3.316583383657846, Train-NDCG: 0.9486396217099741, Test-NDCG: 0.9282806231399383\n",
      "Epoch 8/100, Loss: 3.242714271775211, Train-NDCG: 0.9498765240656618, Test-NDCG: 0.9261614501673698\n",
      "Epoch 9/100, Loss: 3.1672654426241498, Train-NDCG: 0.9521240538954875, Test-NDCG: 0.9296348016521437\n",
      "Epoch 10/100, Loss: 3.155927246737193, Train-NDCG: 0.9541287906123014, Test-NDCG: 0.9277136129321675\n",
      "Epoch 11/100, Loss: 3.139146260324731, Train-NDCG: 0.9543234997579119, Test-NDCG: 0.9315448465303662\n",
      "Epoch 12/100, Loss: 2.9643775213195616, Train-NDCG: 0.9564851807353445, Test-NDCG: 0.9309859772963173\n",
      "Epoch 13/100, Loss: 2.880059502210962, Train-NDCG: 0.9573138484624955, Test-NDCG: 0.9272514545257659\n",
      "Epoch 14/100, Loss: 2.8031680464744566, Train-NDCG: 0.958463237468045, Test-NDCG: 0.9290840386724012\n",
      "Epoch 15/100, Loss: 2.712310865413712, Train-NDCG: 0.9605508723236643, Test-NDCG: 0.9272695160869642\n",
      "Epoch 16/100, Loss: 2.6661153664071877, Train-NDCG: 0.9611176273285822, Test-NDCG: 0.926953729762027\n",
      "Epoch 17/100, Loss: 2.6314568420490585, Train-NDCG: 0.9618186560808926, Test-NDCG: 0.931052745260844\n",
      "Epoch 18/100, Loss: 2.5446506223046637, Train-NDCG: 0.9634554147108485, Test-NDCG: 0.9299908987647034\n",
      "Epoch 19/100, Loss: 2.5390925215669427, Train-NDCG: 0.9629569798710315, Test-NDCG: 0.9292615977386162\n",
      "Epoch 20/100, Loss: 2.5211444929421667, Train-NDCG: 0.963686044953922, Test-NDCG: 0.9318979114053885\n",
      "Epoch 21/100, Loss: 2.4665871778166437, Train-NDCG: 0.965223486076384, Test-NDCG: 0.9299094867900575\n",
      "Epoch 22/100, Loss: 2.4047994768045036, Train-NDCG: 0.9656240118310552, Test-NDCG: 0.9279727979854581\n",
      "Epoch 23/100, Loss: 2.3688998797571803, Train-NDCG: 0.9662733906136377, Test-NDCG: 0.9271816259759991\n",
      "Epoch 24/100, Loss: 2.3173993033098887, Train-NDCG: 0.9674171466384618, Test-NDCG: 0.9285440829211974\n",
      "Epoch 25/100, Loss: 2.31048237659845, Train-NDCG: 0.9679424246995482, Test-NDCG: 0.9312889439407294\n",
      "Epoch 26/100, Loss: 2.3059084931051874, Train-NDCG: 0.9680284825085134, Test-NDCG: 0.9257596396517993\n",
      "Epoch 27/100, Loss: 2.2741957148873664, Train-NDCG: 0.968733210768632, Test-NDCG: 0.9293963281631118\n",
      "Epoch 28/100, Loss: 2.189578758952129, Train-NDCG: 0.9698276470206095, Test-NDCG: 0.9249882341576754\n",
      "Epoch 29/100, Loss: 2.179017395930118, Train-NDCG: 0.9704150852399639, Test-NDCG: 0.9296009983392932\n",
      "Epoch 30/100, Loss: 2.1791704169957034, Train-NDCG: 0.9693526555267504, Test-NDCG: 0.9293076625940953\n",
      "Epoch 31/100, Loss: 2.153000077641154, Train-NDCG: 0.9712669036716126, Test-NDCG: 0.9239465519074137\n",
      "Epoch 32/100, Loss: 2.16753055568201, Train-NDCG: 0.9708152068462522, Test-NDCG: 0.9320296414610529\n",
      "Epoch 33/100, Loss: 2.13253939611366, Train-NDCG: 0.9716516323887727, Test-NDCG: 0.925511363507162\n",
      "Epoch 34/100, Loss: 2.0820273232029143, Train-NDCG: 0.9710140307419888, Test-NDCG: 0.92786348073032\n",
      "Epoch 35/100, Loss: 2.0708323887313704, Train-NDCG: 0.9721602608941218, Test-NDCG: 0.9308113885546385\n",
      "Epoch 36/100, Loss: 2.030305485624865, Train-NDCG: 0.972529261309397, Test-NDCG: 0.9304582128699368\n",
      "Epoch 37/100, Loss: 2.015861274319959, Train-NDCG: 0.9729037593511481, Test-NDCG: 0.9286792531010192\n",
      "Epoch 38/100, Loss: 2.028716229171638, Train-NDCG: 0.9728738041244536, Test-NDCG: 0.9276175125547983\n",
      "Epoch 39/100, Loss: 1.9914630711796772, Train-NDCG: 0.9730790429479765, Test-NDCG: 0.9254544078056313\n",
      "Epoch 40/100, Loss: 1.9743774702390993, Train-NDCG: 0.9735295655634673, Test-NDCG: 0.9283214941145781\n",
      "Epoch 41/100, Loss: 1.9820780326803047, Train-NDCG: 0.9738369859752254, Test-NDCG: 0.9253733856341484\n",
      "Epoch 42/100, Loss: 1.9917163323207074, Train-NDCG: 0.9734093523006027, Test-NDCG: 0.9276116862007534\n",
      "Epoch 43/100, Loss: 1.9182189275701362, Train-NDCG: 0.9742432814095199, Test-NDCG: 0.9258814627981303\n",
      "Epoch 44/100, Loss: 1.969697202760053, Train-NDCG: 0.9731123931984701, Test-NDCG: 0.9296758936629201\n",
      "Epoch 45/100, Loss: 1.9595750048218004, Train-NDCG: 0.9743533207191853, Test-NDCG: 0.9276805900893025\n",
      "Epoch 46/100, Loss: 1.9317044995635388, Train-NDCG: 0.9750066155663188, Test-NDCG: 0.9280467978681735\n",
      "Epoch 47/100, Loss: 1.893965915671314, Train-NDCG: 0.9752549448486458, Test-NDCG: 0.9294389811758028\n",
      "Epoch 48/100, Loss: 1.876753798522145, Train-NDCG: 0.9752808902960642, Test-NDCG: 0.9288098479743541\n",
      "Epoch 49/100, Loss: 1.869838481100209, Train-NDCG: 0.9756230300698785, Test-NDCG: 0.9270475420291406\n",
      "Epoch 50/100, Loss: 1.8917939759880664, Train-NDCG: 0.9752349137537876, Test-NDCG: 0.9295376294674114\n",
      "Epoch 51/100, Loss: 1.853971602112414, Train-NDCG: 0.9753372066269077, Test-NDCG: 0.9284413329917377\n",
      "Epoch 52/100, Loss: 1.8476048055901584, Train-NDCG: 0.9761785848295883, Test-NDCG: 0.9258380078403933\n",
      "Epoch 53/100, Loss: 1.8728589833141809, Train-NDCG: 0.9754810488165767, Test-NDCG: 0.9251181524487607\n",
      "Epoch 54/100, Loss: 1.8382830099887157, Train-NDCG: 0.9757317402301329, Test-NDCG: 0.9290448739185703\n",
      "Epoch 55/100, Loss: 1.817338207256363, Train-NDCG: 0.9762411820446825, Test-NDCG: 0.9277109286317214\n",
      "Epoch 56/100, Loss: 1.7846048212913146, Train-NDCG: 0.9772475361201196, Test-NDCG: 0.9250068693874164\n",
      "Epoch 57/100, Loss: 1.786351886930236, Train-NDCG: 0.9764358562542085, Test-NDCG: 0.9217836021583197\n",
      "Epoch 58/100, Loss: 1.7682153482034981, Train-NDCG: 0.977108775892254, Test-NDCG: 0.9257285194229785\n",
      "Epoch 59/100, Loss: 1.7812696623514934, Train-NDCG: 0.9769841943363715, Test-NDCG: 0.9257723599685906\n",
      "Epoch 60/100, Loss: 1.7827870297144695, Train-NDCG: 0.9765092494668305, Test-NDCG: 0.9282625125672336\n",
      "Epoch 61/100, Loss: 1.7847882987504982, Train-NDCG: 0.9768753725236788, Test-NDCG: 0.9248917191485418\n",
      "Epoch 62/100, Loss: 1.7714938236288278, Train-NDCG: 0.976777209308671, Test-NDCG: 0.9269954705301339\n",
      "Epoch 63/100, Loss: 1.736409577583692, Train-NDCG: 0.9771039294414315, Test-NDCG: 0.9282577465842871\n",
      "Epoch 64/100, Loss: 1.749218974917768, Train-NDCG: 0.9771884173373927, Test-NDCG: 0.9261727385841825\n",
      "Epoch 65/100, Loss: 1.7599722044295576, Train-NDCG: 0.9764348987602713, Test-NDCG: 0.9259101053135397\n",
      "Epoch 66/100, Loss: 1.7570306246539196, Train-NDCG: 0.9772550829264766, Test-NDCG: 0.9261897565094173\n",
      "Epoch 67/100, Loss: 1.7357551321207758, Train-NDCG: 0.9771503428584155, Test-NDCG: 0.9273939576386442\n",
      "Epoch 68/100, Loss: 1.7069477772138204, Train-NDCG: 0.9777803673250817, Test-NDCG: 0.92447400901352\n",
      "Epoch 69/100, Loss: 1.7653137960347784, Train-NDCG: 0.9772973915756336, Test-NDCG: 0.9284513304652847\n",
      "Epoch 70/100, Loss: 1.7344138771654611, Train-NDCG: 0.9774679996359189, Test-NDCG: 0.9293479017774147\n",
      "Epoch 71/100, Loss: 1.796388937479042, Train-NDCG: 0.9775822419544213, Test-NDCG: 0.9209900928151469\n",
      "Epoch 72/100, Loss: 1.7874027682356088, Train-NDCG: 0.9760746264694553, Test-NDCG: 0.9275033575160458\n",
      "Epoch 73/100, Loss: 1.7506734296859028, Train-NDCG: 0.9781533116041683, Test-NDCG: 0.9262632233432879\n",
      "Epoch 74/100, Loss: 1.6926140156136937, Train-NDCG: 0.9783832922965879, Test-NDCG: 0.9237546284335346\n",
      "Epoch 75/100, Loss: 1.6616768706275755, Train-NDCG: 0.9785099706546538, Test-NDCG: 0.9260443085234029\n",
      "Epoch 76/100, Loss: 1.6458034860800548, Train-NDCG: 0.9793405301781153, Test-NDCG: 0.9264788352844471\n",
      "Epoch 77/100, Loss: 1.6343844173902489, Train-NDCG: 0.9789921274393684, Test-NDCG: 0.9256250653751864\n",
      "Epoch 78/100, Loss: 1.6362713906779347, Train-NDCG: 0.9789311114357185, Test-NDCG: 0.9244385660210339\n",
      "Epoch 79/100, Loss: 1.651938506637711, Train-NDCG: 0.9791419657572853, Test-NDCG: 0.9259871861215263\n",
      "Epoch 80/100, Loss: 1.8770377195025065, Train-NDCG: 0.9775905726135882, Test-NDCG: 0.925898910515313\n",
      "Epoch 81/100, Loss: 1.659990537310221, Train-NDCG: 0.9784286113695565, Test-NDCG: 0.9241773815955532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 1.6102178870913495, Train-NDCG: 0.9793606240851883, Test-NDCG: 0.9241635741031846\n",
      "Epoch 83/100, Loss: 1.5623822476490434, Train-NDCG: 0.9801604144086863, Test-NDCG: 0.925317507645462\n",
      "Epoch 84/100, Loss: 1.5938373679138091, Train-NDCG: 0.9801422856592393, Test-NDCG: 0.9256889564542917\n",
      "Epoch 85/100, Loss: 1.5799727040601064, Train-NDCG: 0.9796950351835719, Test-NDCG: 0.9233609467806415\n",
      "Epoch 86/100, Loss: 1.5952254869492657, Train-NDCG: 0.9797937807695519, Test-NDCG: 0.9228269109701509\n",
      "Epoch 87/100, Loss: 1.6205686737974005, Train-NDCG: 0.97897756850834, Test-NDCG: 0.9257205954193741\n",
      "Epoch 88/100, Loss: 1.5913343223821685, Train-NDCG: 0.9801544683170417, Test-NDCG: 0.9260602692765696\n",
      "Epoch 89/100, Loss: 1.612615851106414, Train-NDCG: 0.9793446606637417, Test-NDCG: 0.9273775616572381\n",
      "Epoch 90/100, Loss: 1.5672130576817387, Train-NDCG: 0.9804565743379946, Test-NDCG: 0.9238000365071672\n",
      "Epoch 91/100, Loss: 1.5251398067876516, Train-NDCG: 0.9802510892316484, Test-NDCG: 0.9221860939681402\n",
      "Epoch 92/100, Loss: 1.5650519202272577, Train-NDCG: 0.9798776621839648, Test-NDCG: 0.9235628587164415\n",
      "Epoch 93/100, Loss: 1.5792020224663148, Train-NDCG: 0.9796322622067207, Test-NDCG: 0.923060170625465\n",
      "Epoch 94/100, Loss: 1.5627378982233713, Train-NDCG: 0.9803972407573741, Test-NDCG: 0.9274403784200478\n",
      "Epoch 95/100, Loss: 1.5266014217612256, Train-NDCG: 0.9808007313772326, Test-NDCG: 0.9220458083605872\n",
      "Epoch 96/100, Loss: 1.5158446092921567, Train-NDCG: 0.9810967755231306, Test-NDCG: 0.9221768169524746\n",
      "Epoch 97/100, Loss: 1.531199864917491, Train-NDCG: 0.9804662161349685, Test-NDCG: 0.9235430354279678\n",
      "Epoch 98/100, Loss: 1.533915191529745, Train-NDCG: 0.9807100605385312, Test-NDCG: 0.9222676395535565\n",
      "Epoch 99/100, Loss: 1.5225359031953007, Train-NDCG: 0.9808664026328645, Test-NDCG: 0.9251700088505226\n",
      "Epoch 100/100, Loss: 1.4930103798946703, Train-NDCG: 0.9811006770957525, Test-NDCG: 0.9231891562089645\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 1).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "train_groups = list(train_data.groupby('keyword').size().to_numpy())\n",
    "test_groups = list(test_data.groupby('keyword').size().to_numpy())\n",
    "\n",
    "train_input, train_target = torch.tensor(train_data[columns_to_normalize].values), torch.tensor(train_data['rank'].values)\n",
    "test_input, test_target = torch.tensor(test_data[columns_to_normalize].values), torch.tensor(test_data['rank'].values)\n",
    "\n",
    "train_dataset = MyDataset(train_input.float(), train_target.float())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "test_dataset = MyDataset(test_input.float(), test_target.float())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize the ListNet model\n",
    "input_dim = train_input.size()[1]\n",
    "hidden_dim = 512  # Define the desired number of hidden units\n",
    "output_dim = 1  # Assuming you are predicting a single value per query\n",
    "\n",
    "# Initialize the ListNet model\n",
    "model = Net(input_dim, hidden_dim, output_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader, test_dataloader, train_groups, test_groups, l, 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "147e60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9243546765686688, 0.9371862681784394, 1.0, 0.7150798450205982)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1b = []\n",
    "out_test = torch.tensor([])\n",
    "rank_test = torch.tensor([])\n",
    "for a, b in test_dataloader:\n",
    "    test_output = model(a)\n",
    "    out_test = torch.cat((out_test, test_output), dim=0)\n",
    "    rank_test = torch.cat((rank_test, b), dim=0)\n",
    "            \n",
    "out_test = torch.split(out_test, test_groups)\n",
    "rank_test = torch.split(rank_test, test_groups)\n",
    "for t3, t4 in zip(out_test, rank_test):\n",
    "    if len(t3)==1:\n",
    "        pass\n",
    "    else:\n",
    "        scores1b.append(nd(np.asarray([t4.tolist()]), np.asarray([t3.view(-1).tolist()])))\n",
    "np.mean(scores1b), np.median(scores1b), max(scores1b), min(scores1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc247d40",
   "metadata": {},
   "source": [
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ea5cebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 6.991443266712346, Train-NDCG: 0.9190343155249036, Test-NDCG: 0.9186446192762954\n",
      "Epoch 2/100, Loss: 4.614868367470584, Train-NDCG: 0.9272135790106, Test-NDCG: 0.9179232294512732\n",
      "Epoch 3/100, Loss: 3.9816137226750548, Train-NDCG: 0.9341352925933443, Test-NDCG: 0.9217413350479815\n",
      "Epoch 4/100, Loss: 3.896985947217756, Train-NDCG: 0.9388411788093939, Test-NDCG: 0.9220038879171398\n",
      "Epoch 5/100, Loss: 3.55338031241616, Train-NDCG: 0.9418755002220977, Test-NDCG: 0.9247276434846885\n",
      "Epoch 6/100, Loss: 3.3866182299493586, Train-NDCG: 0.9451221024873551, Test-NDCG: 0.9239939673730099\n",
      "Epoch 7/100, Loss: 3.222157968479453, Train-NDCG: 0.9482035721123064, Test-NDCG: 0.9257370899022722\n",
      "Epoch 8/100, Loss: 3.18371483770389, Train-NDCG: 0.9506818593333264, Test-NDCG: 0.9255231315521868\n",
      "Epoch 9/100, Loss: 3.0777522151909986, Train-NDCG: 0.951297565062319, Test-NDCG: 0.926349829177642\n",
      "Epoch 10/100, Loss: 2.9575061615809655, Train-NDCG: 0.9542275695515148, Test-NDCG: 0.9260397226561695\n",
      "Epoch 11/100, Loss: 2.8700256272427085, Train-NDCG: 0.9555365307294195, Test-NDCG: 0.9279078615758399\n",
      "Epoch 12/100, Loss: 2.8018865931091956, Train-NDCG: 0.9574935120946286, Test-NDCG: 0.9271120627896412\n",
      "Epoch 13/100, Loss: 2.7121206235538406, Train-NDCG: 0.9586915626620583, Test-NDCG: 0.9276870565987726\n",
      "Epoch 14/100, Loss: 2.690378383525367, Train-NDCG: 0.9595562455893814, Test-NDCG: 0.9285960151935161\n",
      "Epoch 15/100, Loss: 2.6108236648504017, Train-NDCG: 0.9614939166338168, Test-NDCG: 0.9286651527456005\n",
      "Epoch 16/100, Loss: 2.598367423542495, Train-NDCG: 0.9619631321519561, Test-NDCG: 0.9265874770767832\n",
      "Epoch 17/100, Loss: 2.5636308436254853, Train-NDCG: 0.9632046842984182, Test-NDCG: 0.9281052359183233\n",
      "Epoch 18/100, Loss: 2.4839990153474716, Train-NDCG: 0.9640072879625579, Test-NDCG: 0.9269107711056828\n",
      "Epoch 19/100, Loss: 2.4476967163166954, Train-NDCG: 0.9651446609766744, Test-NDCG: 0.9247449758849714\n",
      "Epoch 20/100, Loss: 2.359297900813297, Train-NDCG: 0.9663124946804967, Test-NDCG: 0.9270749796234942\n",
      "Epoch 21/100, Loss: 2.3003428789597113, Train-NDCG: 0.9671811776586544, Test-NDCG: 0.9280201711398659\n",
      "Epoch 22/100, Loss: 2.287296772003174, Train-NDCG: 0.9685099700074287, Test-NDCG: 0.9268657054346618\n",
      "Epoch 23/100, Loss: 2.272070190281544, Train-NDCG: 0.9684180913149284, Test-NDCG: 0.9272762214987927\n",
      "Epoch 24/100, Loss: 2.242470548063227, Train-NDCG: 0.968346486427543, Test-NDCG: 0.9259515127523336\n",
      "Epoch 25/100, Loss: 2.246101070086933, Train-NDCG: 0.9684818159718219, Test-NDCG: 0.9272121227483878\n",
      "Epoch 26/100, Loss: 2.166520742102734, Train-NDCG: 0.9697310610254377, Test-NDCG: 0.9276607795764713\n",
      "Epoch 27/100, Loss: 2.1083687828148454, Train-NDCG: 0.970896728522675, Test-NDCG: 0.9286476163709583\n",
      "Epoch 28/100, Loss: 2.154446755219432, Train-NDCG: 0.9700279856040356, Test-NDCG: 0.9279565568812449\n",
      "Epoch 29/100, Loss: 2.0949869757717097, Train-NDCG: 0.971507452143204, Test-NDCG: 0.928030152642909\n",
      "Epoch 30/100, Loss: 2.118938007690374, Train-NDCG: 0.9712009485007814, Test-NDCG: 0.9306418009739041\n",
      "Epoch 31/100, Loss: 2.1245801909600646, Train-NDCG: 0.9700498874451242, Test-NDCG: 0.9291666190167178\n",
      "Epoch 32/100, Loss: 2.092896456133972, Train-NDCG: 0.9715020183586097, Test-NDCG: 0.9284134813265389\n",
      "Epoch 33/100, Loss: 2.0144452254459697, Train-NDCG: 0.9717499057236406, Test-NDCG: 0.9293800682683172\n",
      "Epoch 34/100, Loss: 1.9693602743802718, Train-NDCG: 0.9730762122278396, Test-NDCG: 0.92978847529167\n",
      "Epoch 35/100, Loss: 1.981960912280291, Train-NDCG: 0.9727992196238963, Test-NDCG: 0.929548082630809\n",
      "Epoch 36/100, Loss: 1.9648311894061496, Train-NDCG: 0.9726431379899431, Test-NDCG: 0.9291396053057129\n",
      "Epoch 37/100, Loss: 1.9424183519141187, Train-NDCG: 0.9732073307966744, Test-NDCG: 0.9293439861867141\n",
      "Epoch 38/100, Loss: 1.952607023918513, Train-NDCG: 0.9741237304204963, Test-NDCG: 0.9290077465462921\n",
      "Epoch 39/100, Loss: 2.0062977000347617, Train-NDCG: 0.9717136777061118, Test-NDCG: 0.9295434783413402\n",
      "Epoch 40/100, Loss: 1.9792394639508237, Train-NDCG: 0.9730731860029999, Test-NDCG: 0.9285974911101637\n",
      "Epoch 41/100, Loss: 1.9132762863074693, Train-NDCG: 0.9746230218227843, Test-NDCG: 0.9303595197043455\n",
      "Epoch 42/100, Loss: 1.8813312790058192, Train-NDCG: 0.974268520862665, Test-NDCG: 0.9284535800536051\n",
      "Epoch 43/100, Loss: 1.8411128238132857, Train-NDCG: 0.9754115279764143, Test-NDCG: 0.92927572478686\n",
      "Epoch 44/100, Loss: 1.8219694259386618, Train-NDCG: 0.9758851345862637, Test-NDCG: 0.9292924801497454\n",
      "Epoch 45/100, Loss: 1.8014354723171122, Train-NDCG: 0.9753280243294712, Test-NDCG: 0.9308857522573841\n",
      "Epoch 46/100, Loss: 1.8078722908224873, Train-NDCG: 0.9754189652073144, Test-NDCG: 0.929293701191914\n",
      "Epoch 47/100, Loss: 1.8124473409600628, Train-NDCG: 0.9755243740236967, Test-NDCG: 0.9314107577449656\n",
      "Epoch 48/100, Loss: 1.8233229007969782, Train-NDCG: 0.975869321213624, Test-NDCG: 0.9265364693183552\n",
      "Epoch 49/100, Loss: 1.8611790625360405, Train-NDCG: 0.974891781094555, Test-NDCG: 0.9256379511708952\n",
      "Epoch 50/100, Loss: 1.877952267027017, Train-NDCG: 0.9743478867887962, Test-NDCG: 0.9301399730892744\n",
      "Epoch 51/100, Loss: 1.8332820872513993, Train-NDCG: 0.9743740853864341, Test-NDCG: 0.929002830976146\n",
      "Epoch 52/100, Loss: 1.790930595936127, Train-NDCG: 0.9760669262293798, Test-NDCG: 0.9298928998105765\n",
      "Epoch 53/100, Loss: 1.7527888551065065, Train-NDCG: 0.9764386883234025, Test-NDCG: 0.9298216017837698\n",
      "Epoch 54/100, Loss: 1.707635177932318, Train-NDCG: 0.9772897744596514, Test-NDCG: 0.9277952749754932\n",
      "Epoch 55/100, Loss: 1.6981568926023047, Train-NDCG: 0.9776736866869536, Test-NDCG: 0.9298943155176804\n",
      "Epoch 56/100, Loss: 1.717248505397329, Train-NDCG: 0.9765316715271362, Test-NDCG: 0.9289348140366503\n",
      "Epoch 57/100, Loss: 1.7620920394954172, Train-NDCG: 0.9769591577776133, Test-NDCG: 0.9290420882640087\n",
      "Epoch 58/100, Loss: 1.8048109491039248, Train-NDCG: 0.9752030796411895, Test-NDCG: 0.9289560752412486\n",
      "Epoch 59/100, Loss: 1.8200514964687013, Train-NDCG: 0.97544939413147, Test-NDCG: 0.928454658460626\n",
      "Epoch 60/100, Loss: 1.710299594454395, Train-NDCG: 0.9762495386133562, Test-NDCG: 0.9288070167614402\n",
      "Epoch 61/100, Loss: 1.703222422779185, Train-NDCG: 0.9770515552991345, Test-NDCG: 0.9284631355589427\n",
      "Epoch 62/100, Loss: 1.6889992121932575, Train-NDCG: 0.9774684515717592, Test-NDCG: 0.9288050036378495\n",
      "Epoch 63/100, Loss: 1.6936571324262226, Train-NDCG: 0.9772300735125575, Test-NDCG: 0.9291117126672916\n",
      "Epoch 64/100, Loss: 1.7048231751016043, Train-NDCG: 0.9776145068109426, Test-NDCG: 0.9302027630979938\n",
      "Epoch 65/100, Loss: 1.6235979305743014, Train-NDCG: 0.9782026369428652, Test-NDCG: 0.9307190431047876\n",
      "Epoch 66/100, Loss: 1.6400991538810787, Train-NDCG: 0.9785987288817958, Test-NDCG: 0.9295015150637439\n",
      "Epoch 67/100, Loss: 1.6463428931762871, Train-NDCG: 0.9777646408852061, Test-NDCG: 0.9312709907681505\n",
      "Epoch 68/100, Loss: 1.6718402442712228, Train-NDCG: 0.9775200918460201, Test-NDCG: 0.9279897142981895\n",
      "Epoch 69/100, Loss: 1.6566135590009898, Train-NDCG: 0.977919995868573, Test-NDCG: 0.9281881540962738\n",
      "Epoch 70/100, Loss: 1.651349426075382, Train-NDCG: 0.9777239731866704, Test-NDCG: 0.9304320711034534\n",
      "Epoch 71/100, Loss: 1.7238101662652006, Train-NDCG: 0.9765032657923792, Test-NDCG: 0.9311426436028887\n",
      "Epoch 72/100, Loss: 1.6765327947666344, Train-NDCG: 0.9777260321260097, Test-NDCG: 0.9303391717806935\n",
      "Epoch 73/100, Loss: 1.6031212889164397, Train-NDCG: 0.9786272186359851, Test-NDCG: 0.9305734443261292\n",
      "Epoch 74/100, Loss: 1.586269865192256, Train-NDCG: 0.9782330906641121, Test-NDCG: 0.9307508697814659\n",
      "Epoch 75/100, Loss: 1.5611576324383032, Train-NDCG: 0.9787360484305218, Test-NDCG: 0.931188282786719\n",
      "Epoch 76/100, Loss: 1.579151402616385, Train-NDCG: 0.9788223838936617, Test-NDCG: 0.9285817322025665\n",
      "Epoch 77/100, Loss: 1.5989462445372518, Train-NDCG: 0.9787082492217706, Test-NDCG: 0.9324455437709932\n",
      "Epoch 78/100, Loss: 1.5906711087643521, Train-NDCG: 0.9791255276117531, Test-NDCG: 0.9294838541699728\n",
      "Epoch 79/100, Loss: 1.6014220998848525, Train-NDCG: 0.9787217106537841, Test-NDCG: 0.9324303090460423\n",
      "Epoch 80/100, Loss: 1.5949205895433727, Train-NDCG: 0.9792857444496513, Test-NDCG: 0.9307747716935262\n",
      "Epoch 81/100, Loss: 1.5625682847158422, Train-NDCG: 0.9792008362390408, Test-NDCG: 0.9314154780018457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 1.578314842023317, Train-NDCG: 0.979588935651885, Test-NDCG: 0.9310318949514436\n",
      "Epoch 83/100, Loss: 1.5876173442862567, Train-NDCG: 0.9790059949546549, Test-NDCG: 0.9312097412196916\n",
      "Epoch 84/100, Loss: 1.568233361738978, Train-NDCG: 0.9792658394740809, Test-NDCG: 0.9326522095273581\n",
      "Epoch 85/100, Loss: 1.5925545280853521, Train-NDCG: 0.9784895481161606, Test-NDCG: 0.9310068520366217\n",
      "Epoch 86/100, Loss: 1.5722685587203618, Train-NDCG: 0.9797846421226126, Test-NDCG: 0.9294744574234304\n",
      "Epoch 87/100, Loss: 1.5461655150628784, Train-NDCG: 0.9795343651532601, Test-NDCG: 0.930380273717688\n",
      "Epoch 88/100, Loss: 1.5846958009942065, Train-NDCG: 0.979737786033023, Test-NDCG: 0.9290033202590531\n",
      "Epoch 89/100, Loss: 1.7227677341948435, Train-NDCG: 0.9766817220138069, Test-NDCG: 0.9287967385764268\n",
      "Epoch 90/100, Loss: 1.595078486262016, Train-NDCG: 0.9789980335952818, Test-NDCG: 0.9321748822612334\n",
      "Epoch 91/100, Loss: 1.5282082719348589, Train-NDCG: 0.9800768314225141, Test-NDCG: 0.932299199273846\n",
      "Epoch 92/100, Loss: 1.4991154188597666, Train-NDCG: 0.980478350470733, Test-NDCG: 0.9313854257773603\n",
      "Epoch 93/100, Loss: 1.5037224744853463, Train-NDCG: 0.9803381494009813, Test-NDCG: 0.9331515389035581\n",
      "Epoch 94/100, Loss: 1.5322093414594826, Train-NDCG: 0.9799454062788476, Test-NDCG: 0.9318325899921258\n",
      "Epoch 95/100, Loss: 1.5142803873902964, Train-NDCG: 0.9803392580221288, Test-NDCG: 0.9307432316870947\n",
      "Epoch 96/100, Loss: 1.489859497120369, Train-NDCG: 0.9811782628067423, Test-NDCG: 0.931255014389454\n",
      "Epoch 97/100, Loss: 1.516399538488064, Train-NDCG: 0.9808998287414883, Test-NDCG: 0.9314624295083422\n",
      "Epoch 98/100, Loss: 1.495751402747718, Train-NDCG: 0.9803705994169568, Test-NDCG: 0.9310993545269832\n",
      "Epoch 99/100, Loss: 1.4722178967543018, Train-NDCG: 0.9802736928309449, Test-NDCG: 0.9303884463521039\n",
      "Epoch 100/100, Loss: 1.499702487104726, Train-NDCG: 0.9812867782593183, Test-NDCG: 0.9297480333332331\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 7).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "train_groups = list(train_data.groupby('keyword').size().to_numpy())\n",
    "test_groups = list(test_data.groupby('keyword').size().to_numpy())\n",
    "\n",
    "train_input, train_target = torch.tensor(train_data[columns_to_normalize].values), torch.tensor(train_data['rank'].values)\n",
    "test_input, test_target = torch.tensor(test_data[columns_to_normalize].values), torch.tensor(test_data['rank'].values)\n",
    "\n",
    "train_dataset = MyDataset(train_input.float(), train_target.float())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "test_dataset = MyDataset(test_input.float(), test_target.float())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize the ListNet model\n",
    "input_dim = train_input.size()[1]\n",
    "hidden_dim = 512  # Define the desired number of hidden units\n",
    "output_dim = 1  # Assuming you are predicting a single value per query\n",
    "\n",
    "# Initialize the ListNet model\n",
    "model = Net(input_dim, hidden_dim, output_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader, test_dataloader, train_groups, test_groups, l, 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0a1d031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9298153061646198, 0.942119596428084, 0.9989773240205482, 0.6967511612000046)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1c = []\n",
    "out_test = torch.tensor([])\n",
    "rank_test = torch.tensor([])\n",
    "for a, b in test_dataloader:\n",
    "    test_output = model(a)\n",
    "    out_test = torch.cat((out_test, test_output), dim=0)\n",
    "    rank_test = torch.cat((rank_test, b), dim=0)\n",
    "            \n",
    "out_test = torch.split(out_test, test_groups)\n",
    "rank_test = torch.split(rank_test, test_groups)\n",
    "for t3, t4 in zip(out_test, rank_test):\n",
    "    if len(t3)==1:\n",
    "        pass\n",
    "    else:\n",
    "        scores1c.append(nd(np.asarray([t4.tolist()]), np.asarray([t3.view(-1).tolist()])))\n",
    "np.mean(scores1c), np.median(scores1c), max(scores1c), min(scores1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ebc25f",
   "metadata": {},
   "source": [
    "## Pairwise Learning-To-Rank Model\n",
    "- For RankNet: Loss function = Pairwise Loss (similar to negative log likelihood loss of relative ranking/preferences)\n",
    "    - Use Neural Net model\n",
    "- For LambdaRank or LambdaMart (boosted tree version of LambdaRank): Directly using gradient for Gradient Descent optimization, to improve evaluation metric\n",
    "    - Using XGBoost Model\n",
    "- Evaluation Metric: NDCG\n",
    "\n",
    "Intuition: comparing a pair of datapoints is usually easier to evaluate a single one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e17bc2",
   "metadata": {},
   "source": [
    "### RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26cbc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_grad_enabled(True) # Enable GPU\n",
    "torch.cuda.empty_cache() # Clear cuda cache (if any)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        target = self.targets[index]\n",
    "        return feature, target\n",
    "    \n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self, num_feature):\n",
    "        super(RankNet, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear( num_feature, 512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.output_sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1,input_2):\n",
    "        s1 = self.model(input_1)\n",
    "        s2 = self.model(input_2)\n",
    "        out = self.output_sig(s1-s2)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, input_):\n",
    "        s = self.model(input_)\n",
    "        return s\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear: # by checking the type we can init different layers in different ways\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "def pairwise_format(data, columns_to_normalize):\n",
    "    grouped_data = data.groupby('keyword')\n",
    "    query_features1 = []  # List to store query features\n",
    "    query_features2 = []\n",
    "    query_urls = []  # List to store URLs for each query\n",
    "    query_ranks = []  # List to store ranks for each URL\n",
    "\n",
    "    for query, group in grouped_data:\n",
    "        if len(group['rank'].tolist()) == 10:\n",
    "            urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "            ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "            features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "\n",
    "            num_urls = len(urls)\n",
    "            r = []\n",
    "            url = []\n",
    "            f1 = []\n",
    "            f2 = []\n",
    "            for i in range(num_urls):\n",
    "                for j in range(num_urls):\n",
    "                    if ranks[i] == ranks[j]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        url.append((urls[i], urls[j]))\n",
    "                        query_urls.append(url)\n",
    "                        r.append(1 if ranks[i] < ranks[j] else 0)\n",
    "                        query_ranks.append(r)\n",
    "                        f1.append(list(features[i]))\n",
    "                        query_features1.append(f1)\n",
    "                        f2.append(list(features[j]))\n",
    "                        query_features2.append(f2)\n",
    "        else:\n",
    "            pass \n",
    "    \n",
    "    input1, input2 = torch.tensor(query_features1, dtype=torch.float32), torch.tensor(query_features2, dtype=torch.float32)\n",
    "    target_data = torch.tensor(query_ranks, dtype=torch.long)\n",
    "    \n",
    "    return input1, input2, target_data\n",
    "\n",
    "def val_format(data, columns_to_normalize):\n",
    "    grouped_data = data.groupby('keyword')\n",
    "    query_features = []  # List to store query features\n",
    "    query_urls = []  # List to store URLs for each query\n",
    "    query_ranks = []  # List to store ranks for each URL\n",
    "\n",
    "    for query, group in grouped_data:\n",
    "        if len(group['rank'].tolist()) == 10:\n",
    "            urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "            ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "            features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "\n",
    "            query_urls.append(urls)\n",
    "            query_ranks.append(ranks)\n",
    "            query_features.append(features)\n",
    "        else:\n",
    "            pass \n",
    "    \n",
    "    input_data = torch.tensor(query_features, dtype=torch.float32)\n",
    "    target_data = torch.tensor(query_ranks, dtype=torch.long)\n",
    "    \n",
    "    return input_data, target_data\n",
    "\n",
    "def train_ltr(net, train_iter1, train_iter2, train_iter_check, test_iter_check, loss, num_epochs, optimizer):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "        \n",
    "    loss_ = []\n",
    "    train_ndcg = []\n",
    "    test_ndcg = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        total_ndcg = []\n",
    "        total_test_ndcg = []\n",
    "        \n",
    "        for (i1, v1), (i2, v2) in zip(train_iter1, train_iter2):\n",
    "            i1, i2, v1 = i1.to(device), i2.to(device), v1.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(i1, i2)\n",
    "            v1 = v1.float()\n",
    "            loss = l(output.view(-1,90), v1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "        for t1, t2 in train_check_loader:\n",
    "            t1, t2 = t1.to(device), t2.to(device)\n",
    "            train_output = net.predict(t1)\n",
    "            \n",
    "            for t3, t4 in zip(train_output, t2):\n",
    "                ndcg_score = nd(np.asarray([t4.flatten().tolist()]), np.asarray([t3.flatten().tolist()]))\n",
    "                total_ndcg.append(ndcg_score)\n",
    "            \n",
    "        for a, b in test_check_loader:\n",
    "            a, b = a.to(device), b.to(device)\n",
    "            test_output = net.predict(a)\n",
    "            \n",
    "            for a1, b1 in zip(test_output, b):\n",
    "                test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "                total_test_ndcg.append(test_ndcg_score)\n",
    "            \n",
    "        average_loss = total_loss/num_batches\n",
    "        average_ndcg = np.mean(total_ndcg)\n",
    "        average_test_ndcg = np.mean(total_test_ndcg)\n",
    "        \n",
    "        loss_.append(average_loss)\n",
    "        train_ndcg.append(average_ndcg)\n",
    "        test_ndcg.append(average_test_ndcg)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Train-NDCG: {average_ndcg}, Test-NDCG: {average_test_ndcg}\")\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
    "    return loss_, train_ndcg, test_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1242d17",
   "metadata": {},
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3db8b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.37994626299064965, Train-NDCG: 0.6883393306186818, Test-NDCG: 0.6883189947478938\n",
      "Epoch 2/20, Loss: 0.333599625623261, Train-NDCG: 0.6864503579201632, Test-NDCG: 0.6870898430768239\n",
      "Epoch 3/20, Loss: 0.3262672384412094, Train-NDCG: 0.6858266212335532, Test-NDCG: 0.6852665286730466\n",
      "Epoch 4/20, Loss: 0.3238866682811182, Train-NDCG: 0.6860502470729599, Test-NDCG: 0.6842231035346645\n",
      "Epoch 5/20, Loss: 0.3226418293622705, Train-NDCG: 0.686028312677866, Test-NDCG: 0.6864033309854392\n",
      "Epoch 6/20, Loss: 0.3214563580255908, Train-NDCG: 0.6854365287497938, Test-NDCG: 0.6850413781925608\n",
      "Epoch 7/20, Loss: 0.3204760282195385, Train-NDCG: 0.6849785373561215, Test-NDCG: 0.6852325791331767\n",
      "Epoch 8/20, Loss: 0.3197698192643701, Train-NDCG: 0.6840045312437845, Test-NDCG: 0.6843710025592218\n",
      "Epoch 9/20, Loss: 0.3194179110105525, Train-NDCG: 0.6853130276267677, Test-NDCG: 0.6851869437231082\n",
      "Epoch 10/20, Loss: 0.31956622724142564, Train-NDCG: 0.685922592927341, Test-NDCG: 0.6853838413555515\n",
      "Epoch 11/20, Loss: 0.31897126796825154, Train-NDCG: 0.6852490631146919, Test-NDCG: 0.6847814411694262\n",
      "Epoch 12/20, Loss: 0.31863462249877045, Train-NDCG: 0.6848920276484365, Test-NDCG: 0.6863305109816307\n",
      "Epoch 13/20, Loss: 0.3185816120270362, Train-NDCG: 0.6852855878402212, Test-NDCG: 0.6848414320392663\n",
      "Epoch 14/20, Loss: 0.3186277961785196, Train-NDCG: 0.6849883103746265, Test-NDCG: 0.6848189399432296\n",
      "Epoch 15/20, Loss: 0.3180577038149575, Train-NDCG: 0.6853255377193483, Test-NDCG: 0.6862025867120589\n",
      "Epoch 16/20, Loss: 0.318171658098872, Train-NDCG: 0.6844474998908181, Test-NDCG: 0.6849979796491991\n",
      "Epoch 17/20, Loss: 0.3179732014557972, Train-NDCG: 0.6857407363324001, Test-NDCG: 0.6854382091098038\n",
      "Epoch 18/20, Loss: 0.3174918193175183, Train-NDCG: 0.6846348334466823, Test-NDCG: 0.6847832219163268\n",
      "Epoch 19/20, Loss: 0.31787512268111967, Train-NDCG: 0.6849799227877099, Test-NDCG: 0.6854380006957491\n",
      "Epoch 20/20, Loss: 0.31728388806868574, Train-NDCG: 0.6858114310471773, Test-NDCG: 0.6851720162029242\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 7).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds] \n",
    "\n",
    "#Re-format to train pairwise\n",
    "train_input1, train_input2, train_target = pairwise_format(train_data, columns_to_normalize)\n",
    "test_input1, test_input2, test_target = pairwise_format(test_data, columns_to_normalize)\n",
    "\n",
    "#Format to check model perf on train and val set\n",
    "train_input_check, train_target_check = val_format(train_data, columns_to_normalize)\n",
    "test_input_check, test_target_check = val_format(train_data, columns_to_normalize)\n",
    "\n",
    "# Used to train model\n",
    "train_dataset1 = MyDataset(train_input1, train_target)\n",
    "train_dataset2 = MyDataset(train_input2, train_target)\n",
    "train_dataloader1 = DataLoader(train_dataset1, batch_size=16, shuffle=True)\n",
    "train_dataloader2 = DataLoader(train_dataset2, batch_size=16, shuffle=True)\n",
    "\n",
    "# Used to validate NDCG\n",
    "train_dataset = MyDataset(train_input_check, train_target_check)\n",
    "test_dataset = MyDataset(test_input_check, test_target_check)\n",
    "train_check_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_check_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#Initialize model\n",
    "model = RankNet(num_feature=76).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = nn.BCELoss().to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader1, train_dataloader2, train_check_loader, test_check_loader, l, 20, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e968487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6863407873921439,\n",
       " 0.6792533816775902,\n",
       " 0.8596060348214233,\n",
       " 0.6678559202923124)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2a = []\n",
    "for a, b in test_check_loader:\n",
    "    a, b = a.to(device), b.to(device)\n",
    "    test_output = model.predict(a)\n",
    "            \n",
    "    for a1, b1 in zip(test_output, b):\n",
    "        test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "        score2a.append(test_ndcg_score)\n",
    "np.mean(score2a), np.median(score2a), max(score2a), min(score2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8fc15",
   "metadata": {},
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a11597af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.3956162796843389, Train-NDCG: 0.6901627005469467, Test-NDCG: 0.690666986970102\n",
      "Epoch 2/20, Loss: 0.34990119478862935, Train-NDCG: 0.6889249799358067, Test-NDCG: 0.6876667155423212\n",
      "Epoch 3/20, Loss: 0.34192840026150984, Train-NDCG: 0.6880513884422468, Test-NDCG: 0.6889168611674454\n",
      "Epoch 4/20, Loss: 0.3388544781993727, Train-NDCG: 0.6877918411610507, Test-NDCG: 0.6882252238590136\n",
      "Epoch 5/20, Loss: 0.3369176027485795, Train-NDCG: 0.687840067481892, Test-NDCG: 0.6884212202068759\n",
      "Epoch 6/20, Loss: 0.33540937970860035, Train-NDCG: 0.6872671967319295, Test-NDCG: 0.6885450734237394\n",
      "Epoch 7/20, Loss: 0.3350853213424763, Train-NDCG: 0.6894110938637562, Test-NDCG: 0.6898201624152641\n",
      "Epoch 8/20, Loss: 0.33423599840024093, Train-NDCG: 0.6873047623701604, Test-NDCG: 0.6872577255676731\n",
      "Epoch 9/20, Loss: 0.33414142289485566, Train-NDCG: 0.6868829714117001, Test-NDCG: 0.6868659478993848\n",
      "Epoch 10/20, Loss: 0.33385821061162235, Train-NDCG: 0.689777728471629, Test-NDCG: 0.6885626765701659\n",
      "Epoch 11/20, Loss: 0.3333666640750611, Train-NDCG: 0.6880766766917978, Test-NDCG: 0.6867172541129842\n",
      "Epoch 12/20, Loss: 0.3336221278198051, Train-NDCG: 0.6871202093340498, Test-NDCG: 0.6873941667306753\n",
      "Epoch 13/20, Loss: 0.33250704544257687, Train-NDCG: 0.6866502491523235, Test-NDCG: 0.6866711820086204\n",
      "Epoch 14/20, Loss: 0.3326127997438823, Train-NDCG: 0.6889496279523118, Test-NDCG: 0.688676137629165\n",
      "Epoch 15/20, Loss: 0.33321698481167994, Train-NDCG: 0.6880447413569447, Test-NDCG: 0.6877003063958247\n",
      "Epoch 16/20, Loss: 0.33251036354423846, Train-NDCG: 0.6879603787147822, Test-NDCG: 0.6876710959070361\n",
      "Epoch 17/20, Loss: 0.33269252818166184, Train-NDCG: 0.6879197249550392, Test-NDCG: 0.6876845482069023\n",
      "Epoch 18/20, Loss: 0.3330268730077412, Train-NDCG: 0.6863711292038723, Test-NDCG: 0.6860745810204367\n",
      "Epoch 19/20, Loss: 0.33203398697090336, Train-NDCG: 0.6874647020373439, Test-NDCG: 0.6874728845052572\n",
      "Epoch 20/20, Loss: 0.33202092879811085, Train-NDCG: 0.6888190819469242, Test-NDCG: 0.6871933580130932\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 1).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds] \n",
    "\n",
    "#Re-format to train pairwise\n",
    "train_input1, train_input2, train_target = pairwise_format(train_data, columns_to_normalize)\n",
    "test_input1, test_input2, test_target = pairwise_format(test_data, columns_to_normalize)\n",
    "\n",
    "#Format to check model perf on train and val set\n",
    "train_input_check, train_target_check = val_format(train_data, columns_to_normalize)\n",
    "test_input_check, test_target_check = val_format(train_data, columns_to_normalize)\n",
    "\n",
    "# Used to train model\n",
    "train_dataset1 = MyDataset(train_input1, train_target)\n",
    "train_dataset2 = MyDataset(train_input2, train_target)\n",
    "train_dataloader1 = DataLoader(train_dataset1, batch_size=16, shuffle=True)\n",
    "train_dataloader2 = DataLoader(train_dataset2, batch_size=16, shuffle=True)\n",
    "\n",
    "# Used to validate NDCG\n",
    "train_dataset = MyDataset(train_input_check, train_target_check)\n",
    "test_dataset = MyDataset(test_input_check, test_target_check)\n",
    "train_check_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_check_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#Initialize model\n",
    "model = RankNet(num_feature=76).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = nn.BCELoss().to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader1, train_dataloader2, train_check_loader, test_check_loader, l, 20, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9d0e587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6881247246043845,\n",
       " 0.6807197679479599,\n",
       " 0.8880113752241318,\n",
       " 0.6678559202923124)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2b = []\n",
    "for a, b in test_check_loader:\n",
    "    a, b = a.to(device), b.to(device)\n",
    "    test_output = model.predict(a)\n",
    "            \n",
    "    for a1, b1 in zip(test_output, b):\n",
    "        test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "        score2b.append(test_ndcg_score)\n",
    "np.mean(score2b), np.median(score2b), max(score2b), min(score2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e099d",
   "metadata": {},
   "source": [
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a8ecd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.39837799665595713, Train-NDCG: 0.6910695645926642, Test-NDCG: 0.6901967059171198\n",
      "Epoch 2/20, Loss: 0.3540858403915384, Train-NDCG: 0.6896466965925245, Test-NDCG: 0.6898209388455452\n",
      "Epoch 3/20, Loss: 0.3465347528302421, Train-NDCG: 0.6884862307169289, Test-NDCG: 0.6889796957208523\n",
      "Epoch 4/20, Loss: 0.3435737770050764, Train-NDCG: 0.6874196980307218, Test-NDCG: 0.6877304933053008\n",
      "Epoch 5/20, Loss: 0.3415521091374103, Train-NDCG: 0.6887944552303292, Test-NDCG: 0.6888461010919631\n",
      "Epoch 6/20, Loss: 0.34043553012694855, Train-NDCG: 0.6871725329942826, Test-NDCG: 0.6875695903256098\n",
      "Epoch 7/20, Loss: 0.3388023389367542, Train-NDCG: 0.6883771823676195, Test-NDCG: 0.6877078905016932\n",
      "Epoch 8/20, Loss: 0.33877588911370066, Train-NDCG: 0.6881006764777948, Test-NDCG: 0.6877071104009813\n",
      "Epoch 9/20, Loss: 0.33859045983990654, Train-NDCG: 0.6880361996800776, Test-NDCG: 0.6884913867418643\n",
      "Epoch 10/20, Loss: 0.3377907997055445, Train-NDCG: 0.6887385850026507, Test-NDCG: 0.6875775200058901\n",
      "Epoch 11/20, Loss: 0.3377440290933009, Train-NDCG: 0.688167620120789, Test-NDCG: 0.6890373822191431\n",
      "Epoch 12/20, Loss: 0.33706514871058363, Train-NDCG: 0.6886654488866532, Test-NDCG: 0.6876842285437822\n",
      "Epoch 13/20, Loss: 0.33739801597160596, Train-NDCG: 0.6886946473137419, Test-NDCG: 0.6879826534541451\n",
      "Epoch 14/20, Loss: 0.3369520065316465, Train-NDCG: 0.6898880153921276, Test-NDCG: 0.6892009937700966\n",
      "Epoch 15/20, Loss: 0.337056529877979, Train-NDCG: 0.6881617270325822, Test-NDCG: 0.687349539672476\n",
      "Epoch 16/20, Loss: 0.3366355213899321, Train-NDCG: 0.6874535922262176, Test-NDCG: 0.6870777376074729\n",
      "Epoch 17/20, Loss: 0.33609219835489057, Train-NDCG: 0.6881863478498166, Test-NDCG: 0.6883151473528505\n",
      "Epoch 18/20, Loss: 0.3368752271926496, Train-NDCG: 0.6880489394517665, Test-NDCG: 0.687675708992046\n",
      "Epoch 19/20, Loss: 0.3365452068101149, Train-NDCG: 0.6883982940359209, Test-NDCG: 0.6884262001752047\n",
      "Epoch 20/20, Loss: 0.3370322768557041, Train-NDCG: 0.6880700003776801, Test-NDCG: 0.6878736160818161\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 5).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds] \n",
    "\n",
    "#Re-format to train pairwise\n",
    "train_input1, train_input2, train_target = pairwise_format(train_data, columns_to_normalize)\n",
    "test_input1, test_input2, test_target = pairwise_format(test_data, columns_to_normalize)\n",
    "\n",
    "#Format to check model perf on train and val set\n",
    "train_input_check, train_target_check = val_format(train_data, columns_to_normalize)\n",
    "test_input_check, test_target_check = val_format(train_data, columns_to_normalize)\n",
    "\n",
    "# Used to train model\n",
    "train_dataset1 = MyDataset(train_input1, train_target)\n",
    "train_dataset2 = MyDataset(train_input2, train_target)\n",
    "train_dataloader1 = DataLoader(train_dataset1, batch_size=16, shuffle=True)\n",
    "train_dataloader2 = DataLoader(train_dataset2, batch_size=16, shuffle=True)\n",
    "\n",
    "# Used to validate NDCG\n",
    "train_dataset = MyDataset(train_input_check, train_target_check)\n",
    "test_dataset = MyDataset(test_input_check, test_target_check)\n",
    "train_check_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_check_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#Initialize model\n",
    "model = RankNet(num_feature=76).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = nn.BCELoss().to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader1, train_dataloader2, train_check_loader, test_check_loader, l, 20, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e47f5805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6867012140533927, 0.6795468045830441, 0.868847788818665, 0.6678559202923124)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2c = []\n",
    "for a, b in test_check_loader:\n",
    "    a, b = a.to(device), b.to(device)\n",
    "    test_output = model.predict(a)\n",
    "            \n",
    "    for a1, b1 in zip(test_output, b):\n",
    "        test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "        score2c.append(test_ndcg_score)\n",
    "np.mean(score2c), np.median(score2c), max(score2c), min(score2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fa940",
   "metadata": {},
   "source": [
    "### LambdaRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ac6f1",
   "metadata": {},
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3c072c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10259\n",
      "[LightGBM] [Info] Number of data points in the train set: 6621, number of used features: 71\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[1]\ttraining's ndcg@1: 0.333758\ttraining's ndcg@2: 0.391452\ttraining's ndcg@3: 0.47024\ttraining's ndcg@4: 0.547874\ttraining's ndcg@5: 0.619914\tvalid_1's ndcg@1: 0.255738\tvalid_1's ndcg@2: 0.324473\tvalid_1's ndcg@3: 0.398701\tvalid_1's ndcg@4: 0.486909\tvalid_1's ndcg@5: 0.553111\n",
      "[2]\ttraining's ndcg@1: 0.570076\ttraining's ndcg@2: 0.633254\ttraining's ndcg@3: 0.689085\ttraining's ndcg@4: 0.729717\ttraining's ndcg@5: 0.768455\tvalid_1's ndcg@1: 0.390628\tvalid_1's ndcg@2: 0.484569\tvalid_1's ndcg@3: 0.534648\tvalid_1's ndcg@4: 0.603078\tvalid_1's ndcg@5: 0.660952\n",
      "[3]\ttraining's ndcg@1: 0.636204\ttraining's ndcg@2: 0.681956\ttraining's ndcg@3: 0.72571\ttraining's ndcg@4: 0.760516\ttraining's ndcg@5: 0.797268\tvalid_1's ndcg@1: 0.420091\tvalid_1's ndcg@2: 0.494252\tvalid_1's ndcg@3: 0.55319\tvalid_1's ndcg@4: 0.617322\tvalid_1's ndcg@5: 0.669076\n",
      "[4]\ttraining's ndcg@1: 0.677028\ttraining's ndcg@2: 0.711508\ttraining's ndcg@3: 0.750847\ttraining's ndcg@4: 0.781914\ttraining's ndcg@5: 0.813432\tvalid_1's ndcg@1: 0.41119\tvalid_1's ndcg@2: 0.483157\tvalid_1's ndcg@3: 0.563647\tvalid_1's ndcg@4: 0.620144\tvalid_1's ndcg@5: 0.668057\n",
      "[5]\ttraining's ndcg@1: 0.685573\ttraining's ndcg@2: 0.721994\ttraining's ndcg@3: 0.76275\ttraining's ndcg@4: 0.79133\ttraining's ndcg@5: 0.822435\tvalid_1's ndcg@1: 0.409959\tvalid_1's ndcg@2: 0.506121\tvalid_1's ndcg@3: 0.573206\tvalid_1's ndcg@4: 0.635949\tvalid_1's ndcg@5: 0.674336\n",
      "[6]\ttraining's ndcg@1: 0.711779\ttraining's ndcg@2: 0.741352\ttraining's ndcg@3: 0.776444\ttraining's ndcg@4: 0.805341\ttraining's ndcg@5: 0.832125\tvalid_1's ndcg@1: 0.47468\tvalid_1's ndcg@2: 0.529692\tvalid_1's ndcg@3: 0.598411\tvalid_1's ndcg@4: 0.662734\tvalid_1's ndcg@5: 0.6971\n",
      "[7]\ttraining's ndcg@1: 0.731191\ttraining's ndcg@2: 0.754867\ttraining's ndcg@3: 0.786283\ttraining's ndcg@4: 0.814097\ttraining's ndcg@5: 0.840612\tvalid_1's ndcg@1: 0.472192\tvalid_1's ndcg@2: 0.535184\tvalid_1's ndcg@3: 0.60053\tvalid_1's ndcg@4: 0.662087\tvalid_1's ndcg@5: 0.705758\n",
      "[8]\ttraining's ndcg@1: 0.736953\ttraining's ndcg@2: 0.763696\ttraining's ndcg@3: 0.791711\ttraining's ndcg@4: 0.820931\ttraining's ndcg@5: 0.84349\tvalid_1's ndcg@1: 0.4718\tvalid_1's ndcg@2: 0.534349\tvalid_1's ndcg@3: 0.598155\tvalid_1's ndcg@4: 0.663721\tvalid_1's ndcg@5: 0.70816\n",
      "[9]\ttraining's ndcg@1: 0.754693\ttraining's ndcg@2: 0.77442\ttraining's ndcg@3: 0.802792\ttraining's ndcg@4: 0.829661\ttraining's ndcg@5: 0.850568\tvalid_1's ndcg@1: 0.497461\tvalid_1's ndcg@2: 0.54397\tvalid_1's ndcg@3: 0.610412\tvalid_1's ndcg@4: 0.67272\tvalid_1's ndcg@5: 0.713442\n",
      "[10]\ttraining's ndcg@1: 0.777155\ttraining's ndcg@2: 0.786242\ttraining's ndcg@3: 0.814843\ttraining's ndcg@4: 0.837811\ttraining's ndcg@5: 0.85906\tvalid_1's ndcg@1: 0.504339\tvalid_1's ndcg@2: 0.55239\tvalid_1's ndcg@3: 0.616263\tvalid_1's ndcg@4: 0.671501\tvalid_1's ndcg@5: 0.714721\n",
      "[11]\ttraining's ndcg@1: 0.787159\ttraining's ndcg@2: 0.796319\ttraining's ndcg@3: 0.822326\ttraining's ndcg@4: 0.844899\ttraining's ndcg@5: 0.864134\tvalid_1's ndcg@1: 0.504802\tvalid_1's ndcg@2: 0.548122\tvalid_1's ndcg@3: 0.615058\tvalid_1's ndcg@4: 0.671624\tvalid_1's ndcg@5: 0.716637\n",
      "[12]\ttraining's ndcg@1: 0.788048\ttraining's ndcg@2: 0.802905\ttraining's ndcg@3: 0.826044\ttraining's ndcg@4: 0.847084\ttraining's ndcg@5: 0.866775\tvalid_1's ndcg@1: 0.497071\tvalid_1's ndcg@2: 0.550672\tvalid_1's ndcg@3: 0.613874\tvalid_1's ndcg@4: 0.672425\tvalid_1's ndcg@5: 0.715055\n",
      "[13]\ttraining's ndcg@1: 0.798769\ttraining's ndcg@2: 0.809091\ttraining's ndcg@3: 0.831977\ttraining's ndcg@4: 0.850463\ttraining's ndcg@5: 0.870468\tvalid_1's ndcg@1: 0.510622\tvalid_1's ndcg@2: 0.559257\tvalid_1's ndcg@3: 0.626006\tvalid_1's ndcg@4: 0.67985\tvalid_1's ndcg@5: 0.721574\n",
      "[14]\ttraining's ndcg@1: 0.811691\ttraining's ndcg@2: 0.811973\ttraining's ndcg@3: 0.833677\ttraining's ndcg@4: 0.855674\ttraining's ndcg@5: 0.874073\tvalid_1's ndcg@1: 0.508554\tvalid_1's ndcg@2: 0.564237\tvalid_1's ndcg@3: 0.627847\tvalid_1's ndcg@4: 0.677785\tvalid_1's ndcg@5: 0.725976\n",
      "[15]\ttraining's ndcg@1: 0.82188\ttraining's ndcg@2: 0.820853\ttraining's ndcg@3: 0.84029\ttraining's ndcg@4: 0.861427\ttraining's ndcg@5: 0.879186\tvalid_1's ndcg@1: 0.501793\tvalid_1's ndcg@2: 0.560866\tvalid_1's ndcg@3: 0.6263\tvalid_1's ndcg@4: 0.676554\tvalid_1's ndcg@5: 0.725834\n",
      "[16]\ttraining's ndcg@1: 0.829123\ttraining's ndcg@2: 0.826779\ttraining's ndcg@3: 0.844361\ttraining's ndcg@4: 0.864643\ttraining's ndcg@5: 0.881771\tvalid_1's ndcg@1: 0.515227\tvalid_1's ndcg@2: 0.56558\tvalid_1's ndcg@3: 0.627369\tvalid_1's ndcg@4: 0.681721\tvalid_1's ndcg@5: 0.732051\n",
      "[17]\ttraining's ndcg@1: 0.833208\ttraining's ndcg@2: 0.832162\ttraining's ndcg@3: 0.847014\ttraining's ndcg@4: 0.867451\ttraining's ndcg@5: 0.884533\tvalid_1's ndcg@1: 0.494264\tvalid_1's ndcg@2: 0.573786\tvalid_1's ndcg@3: 0.623283\tvalid_1's ndcg@4: 0.67829\tvalid_1's ndcg@5: 0.726079\n",
      "[18]\ttraining's ndcg@1: 0.838806\ttraining's ndcg@2: 0.837957\ttraining's ndcg@3: 0.853082\ttraining's ndcg@4: 0.870244\ttraining's ndcg@5: 0.888167\tvalid_1's ndcg@1: 0.506614\tvalid_1's ndcg@2: 0.557532\tvalid_1's ndcg@3: 0.620359\tvalid_1's ndcg@4: 0.67834\tvalid_1's ndcg@5: 0.727948\n",
      "[19]\ttraining's ndcg@1: 0.838273\ttraining's ndcg@2: 0.842432\ttraining's ndcg@3: 0.855837\ttraining's ndcg@4: 0.872308\ttraining's ndcg@5: 0.889278\tvalid_1's ndcg@1: 0.509684\tvalid_1's ndcg@2: 0.560116\tvalid_1's ndcg@3: 0.627448\tvalid_1's ndcg@4: 0.676815\tvalid_1's ndcg@5: 0.729569\n",
      "[20]\ttraining's ndcg@1: 0.844405\ttraining's ndcg@2: 0.846452\ttraining's ndcg@3: 0.859395\ttraining's ndcg@4: 0.875329\ttraining's ndcg@5: 0.891612\tvalid_1's ndcg@1: 0.509138\tvalid_1's ndcg@2: 0.560341\tvalid_1's ndcg@3: 0.62291\tvalid_1's ndcg@4: 0.67775\tvalid_1's ndcg@5: 0.72725\n",
      "[21]\ttraining's ndcg@1: 0.858037\ttraining's ndcg@2: 0.853452\ttraining's ndcg@3: 0.864968\ttraining's ndcg@4: 0.881412\ttraining's ndcg@5: 0.897929\tvalid_1's ndcg@1: 0.515423\tvalid_1's ndcg@2: 0.564314\tvalid_1's ndcg@3: 0.632272\tvalid_1's ndcg@4: 0.681668\tvalid_1's ndcg@5: 0.73112\n",
      "[22]\ttraining's ndcg@1: 0.863723\ttraining's ndcg@2: 0.854794\ttraining's ndcg@3: 0.866455\ttraining's ndcg@4: 0.883234\ttraining's ndcg@5: 0.898886\tvalid_1's ndcg@1: 0.506896\tvalid_1's ndcg@2: 0.564638\tvalid_1's ndcg@3: 0.63117\tvalid_1's ndcg@4: 0.679483\tvalid_1's ndcg@5: 0.729494\n",
      "[23]\ttraining's ndcg@1: 0.870745\ttraining's ndcg@2: 0.858431\ttraining's ndcg@3: 0.870157\ttraining's ndcg@4: 0.887311\ttraining's ndcg@5: 0.902462\tvalid_1's ndcg@1: 0.501531\tvalid_1's ndcg@2: 0.557991\tvalid_1's ndcg@3: 0.636989\tvalid_1's ndcg@4: 0.6855\tvalid_1's ndcg@5: 0.72971\n",
      "[24]\ttraining's ndcg@1: 0.873057\ttraining's ndcg@2: 0.861059\ttraining's ndcg@3: 0.872793\ttraining's ndcg@4: 0.888632\ttraining's ndcg@5: 0.903846\tvalid_1's ndcg@1: 0.489842\tvalid_1's ndcg@2: 0.571468\tvalid_1's ndcg@3: 0.640098\tvalid_1's ndcg@4: 0.68705\tvalid_1's ndcg@5: 0.726616\n",
      "[25]\ttraining's ndcg@1: 0.873945\ttraining's ndcg@2: 0.864615\ttraining's ndcg@3: 0.873967\ttraining's ndcg@4: 0.889055\ttraining's ndcg@5: 0.905615\tvalid_1's ndcg@1: 0.51447\tvalid_1's ndcg@2: 0.564902\tvalid_1's ndcg@3: 0.648163\tvalid_1's ndcg@4: 0.689067\tvalid_1's ndcg@5: 0.7309\n",
      "[26]\ttraining's ndcg@1: 0.879988\ttraining's ndcg@2: 0.867688\ttraining's ndcg@3: 0.877222\ttraining's ndcg@4: 0.8917\ttraining's ndcg@5: 0.907687\tvalid_1's ndcg@1: 0.516811\tvalid_1's ndcg@2: 0.573952\tvalid_1's ndcg@3: 0.644882\tvalid_1's ndcg@4: 0.689097\tvalid_1's ndcg@5: 0.730548\n",
      "[27]\ttraining's ndcg@1: 0.887364\ttraining's ndcg@2: 0.874308\ttraining's ndcg@3: 0.881066\ttraining's ndcg@4: 0.895442\ttraining's ndcg@5: 0.910629\tvalid_1's ndcg@1: 0.496706\tvalid_1's ndcg@2: 0.562374\tvalid_1's ndcg@3: 0.638466\tvalid_1's ndcg@4: 0.683636\tvalid_1's ndcg@5: 0.725264\n",
      "[28]\ttraining's ndcg@1: 0.890209\ttraining's ndcg@2: 0.877024\ttraining's ndcg@3: 0.884015\ttraining's ndcg@4: 0.898448\ttraining's ndcg@5: 0.913598\tvalid_1's ndcg@1: 0.495509\tvalid_1's ndcg@2: 0.560264\tvalid_1's ndcg@3: 0.642199\tvalid_1's ndcg@4: 0.686391\tvalid_1's ndcg@5: 0.726099\n",
      "[29]\ttraining's ndcg@1: 0.889498\ttraining's ndcg@2: 0.878935\ttraining's ndcg@3: 0.884727\ttraining's ndcg@4: 0.899425\ttraining's ndcg@5: 0.913851\tvalid_1's ndcg@1: 0.490295\tvalid_1's ndcg@2: 0.55709\tvalid_1's ndcg@3: 0.63413\tvalid_1's ndcg@4: 0.684575\tvalid_1's ndcg@5: 0.723226\n",
      "[30]\ttraining's ndcg@1: 0.892342\ttraining's ndcg@2: 0.883226\ttraining's ndcg@3: 0.887256\ttraining's ndcg@4: 0.900689\ttraining's ndcg@5: 0.915119\tvalid_1's ndcg@1: 0.501549\tvalid_1's ndcg@2: 0.557528\tvalid_1's ndcg@3: 0.637825\tvalid_1's ndcg@4: 0.687555\tvalid_1's ndcg@5: 0.724998\n",
      "[31]\ttraining's ndcg@1: 0.893763\ttraining's ndcg@2: 0.883865\ttraining's ndcg@3: 0.889639\ttraining's ndcg@4: 0.901379\ttraining's ndcg@5: 0.916431\tvalid_1's ndcg@1: 0.517061\tvalid_1's ndcg@2: 0.567272\tvalid_1's ndcg@3: 0.648277\tvalid_1's ndcg@4: 0.692019\tvalid_1's ndcg@5: 0.730356\n",
      "[32]\ttraining's ndcg@1: 0.893763\ttraining's ndcg@2: 0.885421\ttraining's ndcg@3: 0.890198\ttraining's ndcg@4: 0.903071\ttraining's ndcg@5: 0.917148\tvalid_1's ndcg@1: 0.514996\tvalid_1's ndcg@2: 0.577045\tvalid_1's ndcg@3: 0.644158\tvalid_1's ndcg@4: 0.694086\tvalid_1's ndcg@5: 0.731346\n",
      "[33]\ttraining's ndcg@1: 0.894474\ttraining's ndcg@2: 0.888494\ttraining's ndcg@3: 0.89339\ttraining's ndcg@4: 0.904946\ttraining's ndcg@5: 0.918285\tvalid_1's ndcg@1: 0.517119\tvalid_1's ndcg@2: 0.570463\tvalid_1's ndcg@3: 0.642415\tvalid_1's ndcg@4: 0.698259\tvalid_1's ndcg@5: 0.73302\n",
      "[34]\ttraining's ndcg@1: 0.895718\ttraining's ndcg@2: 0.890178\ttraining's ndcg@3: 0.892895\ttraining's ndcg@4: 0.906758\ttraining's ndcg@5: 0.919199\tvalid_1's ndcg@1: 0.521649\tvalid_1's ndcg@2: 0.573151\tvalid_1's ndcg@3: 0.643899\tvalid_1's ndcg@4: 0.699734\tvalid_1's ndcg@5: 0.732748\n",
      "[35]\ttraining's ndcg@1: 0.896429\ttraining's ndcg@2: 0.892148\ttraining's ndcg@3: 0.893844\ttraining's ndcg@4: 0.908059\ttraining's ndcg@5: 0.920284\tvalid_1's ndcg@1: 0.52238\tvalid_1's ndcg@2: 0.572434\tvalid_1's ndcg@3: 0.642296\tvalid_1's ndcg@4: 0.695662\tvalid_1's ndcg@5: 0.735567\n",
      "[36]\ttraining's ndcg@1: 0.89714\ttraining's ndcg@2: 0.893244\ttraining's ndcg@3: 0.895841\ttraining's ndcg@4: 0.90876\ttraining's ndcg@5: 0.921427\tvalid_1's ndcg@1: 0.522789\tvalid_1's ndcg@2: 0.575616\tvalid_1's ndcg@3: 0.649698\tvalid_1's ndcg@4: 0.699066\tvalid_1's ndcg@5: 0.736679\n",
      "[37]\ttraining's ndcg@1: 0.898562\ttraining's ndcg@2: 0.895452\ttraining's ndcg@3: 0.897355\ttraining's ndcg@4: 0.910228\ttraining's ndcg@5: 0.922938\tvalid_1's ndcg@1: 0.518412\tvalid_1's ndcg@2: 0.572891\tvalid_1's ndcg@3: 0.642357\tvalid_1's ndcg@4: 0.69935\tvalid_1's ndcg@5: 0.733645\n",
      "[38]\ttraining's ndcg@1: 0.898562\ttraining's ndcg@2: 0.895628\ttraining's ndcg@3: 0.899152\ttraining's ndcg@4: 0.910556\ttraining's ndcg@5: 0.923089\tvalid_1's ndcg@1: 0.510161\tvalid_1's ndcg@2: 0.571349\tvalid_1's ndcg@3: 0.648089\tvalid_1's ndcg@4: 0.698921\tvalid_1's ndcg@5: 0.733015\n",
      "[39]\ttraining's ndcg@1: 0.901939\ttraining's ndcg@2: 0.897161\ttraining's ndcg@3: 0.900738\ttraining's ndcg@4: 0.912156\ttraining's ndcg@5: 0.924723\tvalid_1's ndcg@1: 0.514579\tvalid_1's ndcg@2: 0.571868\tvalid_1's ndcg@3: 0.647763\tvalid_1's ndcg@4: 0.702775\tvalid_1's ndcg@5: 0.734912\n",
      "[40]\ttraining's ndcg@1: 0.903362\ttraining's ndcg@2: 0.900187\ttraining's ndcg@3: 0.90287\ttraining's ndcg@4: 0.913011\ttraining's ndcg@5: 0.926048\tvalid_1's ndcg@1: 0.521179\tvalid_1's ndcg@2: 0.570073\tvalid_1's ndcg@3: 0.656361\tvalid_1's ndcg@4: 0.705654\tvalid_1's ndcg@5: 0.736213\n",
      "[41]\ttraining's ndcg@1: 0.904073\ttraining's ndcg@2: 0.901341\ttraining's ndcg@3: 0.904445\ttraining's ndcg@4: 0.914756\ttraining's ndcg@5: 0.926642\tvalid_1's ndcg@1: 0.510903\tvalid_1's ndcg@2: 0.564933\tvalid_1's ndcg@3: 0.648869\tvalid_1's ndcg@4: 0.694455\tvalid_1's ndcg@5: 0.732941\n",
      "[42]\ttraining's ndcg@1: 0.908005\ttraining's ndcg@2: 0.902937\ttraining's ndcg@3: 0.906985\ttraining's ndcg@4: 0.915824\ttraining's ndcg@5: 0.928058\tvalid_1's ndcg@1: 0.514217\tvalid_1's ndcg@2: 0.573836\tvalid_1's ndcg@3: 0.648425\tvalid_1's ndcg@4: 0.692352\tvalid_1's ndcg@5: 0.734124\n",
      "[43]\ttraining's ndcg@1: 0.908005\ttraining's ndcg@2: 0.90491\ttraining's ndcg@3: 0.907828\ttraining's ndcg@4: 0.91657\ttraining's ndcg@5: 0.928953\tvalid_1's ndcg@1: 0.512138\tvalid_1's ndcg@2: 0.580533\tvalid_1's ndcg@3: 0.649909\tvalid_1's ndcg@4: 0.69331\tvalid_1's ndcg@5: 0.734482\n",
      "[44]\ttraining's ndcg@1: 0.912986\ttraining's ndcg@2: 0.908025\ttraining's ndcg@3: 0.910883\ttraining's ndcg@4: 0.919257\ttraining's ndcg@5: 0.930205\tvalid_1's ndcg@1: 0.517969\tvalid_1's ndcg@2: 0.588193\tvalid_1's ndcg@3: 0.651984\tvalid_1's ndcg@4: 0.700041\tvalid_1's ndcg@5: 0.738535\n",
      "[45]\ttraining's ndcg@1: 0.914409\ttraining's ndcg@2: 0.91062\ttraining's ndcg@3: 0.912434\ttraining's ndcg@4: 0.919906\ttraining's ndcg@5: 0.930952\tvalid_1's ndcg@1: 0.514952\tvalid_1's ndcg@2: 0.586581\tvalid_1's ndcg@3: 0.654932\tvalid_1's ndcg@4: 0.698016\tvalid_1's ndcg@5: 0.735906\n",
      "[46]\ttraining's ndcg@1: 0.915653\ttraining's ndcg@2: 0.911101\ttraining's ndcg@3: 0.913629\ttraining's ndcg@4: 0.921491\ttraining's ndcg@5: 0.931658\tvalid_1's ndcg@1: 0.512169\tvalid_1's ndcg@2: 0.578046\tvalid_1's ndcg@3: 0.653726\tvalid_1's ndcg@4: 0.698997\tvalid_1's ndcg@5: 0.734359\n",
      "[47]\ttraining's ndcg@1: 0.91743\ttraining's ndcg@2: 0.912799\ttraining's ndcg@3: 0.915314\ttraining's ndcg@4: 0.922606\ttraining's ndcg@5: 0.932749\tvalid_1's ndcg@1: 0.512733\tvalid_1's ndcg@2: 0.579111\tvalid_1's ndcg@3: 0.656491\tvalid_1's ndcg@4: 0.698839\tvalid_1's ndcg@5: 0.734364\n",
      "[48]\ttraining's ndcg@1: 0.919208\ttraining's ndcg@2: 0.913685\ttraining's ndcg@3: 0.915897\ttraining's ndcg@4: 0.922754\ttraining's ndcg@5: 0.932734\tvalid_1's ndcg@1: 0.504952\tvalid_1's ndcg@2: 0.569255\tvalid_1's ndcg@3: 0.653508\tvalid_1's ndcg@4: 0.695885\tvalid_1's ndcg@5: 0.731718\n",
      "[49]\ttraining's ndcg@1: 0.922051\ttraining's ndcg@2: 0.916387\ttraining's ndcg@3: 0.917642\ttraining's ndcg@4: 0.923642\ttraining's ndcg@5: 0.933827\tvalid_1's ndcg@1: 0.507252\tvalid_1's ndcg@2: 0.575079\tvalid_1's ndcg@3: 0.65224\tvalid_1's ndcg@4: 0.696995\tvalid_1's ndcg@5: 0.734665\n",
      "[50]\ttraining's ndcg@1: 0.922051\ttraining's ndcg@2: 0.917081\ttraining's ndcg@3: 0.918595\ttraining's ndcg@4: 0.925716\ttraining's ndcg@5: 0.934352\tvalid_1's ndcg@1: 0.518919\tvalid_1's ndcg@2: 0.579745\tvalid_1's ndcg@3: 0.655165\tvalid_1's ndcg@4: 0.703485\tvalid_1's ndcg@5: 0.738317\n",
      "[51]\ttraining's ndcg@1: 0.924195\ttraining's ndcg@2: 0.919474\ttraining's ndcg@3: 0.919723\ttraining's ndcg@4: 0.926876\ttraining's ndcg@5: 0.935367\tvalid_1's ndcg@1: 0.51397\tvalid_1's ndcg@2: 0.581366\tvalid_1's ndcg@3: 0.651589\tvalid_1's ndcg@4: 0.70258\tvalid_1's ndcg@5: 0.738416\n",
      "[52]\ttraining's ndcg@1: 0.925972\ttraining's ndcg@2: 0.920911\ttraining's ndcg@3: 0.921443\ttraining's ndcg@4: 0.927751\ttraining's ndcg@5: 0.93633\tvalid_1's ndcg@1: 0.52992\tvalid_1's ndcg@2: 0.585974\tvalid_1's ndcg@3: 0.657563\tvalid_1's ndcg@4: 0.704946\tvalid_1's ndcg@5: 0.742472\n",
      "[53]\ttraining's ndcg@1: 0.925972\ttraining's ndcg@2: 0.921689\ttraining's ndcg@3: 0.922071\ttraining's ndcg@4: 0.928304\ttraining's ndcg@5: 0.936945\tvalid_1's ndcg@1: 0.522895\tvalid_1's ndcg@2: 0.583706\tvalid_1's ndcg@3: 0.65417\tvalid_1's ndcg@4: 0.703357\tvalid_1's ndcg@5: 0.740723\n",
      "[54]\ttraining's ndcg@1: 0.928461\ttraining's ndcg@2: 0.92373\ttraining's ndcg@3: 0.923351\ttraining's ndcg@4: 0.929157\ttraining's ndcg@5: 0.938423\tvalid_1's ndcg@1: 0.532845\tvalid_1's ndcg@2: 0.585448\tvalid_1's ndcg@3: 0.653078\tvalid_1's ndcg@4: 0.70726\tvalid_1's ndcg@5: 0.74517\n",
      "[55]\ttraining's ndcg@1: 0.929883\ttraining's ndcg@2: 0.925621\ttraining's ndcg@3: 0.924279\ttraining's ndcg@4: 0.929902\ttraining's ndcg@5: 0.939149\tvalid_1's ndcg@1: 0.519005\tvalid_1's ndcg@2: 0.573721\tvalid_1's ndcg@3: 0.648044\tvalid_1's ndcg@4: 0.699714\tvalid_1's ndcg@5: 0.739172\n",
      "[56]\ttraining's ndcg@1: 0.932016\ttraining's ndcg@2: 0.92767\ttraining's ndcg@3: 0.925328\ttraining's ndcg@4: 0.931692\ttraining's ndcg@5: 0.940601\tvalid_1's ndcg@1: 0.51521\tvalid_1's ndcg@2: 0.573371\tvalid_1's ndcg@3: 0.642654\tvalid_1's ndcg@4: 0.700496\tvalid_1's ndcg@5: 0.735678\n",
      "[57]\ttraining's ndcg@1: 0.935574\ttraining's ndcg@2: 0.929266\ttraining's ndcg@3: 0.926612\ttraining's ndcg@4: 0.932823\ttraining's ndcg@5: 0.94115\tvalid_1's ndcg@1: 0.516624\tvalid_1's ndcg@2: 0.567024\tvalid_1's ndcg@3: 0.644632\tvalid_1's ndcg@4: 0.700683\tvalid_1's ndcg@5: 0.737807\n",
      "[58]\ttraining's ndcg@1: 0.936285\ttraining's ndcg@2: 0.931182\ttraining's ndcg@3: 0.928068\ttraining's ndcg@4: 0.934266\ttraining's ndcg@5: 0.942222\tvalid_1's ndcg@1: 0.51309\tvalid_1's ndcg@2: 0.56582\tvalid_1's ndcg@3: 0.643744\tvalid_1's ndcg@4: 0.697691\tvalid_1's ndcg@5: 0.737748\n",
      "[59]\ttraining's ndcg@1: 0.936285\ttraining's ndcg@2: 0.931843\ttraining's ndcg@3: 0.929082\ttraining's ndcg@4: 0.935006\ttraining's ndcg@5: 0.943117\tvalid_1's ndcg@1: 0.510876\tvalid_1's ndcg@2: 0.566878\tvalid_1's ndcg@3: 0.644593\tvalid_1's ndcg@4: 0.699759\tvalid_1's ndcg@5: 0.734789\n",
      "[60]\ttraining's ndcg@1: 0.938063\ttraining's ndcg@2: 0.932204\ttraining's ndcg@3: 0.930025\ttraining's ndcg@4: 0.935584\ttraining's ndcg@5: 0.943568\tvalid_1's ndcg@1: 0.515565\tvalid_1's ndcg@2: 0.565682\tvalid_1's ndcg@3: 0.64342\tvalid_1's ndcg@4: 0.699456\tvalid_1's ndcg@5: 0.735809\n",
      "[61]\ttraining's ndcg@1: 0.938063\ttraining's ndcg@2: 0.933205\ttraining's ndcg@3: 0.931029\ttraining's ndcg@4: 0.936626\ttraining's ndcg@5: 0.943831\tvalid_1's ndcg@1: 0.507844\tvalid_1's ndcg@2: 0.569751\tvalid_1's ndcg@3: 0.636555\tvalid_1's ndcg@4: 0.697882\tvalid_1's ndcg@5: 0.733524\n",
      "[62]\ttraining's ndcg@1: 0.93913\ttraining's ndcg@2: 0.933462\ttraining's ndcg@3: 0.931681\ttraining's ndcg@4: 0.937083\ttraining's ndcg@5: 0.944358\tvalid_1's ndcg@1: 0.519287\tvalid_1's ndcg@2: 0.566521\tvalid_1's ndcg@3: 0.640649\tvalid_1's ndcg@4: 0.697987\tvalid_1's ndcg@5: 0.734949\n",
      "[63]\ttraining's ndcg@1: 0.939841\ttraining's ndcg@2: 0.933587\ttraining's ndcg@3: 0.932236\ttraining's ndcg@4: 0.937144\ttraining's ndcg@5: 0.944668\tvalid_1's ndcg@1: 0.521408\tvalid_1's ndcg@2: 0.57395\tvalid_1's ndcg@3: 0.651007\tvalid_1's ndcg@4: 0.70012\tvalid_1's ndcg@5: 0.735147\n",
      "[64]\ttraining's ndcg@1: 0.939841\ttraining's ndcg@2: 0.933843\ttraining's ndcg@3: 0.932557\ttraining's ndcg@4: 0.937855\ttraining's ndcg@5: 0.944858\tvalid_1's ndcg@1: 0.525894\tvalid_1's ndcg@2: 0.580426\tvalid_1's ndcg@3: 0.649921\tvalid_1's ndcg@4: 0.702917\tvalid_1's ndcg@5: 0.738258\n",
      "[65]\ttraining's ndcg@1: 0.940552\ttraining's ndcg@2: 0.934685\ttraining's ndcg@3: 0.934314\ttraining's ndcg@4: 0.93808\ttraining's ndcg@5: 0.946315\tvalid_1's ndcg@1: 0.518515\tvalid_1's ndcg@2: 0.570861\tvalid_1's ndcg@3: 0.646423\tvalid_1's ndcg@4: 0.701273\tvalid_1's ndcg@5: 0.737256\n",
      "[66]\ttraining's ndcg@1: 0.940552\ttraining's ndcg@2: 0.93526\ttraining's ndcg@3: 0.934998\ttraining's ndcg@4: 0.938673\ttraining's ndcg@5: 0.946377\tvalid_1's ndcg@1: 0.525761\tvalid_1's ndcg@2: 0.578441\tvalid_1's ndcg@3: 0.650935\tvalid_1's ndcg@4: 0.705273\tvalid_1's ndcg@5: 0.740821\n",
      "[67]\ttraining's ndcg@1: 0.940552\ttraining's ndcg@2: 0.936039\ttraining's ndcg@3: 0.935708\ttraining's ndcg@4: 0.938788\ttraining's ndcg@5: 0.946655\tvalid_1's ndcg@1: 0.526365\tvalid_1's ndcg@2: 0.576611\tvalid_1's ndcg@3: 0.654468\tvalid_1's ndcg@4: 0.702186\tvalid_1's ndcg@5: 0.740543\n",
      "[68]\ttraining's ndcg@1: 0.941973\ttraining's ndcg@2: 0.937448\ttraining's ndcg@3: 0.936562\ttraining's ndcg@4: 0.939672\ttraining's ndcg@5: 0.947405\tvalid_1's ndcg@1: 0.528942\tvalid_1's ndcg@2: 0.57772\tvalid_1's ndcg@3: 0.652853\tvalid_1's ndcg@4: 0.703704\tvalid_1's ndcg@5: 0.742015\n",
      "[69]\ttraining's ndcg@1: 0.944106\ttraining's ndcg@2: 0.938323\ttraining's ndcg@3: 0.937115\ttraining's ndcg@4: 0.940488\ttraining's ndcg@5: 0.947994\tvalid_1's ndcg@1: 0.533984\tvalid_1's ndcg@2: 0.573705\tvalid_1's ndcg@3: 0.64828\tvalid_1's ndcg@4: 0.702541\tvalid_1's ndcg@5: 0.739253\n",
      "[70]\ttraining's ndcg@1: 0.944106\ttraining's ndcg@2: 0.939261\ttraining's ndcg@3: 0.938648\ttraining's ndcg@4: 0.941158\ttraining's ndcg@5: 0.948346\tvalid_1's ndcg@1: 0.54229\tvalid_1's ndcg@2: 0.577395\tvalid_1's ndcg@3: 0.649917\tvalid_1's ndcg@4: 0.704306\tvalid_1's ndcg@5: 0.740584\n",
      "[71]\ttraining's ndcg@1: 0.945529\ttraining's ndcg@2: 0.939874\ttraining's ndcg@3: 0.939271\ttraining's ndcg@4: 0.941691\ttraining's ndcg@5: 0.949039\tvalid_1's ndcg@1: 0.527878\tvalid_1's ndcg@2: 0.568933\tvalid_1's ndcg@3: 0.64669\tvalid_1's ndcg@4: 0.699158\tvalid_1's ndcg@5: 0.736674\n",
      "[72]\ttraining's ndcg@1: 0.946951\ttraining's ndcg@2: 0.941552\ttraining's ndcg@3: 0.940142\ttraining's ndcg@4: 0.942539\ttraining's ndcg@5: 0.949771\tvalid_1's ndcg@1: 0.523641\tvalid_1's ndcg@2: 0.565802\tvalid_1's ndcg@3: 0.64605\tvalid_1's ndcg@4: 0.698516\tvalid_1's ndcg@5: 0.736492\n",
      "[73]\ttraining's ndcg@1: 0.949184\ttraining's ndcg@2: 0.943606\ttraining's ndcg@3: 0.941842\ttraining's ndcg@4: 0.943758\ttraining's ndcg@5: 0.950597\tvalid_1's ndcg@1: 0.532346\tvalid_1's ndcg@2: 0.565061\tvalid_1's ndcg@3: 0.643626\tvalid_1's ndcg@4: 0.699929\tvalid_1's ndcg@5: 0.736361\n",
      "[74]\ttraining's ndcg@1: 0.949184\ttraining's ndcg@2: 0.94497\ttraining's ndcg@3: 0.942618\ttraining's ndcg@4: 0.944098\ttraining's ndcg@5: 0.951175\tvalid_1's ndcg@1: 0.53632\tvalid_1's ndcg@2: 0.568257\tvalid_1's ndcg@3: 0.648056\tvalid_1's ndcg@4: 0.698785\tvalid_1's ndcg@5: 0.735019\n",
      "[75]\ttraining's ndcg@1: 0.950517\ttraining's ndcg@2: 0.94543\ttraining's ndcg@3: 0.94337\ttraining's ndcg@4: 0.944855\ttraining's ndcg@5: 0.951821\tvalid_1's ndcg@1: 0.536325\tvalid_1's ndcg@2: 0.575019\tvalid_1's ndcg@3: 0.651975\tvalid_1's ndcg@4: 0.702367\tvalid_1's ndcg@5: 0.739165\n",
      "[76]\ttraining's ndcg@1: 0.95194\ttraining's ndcg@2: 0.946845\ttraining's ndcg@3: 0.944112\ttraining's ndcg@4: 0.945763\ttraining's ndcg@5: 0.952153\tvalid_1's ndcg@1: 0.531015\tvalid_1's ndcg@2: 0.572473\tvalid_1's ndcg@3: 0.644408\tvalid_1's ndcg@4: 0.697617\tvalid_1's ndcg@5: 0.73876\n",
      "[77]\ttraining's ndcg@1: 0.953717\ttraining's ndcg@2: 0.948205\ttraining's ndcg@3: 0.945178\ttraining's ndcg@4: 0.946658\ttraining's ndcg@5: 0.952924\tvalid_1's ndcg@1: 0.530403\tvalid_1's ndcg@2: 0.56679\tvalid_1's ndcg@3: 0.641427\tvalid_1's ndcg@4: 0.697037\tvalid_1's ndcg@5: 0.735784\n",
      "[78]\ttraining's ndcg@1: 0.955139\ttraining's ndcg@2: 0.948966\ttraining's ndcg@3: 0.945623\ttraining's ndcg@4: 0.948315\ttraining's ndcg@5: 0.953544\tvalid_1's ndcg@1: 0.533363\tvalid_1's ndcg@2: 0.570207\tvalid_1's ndcg@3: 0.64151\tvalid_1's ndcg@4: 0.696913\tvalid_1's ndcg@5: 0.739425\n",
      "[79]\ttraining's ndcg@1: 0.95585\ttraining's ndcg@2: 0.949507\ttraining's ndcg@3: 0.946291\ttraining's ndcg@4: 0.948556\ttraining's ndcg@5: 0.953963\tvalid_1's ndcg@1: 0.520015\tvalid_1's ndcg@2: 0.577689\tvalid_1's ndcg@3: 0.640539\tvalid_1's ndcg@4: 0.694129\tvalid_1's ndcg@5: 0.733503\n",
      "[80]\ttraining's ndcg@1: 0.95585\ttraining's ndcg@2: 0.950189\ttraining's ndcg@3: 0.947158\ttraining's ndcg@4: 0.949143\ttraining's ndcg@5: 0.954593\tvalid_1's ndcg@1: 0.520015\tvalid_1's ndcg@2: 0.579321\tvalid_1's ndcg@3: 0.637657\tvalid_1's ndcg@4: 0.693171\tvalid_1's ndcg@5: 0.734955\n",
      "[81]\ttraining's ndcg@1: 0.956561\ttraining's ndcg@2: 0.950218\ttraining's ndcg@3: 0.947593\ttraining's ndcg@4: 0.949483\ttraining's ndcg@5: 0.954725\tvalid_1's ndcg@1: 0.519308\tvalid_1's ndcg@2: 0.579254\tvalid_1's ndcg@3: 0.642153\tvalid_1's ndcg@4: 0.693614\tvalid_1's ndcg@5: 0.732363\n",
      "[82]\ttraining's ndcg@1: 0.957272\ttraining's ndcg@2: 0.951196\ttraining's ndcg@3: 0.948046\ttraining's ndcg@4: 0.950228\ttraining's ndcg@5: 0.955426\tvalid_1's ndcg@1: 0.519838\tvalid_1's ndcg@2: 0.584522\tvalid_1's ndcg@3: 0.643105\tvalid_1's ndcg@4: 0.693465\tvalid_1's ndcg@5: 0.732878\n",
      "[83]\ttraining's ndcg@1: 0.957272\ttraining's ndcg@2: 0.951188\ttraining's ndcg@3: 0.948245\ttraining's ndcg@4: 0.950154\ttraining's ndcg@5: 0.955485\tvalid_1's ndcg@1: 0.518483\tvalid_1's ndcg@2: 0.584994\tvalid_1's ndcg@3: 0.641212\tvalid_1's ndcg@4: 0.694666\tvalid_1's ndcg@5: 0.735573\n",
      "[84]\ttraining's ndcg@1: 0.957983\ttraining's ndcg@2: 0.951558\ttraining's ndcg@3: 0.949324\ttraining's ndcg@4: 0.950608\ttraining's ndcg@5: 0.956168\tvalid_1's ndcg@1: 0.516009\tvalid_1's ndcg@2: 0.585788\tvalid_1's ndcg@3: 0.638097\tvalid_1's ndcg@4: 0.693904\tvalid_1's ndcg@5: 0.736128\n",
      "[85]\ttraining's ndcg@1: 0.958694\ttraining's ndcg@2: 0.952088\ttraining's ndcg@3: 0.949666\ttraining's ndcg@4: 0.951177\ttraining's ndcg@5: 0.956489\tvalid_1's ndcg@1: 0.513181\tvalid_1's ndcg@2: 0.577107\tvalid_1's ndcg@3: 0.638078\tvalid_1's ndcg@4: 0.691949\tvalid_1's ndcg@5: 0.733623\n",
      "[86]\ttraining's ndcg@1: 0.958694\ttraining's ndcg@2: 0.952983\ttraining's ndcg@3: 0.950067\ttraining's ndcg@4: 0.951437\ttraining's ndcg@5: 0.956751\tvalid_1's ndcg@1: 0.524162\tvalid_1's ndcg@2: 0.581242\tvalid_1's ndcg@3: 0.642783\tvalid_1's ndcg@4: 0.693581\tvalid_1's ndcg@5: 0.733957\n",
      "[87]\ttraining's ndcg@1: 0.958694\ttraining's ndcg@2: 0.953569\ttraining's ndcg@3: 0.950012\ttraining's ndcg@4: 0.95154\ttraining's ndcg@5: 0.957431\tvalid_1's ndcg@1: 0.523105\tvalid_1's ndcg@2: 0.581114\tvalid_1's ndcg@3: 0.64328\tvalid_1's ndcg@4: 0.695334\tvalid_1's ndcg@5: 0.736349\n",
      "[88]\ttraining's ndcg@1: 0.958694\ttraining's ndcg@2: 0.95374\ttraining's ndcg@3: 0.950229\ttraining's ndcg@4: 0.952044\ttraining's ndcg@5: 0.957822\tvalid_1's ndcg@1: 0.521274\tvalid_1's ndcg@2: 0.579406\tvalid_1's ndcg@3: 0.642508\tvalid_1's ndcg@4: 0.695306\tvalid_1's ndcg@5: 0.732961\n",
      "[89]\ttraining's ndcg@1: 0.958694\ttraining's ndcg@2: 0.953996\ttraining's ndcg@3: 0.950517\ttraining's ndcg@4: 0.952005\ttraining's ndcg@5: 0.957849\tvalid_1's ndcg@1: 0.525579\tvalid_1's ndcg@2: 0.58096\tvalid_1's ndcg@3: 0.644627\tvalid_1's ndcg@4: 0.696927\tvalid_1's ndcg@5: 0.733955\n",
      "[90]\ttraining's ndcg@1: 0.959405\ttraining's ndcg@2: 0.954963\ttraining's ndcg@3: 0.951197\ttraining's ndcg@4: 0.952633\ttraining's ndcg@5: 0.958478\tvalid_1's ndcg@1: 0.531588\tvalid_1's ndcg@2: 0.578495\tvalid_1's ndcg@3: 0.645379\tvalid_1's ndcg@4: 0.697753\tvalid_1's ndcg@5: 0.738618\n",
      "[91]\ttraining's ndcg@1: 0.959405\ttraining's ndcg@2: 0.95556\ttraining's ndcg@3: 0.952035\ttraining's ndcg@4: 0.952961\ttraining's ndcg@5: 0.958664\tvalid_1's ndcg@1: 0.52575\tvalid_1's ndcg@2: 0.584074\tvalid_1's ndcg@3: 0.64652\tvalid_1's ndcg@4: 0.698868\tvalid_1's ndcg@5: 0.740101\n",
      "[92]\ttraining's ndcg@1: 0.960116\ttraining's ndcg@2: 0.956527\ttraining's ndcg@3: 0.952401\ttraining's ndcg@4: 0.954294\ttraining's ndcg@5: 0.959072\tvalid_1's ndcg@1: 0.512626\tvalid_1's ndcg@2: 0.573692\tvalid_1's ndcg@3: 0.641757\tvalid_1's ndcg@4: 0.693926\tvalid_1's ndcg@5: 0.732076\n",
      "[93]\ttraining's ndcg@1: 0.960827\ttraining's ndcg@2: 0.957417\ttraining's ndcg@3: 0.953593\ttraining's ndcg@4: 0.954697\ttraining's ndcg@5: 0.959705\tvalid_1's ndcg@1: 0.507305\tvalid_1's ndcg@2: 0.569649\tvalid_1's ndcg@3: 0.637235\tvalid_1's ndcg@4: 0.689183\tvalid_1's ndcg@5: 0.729527\n",
      "[94]\ttraining's ndcg@1: 0.961538\ttraining's ndcg@2: 0.957936\ttraining's ndcg@3: 0.953752\ttraining's ndcg@4: 0.955055\ttraining's ndcg@5: 0.960007\tvalid_1's ndcg@1: 0.51826\tvalid_1's ndcg@2: 0.580149\tvalid_1's ndcg@3: 0.642252\tvalid_1's ndcg@4: 0.695971\tvalid_1's ndcg@5: 0.733133\n",
      "[95]\ttraining's ndcg@1: 0.962249\ttraining's ndcg@2: 0.958136\ttraining's ndcg@3: 0.954945\ttraining's ndcg@4: 0.955915\ttraining's ndcg@5: 0.960385\tvalid_1's ndcg@1: 0.517332\tvalid_1's ndcg@2: 0.57006\tvalid_1's ndcg@3: 0.642855\tvalid_1's ndcg@4: 0.693605\tvalid_1's ndcg@5: 0.73023\n",
      "[96]\ttraining's ndcg@1: 0.96296\ttraining's ndcg@2: 0.958591\ttraining's ndcg@3: 0.955547\ttraining's ndcg@4: 0.956534\ttraining's ndcg@5: 0.960621\tvalid_1's ndcg@1: 0.501677\tvalid_1's ndcg@2: 0.568798\tvalid_1's ndcg@3: 0.643085\tvalid_1's ndcg@4: 0.692042\tvalid_1's ndcg@5: 0.727958\n",
      "[97]\ttraining's ndcg@1: 0.963316\ttraining's ndcg@2: 0.958776\ttraining's ndcg@3: 0.955654\ttraining's ndcg@4: 0.956729\ttraining's ndcg@5: 0.960993\tvalid_1's ndcg@1: 0.513832\tvalid_1's ndcg@2: 0.577553\tvalid_1's ndcg@3: 0.644307\tvalid_1's ndcg@4: 0.694413\tvalid_1's ndcg@5: 0.73185\n",
      "[98]\ttraining's ndcg@1: 0.964027\ttraining's ndcg@2: 0.959744\ttraining's ndcg@3: 0.956121\ttraining's ndcg@4: 0.957627\ttraining's ndcg@5: 0.961833\tvalid_1's ndcg@1: 0.513132\tvalid_1's ndcg@2: 0.580042\tvalid_1's ndcg@3: 0.645975\tvalid_1's ndcg@4: 0.695166\tvalid_1's ndcg@5: 0.731386\n",
      "[99]\ttraining's ndcg@1: 0.964027\ttraining's ndcg@2: 0.959945\ttraining's ndcg@3: 0.956893\ttraining's ndcg@4: 0.958239\ttraining's ndcg@5: 0.962236\tvalid_1's ndcg@1: 0.511804\tvalid_1's ndcg@2: 0.580045\tvalid_1's ndcg@3: 0.650831\tvalid_1's ndcg@4: 0.698166\tvalid_1's ndcg@5: 0.733576\n",
      "[100]\ttraining's ndcg@1: 0.965449\ttraining's ndcg@2: 0.960856\ttraining's ndcg@3: 0.957425\ttraining's ndcg@4: 0.958669\ttraining's ndcg@5: 0.962817\tvalid_1's ndcg@1: 0.508811\tvalid_1's ndcg@2: 0.573311\tvalid_1's ndcg@3: 0.647583\tvalid_1's ndcg@4: 0.698766\tvalid_1's ndcg@5: 0.733488\n",
      "[101]\ttraining's ndcg@1: 0.96616\ttraining's ndcg@2: 0.961397\ttraining's ndcg@3: 0.958158\ttraining's ndcg@4: 0.959155\ttraining's ndcg@5: 0.963108\tvalid_1's ndcg@1: 0.520127\tvalid_1's ndcg@2: 0.575169\tvalid_1's ndcg@3: 0.6493\tvalid_1's ndcg@4: 0.698179\tvalid_1's ndcg@5: 0.734263\n",
      "[102]\ttraining's ndcg@1: 0.966871\ttraining's ndcg@2: 0.962662\ttraining's ndcg@3: 0.958735\ttraining's ndcg@4: 0.959712\ttraining's ndcg@5: 0.963717\tvalid_1's ndcg@1: 0.516062\tvalid_1's ndcg@2: 0.574452\tvalid_1's ndcg@3: 0.647849\tvalid_1's ndcg@4: 0.697633\tvalid_1's ndcg@5: 0.733467\n",
      "[103]\ttraining's ndcg@1: 0.967582\ttraining's ndcg@2: 0.963373\ttraining's ndcg@3: 0.959971\ttraining's ndcg@4: 0.960135\ttraining's ndcg@5: 0.964219\tvalid_1's ndcg@1: 0.507224\tvalid_1's ndcg@2: 0.572099\tvalid_1's ndcg@3: 0.646897\tvalid_1's ndcg@4: 0.696613\tvalid_1's ndcg@5: 0.733496\n",
      "[104]\ttraining's ndcg@1: 0.968294\ttraining's ndcg@2: 0.964249\ttraining's ndcg@3: 0.960128\ttraining's ndcg@4: 0.960383\ttraining's ndcg@5: 0.964819\tvalid_1's ndcg@1: 0.51636\tvalid_1's ndcg@2: 0.581978\tvalid_1's ndcg@3: 0.651169\tvalid_1's ndcg@4: 0.700486\tvalid_1's ndcg@5: 0.736102\n",
      "[105]\ttraining's ndcg@1: 0.970427\ttraining's ndcg@2: 0.964848\ttraining's ndcg@3: 0.960651\ttraining's ndcg@4: 0.961037\ttraining's ndcg@5: 0.965401\tvalid_1's ndcg@1: 0.510856\tvalid_1's ndcg@2: 0.577677\tvalid_1's ndcg@3: 0.648719\tvalid_1's ndcg@4: 0.698296\tvalid_1's ndcg@5: 0.734545\n",
      "[106]\ttraining's ndcg@1: 0.970427\ttraining's ndcg@2: 0.965019\ttraining's ndcg@3: 0.961322\ttraining's ndcg@4: 0.961735\ttraining's ndcg@5: 0.966008\tvalid_1's ndcg@1: 0.504061\tvalid_1's ndcg@2: 0.575547\tvalid_1's ndcg@3: 0.649447\tvalid_1's ndcg@4: 0.697347\tvalid_1's ndcg@5: 0.734171\n",
      "[107]\ttraining's ndcg@1: 0.97185\ttraining's ndcg@2: 0.965418\ttraining's ndcg@3: 0.961581\ttraining's ndcg@4: 0.962125\ttraining's ndcg@5: 0.966364\tvalid_1's ndcg@1: 0.508067\tvalid_1's ndcg@2: 0.575608\tvalid_1's ndcg@3: 0.649223\tvalid_1's ndcg@4: 0.698293\tvalid_1's ndcg@5: 0.733909\n",
      "[108]\ttraining's ndcg@1: 0.973272\ttraining's ndcg@2: 0.96684\ttraining's ndcg@3: 0.962192\ttraining's ndcg@4: 0.962802\ttraining's ndcg@5: 0.967001\tvalid_1's ndcg@1: 0.502295\tvalid_1's ndcg@2: 0.572889\tvalid_1's ndcg@3: 0.648509\tvalid_1's ndcg@4: 0.698716\tvalid_1's ndcg@5: 0.732617\n",
      "[109]\ttraining's ndcg@1: 0.973272\ttraining's ndcg@2: 0.967096\ttraining's ndcg@3: 0.962989\ttraining's ndcg@4: 0.962983\ttraining's ndcg@5: 0.967242\tvalid_1's ndcg@1: 0.505653\tvalid_1's ndcg@2: 0.576845\tvalid_1's ndcg@3: 0.648621\tvalid_1's ndcg@4: 0.699524\tvalid_1's ndcg@5: 0.735364\n",
      "[110]\ttraining's ndcg@1: 0.973272\ttraining's ndcg@2: 0.967352\ttraining's ndcg@3: 0.963099\ttraining's ndcg@4: 0.963491\ttraining's ndcg@5: 0.967358\tvalid_1's ndcg@1: 0.503376\tvalid_1's ndcg@2: 0.576205\tvalid_1's ndcg@3: 0.646562\tvalid_1's ndcg@4: 0.699138\tvalid_1's ndcg@5: 0.735367\n",
      "[111]\ttraining's ndcg@1: 0.973272\ttraining's ndcg@2: 0.967778\ttraining's ndcg@3: 0.963775\ttraining's ndcg@4: 0.963634\ttraining's ndcg@5: 0.967395\tvalid_1's ndcg@1: 0.50161\tvalid_1's ndcg@2: 0.575592\tvalid_1's ndcg@3: 0.649747\tvalid_1's ndcg@4: 0.698813\tvalid_1's ndcg@5: 0.735452\n",
      "[112]\ttraining's ndcg@1: 0.973272\ttraining's ndcg@2: 0.968119\ttraining's ndcg@3: 0.964344\ttraining's ndcg@4: 0.964117\ttraining's ndcg@5: 0.967779\tvalid_1's ndcg@1: 0.501277\tvalid_1's ndcg@2: 0.572718\tvalid_1's ndcg@3: 0.646392\tvalid_1's ndcg@4: 0.698135\tvalid_1's ndcg@5: 0.73448\n",
      "[113]\ttraining's ndcg@1: 0.97505\ttraining's ndcg@2: 0.969899\ttraining's ndcg@3: 0.965043\ttraining's ndcg@4: 0.964944\ttraining's ndcg@5: 0.968652\tvalid_1's ndcg@1: 0.498781\tvalid_1's ndcg@2: 0.574175\tvalid_1's ndcg@3: 0.644386\tvalid_1's ndcg@4: 0.695453\tvalid_1's ndcg@5: 0.733929\n",
      "[114]\ttraining's ndcg@1: 0.97505\ttraining's ndcg@2: 0.970538\ttraining's ndcg@3: 0.965319\ttraining's ndcg@4: 0.965186\ttraining's ndcg@5: 0.969008\tvalid_1's ndcg@1: 0.505148\tvalid_1's ndcg@2: 0.576303\tvalid_1's ndcg@3: 0.644892\tvalid_1's ndcg@4: 0.696972\tvalid_1's ndcg@5: 0.735616\n",
      "[115]\ttraining's ndcg@1: 0.97505\ttraining's ndcg@2: 0.970837\ttraining's ndcg@3: 0.965916\ttraining's ndcg@4: 0.965196\ttraining's ndcg@5: 0.969123\tvalid_1's ndcg@1: 0.501691\tvalid_1's ndcg@2: 0.572286\tvalid_1's ndcg@3: 0.643316\tvalid_1's ndcg@4: 0.697613\tvalid_1's ndcg@5: 0.734505\n",
      "[116]\ttraining's ndcg@1: 0.97505\ttraining's ndcg@2: 0.970837\ttraining's ndcg@3: 0.966372\ttraining's ndcg@4: 0.965698\ttraining's ndcg@5: 0.96924\tvalid_1's ndcg@1: 0.504522\tvalid_1's ndcg@2: 0.573166\tvalid_1's ndcg@3: 0.646942\tvalid_1's ndcg@4: 0.698052\tvalid_1's ndcg@5: 0.734216\n",
      "[117]\ttraining's ndcg@1: 0.975772\ttraining's ndcg@2: 0.97121\ttraining's ndcg@3: 0.966529\ttraining's ndcg@4: 0.965953\ttraining's ndcg@5: 0.969381\tvalid_1's ndcg@1: 0.506899\tvalid_1's ndcg@2: 0.568356\tvalid_1's ndcg@3: 0.641768\tvalid_1's ndcg@4: 0.695701\tvalid_1's ndcg@5: 0.73314\n",
      "[118]\ttraining's ndcg@1: 0.975772\ttraining's ndcg@2: 0.971893\ttraining's ndcg@3: 0.96726\ttraining's ndcg@4: 0.966298\ttraining's ndcg@5: 0.969781\tvalid_1's ndcg@1: 0.516446\tvalid_1's ndcg@2: 0.570993\tvalid_1's ndcg@3: 0.645313\tvalid_1's ndcg@4: 0.696925\tvalid_1's ndcg@5: 0.736126\n",
      "[119]\ttraining's ndcg@1: 0.975772\ttraining's ndcg@2: 0.972063\ttraining's ndcg@3: 0.967362\ttraining's ndcg@4: 0.966316\ttraining's ndcg@5: 0.969862\tvalid_1's ndcg@1: 0.515832\tvalid_1's ndcg@2: 0.572852\tvalid_1's ndcg@3: 0.647401\tvalid_1's ndcg@4: 0.695115\tvalid_1's ndcg@5: 0.736244\n",
      "[120]\ttraining's ndcg@1: 0.975772\ttraining's ndcg@2: 0.972404\ttraining's ndcg@3: 0.967488\ttraining's ndcg@4: 0.966762\ttraining's ndcg@5: 0.970317\tvalid_1's ndcg@1: 0.510199\tvalid_1's ndcg@2: 0.567074\tvalid_1's ndcg@3: 0.644917\tvalid_1's ndcg@4: 0.697049\tvalid_1's ndcg@5: 0.735015\n",
      "[121]\ttraining's ndcg@1: 0.977552\ttraining's ndcg@2: 0.973416\ttraining's ndcg@3: 0.968089\ttraining's ndcg@4: 0.967344\ttraining's ndcg@5: 0.971068\tvalid_1's ndcg@1: 0.510199\tvalid_1's ndcg@2: 0.568176\tvalid_1's ndcg@3: 0.644021\tvalid_1's ndcg@4: 0.697931\tvalid_1's ndcg@5: 0.736053\n",
      "[122]\ttraining's ndcg@1: 0.977552\ttraining's ndcg@2: 0.973587\ttraining's ndcg@3: 0.968299\ttraining's ndcg@4: 0.967396\ttraining's ndcg@5: 0.971116\tvalid_1's ndcg@1: 0.513748\tvalid_1's ndcg@2: 0.566122\tvalid_1's ndcg@3: 0.644684\tvalid_1's ndcg@4: 0.698295\tvalid_1's ndcg@5: 0.736078\n",
      "[123]\ttraining's ndcg@1: 0.977552\ttraining's ndcg@2: 0.974141\ttraining's ndcg@3: 0.968543\ttraining's ndcg@4: 0.967416\ttraining's ndcg@5: 0.971305\tvalid_1's ndcg@1: 0.510741\tvalid_1's ndcg@2: 0.564428\tvalid_1's ndcg@3: 0.64461\tvalid_1's ndcg@4: 0.696746\tvalid_1's ndcg@5: 0.735452\n",
      "[124]\ttraining's ndcg@1: 0.977552\ttraining's ndcg@2: 0.974503\ttraining's ndcg@3: 0.968704\ttraining's ndcg@4: 0.967583\ttraining's ndcg@5: 0.971334\tvalid_1's ndcg@1: 0.521353\tvalid_1's ndcg@2: 0.575039\tvalid_1's ndcg@3: 0.644425\tvalid_1's ndcg@4: 0.701219\tvalid_1's ndcg@5: 0.737911\n",
      "[125]\ttraining's ndcg@1: 0.978975\ttraining's ndcg@2: 0.975073\ttraining's ndcg@3: 0.969348\ttraining's ndcg@4: 0.968206\ttraining's ndcg@5: 0.971716\tvalid_1's ndcg@1: 0.5245\tvalid_1's ndcg@2: 0.570071\tvalid_1's ndcg@3: 0.645442\tvalid_1's ndcg@4: 0.700919\tvalid_1's ndcg@5: 0.737644\n",
      "[126]\ttraining's ndcg@1: 0.978975\ttraining's ndcg@2: 0.975579\ttraining's ndcg@3: 0.969459\ttraining's ndcg@4: 0.968335\ttraining's ndcg@5: 0.971922\tvalid_1's ndcg@1: 0.520279\tvalid_1's ndcg@2: 0.570624\tvalid_1's ndcg@3: 0.644996\tvalid_1's ndcg@4: 0.699265\tvalid_1's ndcg@5: 0.736467\n",
      "[127]\ttraining's ndcg@1: 0.979686\ttraining's ndcg@2: 0.97595\ttraining's ndcg@3: 0.970349\ttraining's ndcg@4: 0.968841\ttraining's ndcg@5: 0.972118\tvalid_1's ndcg@1: 0.520986\tvalid_1's ndcg@2: 0.573875\tvalid_1's ndcg@3: 0.644682\tvalid_1's ndcg@4: 0.698016\tvalid_1's ndcg@5: 0.737213\n",
      "[128]\ttraining's ndcg@1: 0.979686\ttraining's ndcg@2: 0.976121\ttraining's ndcg@3: 0.97032\ttraining's ndcg@4: 0.968973\ttraining's ndcg@5: 0.972194\tvalid_1's ndcg@1: 0.525934\tvalid_1's ndcg@2: 0.573569\tvalid_1's ndcg@3: 0.645846\tvalid_1's ndcg@4: 0.701074\tvalid_1's ndcg@5: 0.737907\n",
      "[129]\ttraining's ndcg@1: 0.980397\ttraining's ndcg@2: 0.97632\ttraining's ndcg@3: 0.971394\ttraining's ndcg@4: 0.969308\ttraining's ndcg@5: 0.972656\tvalid_1's ndcg@1: 0.528389\tvalid_1's ndcg@2: 0.57536\tvalid_1's ndcg@3: 0.64656\tvalid_1's ndcg@4: 0.700438\tvalid_1's ndcg@5: 0.739603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130]\ttraining's ndcg@1: 0.981108\ttraining's ndcg@2: 0.97652\ttraining's ndcg@3: 0.971693\ttraining's ndcg@4: 0.969346\ttraining's ndcg@5: 0.972906\tvalid_1's ndcg@1: 0.516762\tvalid_1's ndcg@2: 0.572434\tvalid_1's ndcg@3: 0.642722\tvalid_1's ndcg@4: 0.700002\tvalid_1's ndcg@5: 0.736619\n",
      "[131]\ttraining's ndcg@1: 0.981108\ttraining's ndcg@2: 0.976691\ttraining's ndcg@3: 0.972283\ttraining's ndcg@4: 0.970182\ttraining's ndcg@5: 0.973017\tvalid_1's ndcg@1: 0.521702\tvalid_1's ndcg@2: 0.572041\tvalid_1's ndcg@3: 0.645492\tvalid_1's ndcg@4: 0.700991\tvalid_1's ndcg@5: 0.737863\n",
      "[132]\ttraining's ndcg@1: 0.981108\ttraining's ndcg@2: 0.976947\ttraining's ndcg@3: 0.972394\ttraining's ndcg@4: 0.970183\ttraining's ndcg@5: 0.973156\tvalid_1's ndcg@1: 0.516726\tvalid_1's ndcg@2: 0.569286\tvalid_1's ndcg@3: 0.64132\tvalid_1's ndcg@4: 0.699733\tvalid_1's ndcg@5: 0.736732\n",
      "[133]\ttraining's ndcg@1: 0.981108\ttraining's ndcg@2: 0.976989\ttraining's ndcg@3: 0.972433\ttraining's ndcg@4: 0.970329\ttraining's ndcg@5: 0.973358\tvalid_1's ndcg@1: 0.516417\tvalid_1's ndcg@2: 0.571234\tvalid_1's ndcg@3: 0.642577\tvalid_1's ndcg@4: 0.702755\tvalid_1's ndcg@5: 0.737391\n",
      "[134]\ttraining's ndcg@1: 0.981108\ttraining's ndcg@2: 0.977501\ttraining's ndcg@3: 0.97256\ttraining's ndcg@4: 0.970453\ttraining's ndcg@5: 0.973556\tvalid_1's ndcg@1: 0.51854\tvalid_1's ndcg@2: 0.569794\tvalid_1's ndcg@3: 0.644945\tvalid_1's ndcg@4: 0.70061\tvalid_1's ndcg@5: 0.737573\n",
      "[135]\ttraining's ndcg@1: 0.981108\ttraining's ndcg@2: 0.97878\ttraining's ndcg@3: 0.972857\ttraining's ndcg@4: 0.970642\ttraining's ndcg@5: 0.973667\tvalid_1's ndcg@1: 0.513591\tvalid_1's ndcg@2: 0.567727\tvalid_1's ndcg@3: 0.643551\tvalid_1's ndcg@4: 0.697929\tvalid_1's ndcg@5: 0.736181\n",
      "[136]\ttraining's ndcg@1: 0.982175\ttraining's ndcg@2: 0.97908\ttraining's ndcg@3: 0.973161\ttraining's ndcg@4: 0.97103\ttraining's ndcg@5: 0.97422\tvalid_1's ndcg@1: 0.516397\tvalid_1's ndcg@2: 0.571301\tvalid_1's ndcg@3: 0.646509\tvalid_1's ndcg@4: 0.701788\tvalid_1's ndcg@5: 0.738188\n",
      "[137]\ttraining's ndcg@1: 0.982175\ttraining's ndcg@2: 0.979072\ttraining's ndcg@3: 0.97319\ttraining's ndcg@4: 0.971212\ttraining's ndcg@5: 0.974289\tvalid_1's ndcg@1: 0.518028\tvalid_1's ndcg@2: 0.573787\tvalid_1's ndcg@3: 0.64595\tvalid_1's ndcg@4: 0.702931\tvalid_1's ndcg@5: 0.739758\n",
      "[138]\ttraining's ndcg@1: 0.982175\ttraining's ndcg@2: 0.979072\ttraining's ndcg@3: 0.973306\ttraining's ndcg@4: 0.971254\ttraining's ndcg@5: 0.974265\tvalid_1's ndcg@1: 0.520192\tvalid_1's ndcg@2: 0.576938\tvalid_1's ndcg@3: 0.64589\tvalid_1's ndcg@4: 0.703427\tvalid_1's ndcg@5: 0.74006\n",
      "[139]\ttraining's ndcg@1: 0.982175\ttraining's ndcg@2: 0.979072\ttraining's ndcg@3: 0.973337\ttraining's ndcg@4: 0.971286\ttraining's ndcg@5: 0.974396\tvalid_1's ndcg@1: 0.521367\tvalid_1's ndcg@2: 0.574223\tvalid_1's ndcg@3: 0.645625\tvalid_1's ndcg@4: 0.702661\tvalid_1's ndcg@5: 0.737449\n",
      "[140]\ttraining's ndcg@1: 0.982886\ttraining's ndcg@2: 0.979783\ttraining's ndcg@3: 0.973682\ttraining's ndcg@4: 0.971641\ttraining's ndcg@5: 0.974733\tvalid_1's ndcg@1: 0.513569\tvalid_1's ndcg@2: 0.570848\tvalid_1's ndcg@3: 0.641775\tvalid_1's ndcg@4: 0.700497\tvalid_1's ndcg@5: 0.735144\n",
      "[141]\ttraining's ndcg@1: 0.984308\ttraining's ndcg@2: 0.980523\ttraining's ndcg@3: 0.974231\ttraining's ndcg@4: 0.97226\ttraining's ndcg@5: 0.975267\tvalid_1's ndcg@1: 0.511449\tvalid_1's ndcg@2: 0.565015\tvalid_1's ndcg@3: 0.638764\tvalid_1's ndcg@4: 0.695866\tvalid_1's ndcg@5: 0.733686\n",
      "[142]\ttraining's ndcg@1: 0.984308\ttraining's ndcg@2: 0.980523\ttraining's ndcg@3: 0.974293\ttraining's ndcg@4: 0.972567\ttraining's ndcg@5: 0.97547\tvalid_1's ndcg@1: 0.514589\tvalid_1's ndcg@2: 0.566257\tvalid_1's ndcg@3: 0.640529\tvalid_1's ndcg@4: 0.698793\tvalid_1's ndcg@5: 0.734695\n",
      "[143]\ttraining's ndcg@1: 0.984308\ttraining's ndcg@2: 0.981035\ttraining's ndcg@3: 0.974533\ttraining's ndcg@4: 0.972641\ttraining's ndcg@5: 0.97569\tvalid_1's ndcg@1: 0.511316\tvalid_1's ndcg@2: 0.568112\tvalid_1's ndcg@3: 0.63959\tvalid_1's ndcg@4: 0.698367\tvalid_1's ndcg@5: 0.734833\n",
      "[144]\ttraining's ndcg@1: 0.98573\ttraining's ndcg@2: 0.981861\ttraining's ndcg@3: 0.975009\ttraining's ndcg@4: 0.973254\ttraining's ndcg@5: 0.976124\tvalid_1's ndcg@1: 0.503629\tvalid_1's ndcg@2: 0.570194\tvalid_1's ndcg@3: 0.641607\tvalid_1's ndcg@4: 0.69963\tvalid_1's ndcg@5: 0.733664\n",
      "[145]\ttraining's ndcg@1: 0.98573\ttraining's ndcg@2: 0.982032\ttraining's ndcg@3: 0.975072\ttraining's ndcg@4: 0.973214\ttraining's ndcg@5: 0.976072\tvalid_1's ndcg@1: 0.504428\tvalid_1's ndcg@2: 0.573309\tvalid_1's ndcg@3: 0.642188\tvalid_1's ndcg@4: 0.69887\tvalid_1's ndcg@5: 0.733689\n",
      "[146]\ttraining's ndcg@1: 0.98573\ttraining's ndcg@2: 0.982032\ttraining's ndcg@3: 0.97498\ttraining's ndcg@4: 0.973276\ttraining's ndcg@5: 0.976155\tvalid_1's ndcg@1: 0.504428\tvalid_1's ndcg@2: 0.569282\tvalid_1's ndcg@3: 0.64124\tvalid_1's ndcg@4: 0.699765\tvalid_1's ndcg@5: 0.732971\n",
      "[147]\ttraining's ndcg@1: 0.98573\ttraining's ndcg@2: 0.98221\ttraining's ndcg@3: 0.975122\ttraining's ndcg@4: 0.973815\ttraining's ndcg@5: 0.976324\tvalid_1's ndcg@1: 0.505931\tvalid_1's ndcg@2: 0.567245\tvalid_1's ndcg@3: 0.637361\tvalid_1's ndcg@4: 0.699988\tvalid_1's ndcg@5: 0.735242\n",
      "[148]\ttraining's ndcg@1: 0.98573\ttraining's ndcg@2: 0.98221\ttraining's ndcg@3: 0.975272\ttraining's ndcg@4: 0.973947\ttraining's ndcg@5: 0.976435\tvalid_1's ndcg@1: 0.508758\tvalid_1's ndcg@2: 0.5766\tvalid_1's ndcg@3: 0.639648\tvalid_1's ndcg@4: 0.703621\tvalid_1's ndcg@5: 0.737036\n",
      "[149]\ttraining's ndcg@1: 0.986443\ttraining's ndcg@2: 0.98241\ttraining's ndcg@3: 0.97564\ttraining's ndcg@4: 0.974155\ttraining's ndcg@5: 0.976759\tvalid_1's ndcg@1: 0.506548\tvalid_1's ndcg@2: 0.567501\tvalid_1's ndcg@3: 0.639784\tvalid_1's ndcg@4: 0.700247\tvalid_1's ndcg@5: 0.736127\n",
      "[150]\ttraining's ndcg@1: 0.986443\ttraining's ndcg@2: 0.98241\ttraining's ndcg@3: 0.97564\ttraining's ndcg@4: 0.974123\ttraining's ndcg@5: 0.976726\tvalid_1's ndcg@1: 0.504428\tvalid_1's ndcg@2: 0.575215\tvalid_1's ndcg@3: 0.641889\tvalid_1's ndcg@4: 0.700263\tvalid_1's ndcg@5: 0.736729\n",
      "[151]\ttraining's ndcg@1: 0.987154\ttraining's ndcg@2: 0.98261\ttraining's ndcg@3: 0.975946\ttraining's ndcg@4: 0.974309\ttraining's ndcg@5: 0.977101\tvalid_1's ndcg@1: 0.508459\tvalid_1's ndcg@2: 0.572496\tvalid_1's ndcg@3: 0.640916\tvalid_1's ndcg@4: 0.702496\tvalid_1's ndcg@5: 0.735882\n",
      "[152]\ttraining's ndcg@1: 0.987154\ttraining's ndcg@2: 0.98261\ttraining's ndcg@3: 0.976035\ttraining's ndcg@4: 0.974346\ttraining's ndcg@5: 0.977165\tvalid_1's ndcg@1: 0.501247\tvalid_1's ndcg@2: 0.567158\tvalid_1's ndcg@3: 0.637169\tvalid_1's ndcg@4: 0.699925\tvalid_1's ndcg@5: 0.733373\n",
      "[153]\ttraining's ndcg@1: 0.987865\ttraining's ndcg@2: 0.98298\ttraining's ndcg@3: 0.976249\ttraining's ndcg@4: 0.974765\ttraining's ndcg@5: 0.977493\tvalid_1's ndcg@1: 0.500628\tvalid_1's ndcg@2: 0.569443\tvalid_1's ndcg@3: 0.639141\tvalid_1's ndcg@4: 0.700953\tvalid_1's ndcg@5: 0.733346\n",
      "[154]\ttraining's ndcg@1: 0.987865\ttraining's ndcg@2: 0.98298\ttraining's ndcg@3: 0.976188\ttraining's ndcg@4: 0.974712\ttraining's ndcg@5: 0.977464\tvalid_1's ndcg@1: 0.497798\tvalid_1's ndcg@2: 0.567303\tvalid_1's ndcg@3: 0.63724\tvalid_1's ndcg@4: 0.698449\tvalid_1's ndcg@5: 0.731004\n",
      "[155]\ttraining's ndcg@1: 0.987865\ttraining's ndcg@2: 0.98298\ttraining's ndcg@3: 0.976467\ttraining's ndcg@4: 0.974797\ttraining's ndcg@5: 0.97744\tvalid_1's ndcg@1: 0.502751\tvalid_1's ndcg@2: 0.569202\tvalid_1's ndcg@3: 0.639458\tvalid_1's ndcg@4: 0.700129\tvalid_1's ndcg@5: 0.732258\n",
      "[156]\ttraining's ndcg@1: 0.987865\ttraining's ndcg@2: 0.98298\ttraining's ndcg@3: 0.976892\ttraining's ndcg@4: 0.974844\ttraining's ndcg@5: 0.977609\tvalid_1's ndcg@1: 0.503104\tvalid_1's ndcg@2: 0.571845\tvalid_1's ndcg@3: 0.638742\tvalid_1's ndcg@4: 0.700036\tvalid_1's ndcg@5: 0.734089\n",
      "[157]\ttraining's ndcg@1: 0.988577\ttraining's ndcg@2: 0.98335\ttraining's ndcg@3: 0.977408\ttraining's ndcg@4: 0.975018\ttraining's ndcg@5: 0.977825\tvalid_1's ndcg@1: 0.504165\tvalid_1's ndcg@2: 0.571528\tvalid_1's ndcg@3: 0.636814\tvalid_1's ndcg@4: 0.701438\tvalid_1's ndcg@5: 0.732584\n",
      "[158]\ttraining's ndcg@1: 0.989287\ttraining's ndcg@2: 0.98355\ttraining's ndcg@3: 0.97759\ttraining's ndcg@4: 0.97527\ttraining's ndcg@5: 0.978372\tvalid_1's ndcg@1: 0.501348\tvalid_1's ndcg@2: 0.568787\tvalid_1's ndcg@3: 0.640134\tvalid_1's ndcg@4: 0.700311\tvalid_1's ndcg@5: 0.732276\n",
      "[159]\ttraining's ndcg@1: 0.989287\ttraining's ndcg@2: 0.98355\ttraining's ndcg@3: 0.977652\ttraining's ndcg@4: 0.97542\ttraining's ndcg@5: 0.978445\tvalid_1's ndcg@1: 0.504176\tvalid_1's ndcg@2: 0.567546\tvalid_1's ndcg@3: 0.640473\tvalid_1's ndcg@4: 0.701098\tvalid_1's ndcg@5: 0.732294\n",
      "[160]\ttraining's ndcg@1: 0.989998\ttraining's ndcg@2: 0.983749\ttraining's ndcg@3: 0.977834\ttraining's ndcg@4: 0.975634\ttraining's ndcg@5: 0.978605\tvalid_1's ndcg@1: 0.504792\tvalid_1's ndcg@2: 0.56526\tvalid_1's ndcg@3: 0.636627\tvalid_1's ndcg@4: 0.700629\tvalid_1's ndcg@5: 0.731521\n",
      "[161]\ttraining's ndcg@1: 0.989998\ttraining's ndcg@2: 0.983749\ttraining's ndcg@3: 0.977834\ttraining's ndcg@4: 0.975567\ttraining's ndcg@5: 0.978609\tvalid_1's ndcg@1: 0.507302\tvalid_1's ndcg@2: 0.5725\tvalid_1's ndcg@3: 0.640781\tvalid_1's ndcg@4: 0.703622\tvalid_1's ndcg@5: 0.733537\n",
      "[162]\ttraining's ndcg@1: 0.991421\ttraining's ndcg@2: 0.984319\ttraining's ndcg@3: 0.9782\ttraining's ndcg@4: 0.975967\ttraining's ndcg@5: 0.979125\tvalid_1's ndcg@1: 0.505843\tvalid_1's ndcg@2: 0.571582\tvalid_1's ndcg@3: 0.63929\tvalid_1's ndcg@4: 0.70328\tvalid_1's ndcg@5: 0.733612\n",
      "[163]\ttraining's ndcg@1: 0.991421\ttraining's ndcg@2: 0.984745\ttraining's ndcg@3: 0.978405\ttraining's ndcg@4: 0.976241\ttraining's ndcg@5: 0.979268\tvalid_1's ndcg@1: 0.505841\tvalid_1's ndcg@2: 0.574809\tvalid_1's ndcg@3: 0.641742\tvalid_1's ndcg@4: 0.703963\tvalid_1's ndcg@5: 0.734504\n",
      "[164]\ttraining's ndcg@1: 0.991421\ttraining's ndcg@2: 0.984745\ttraining's ndcg@3: 0.978497\ttraining's ndcg@4: 0.9763\ttraining's ndcg@5: 0.979557\tvalid_1's ndcg@1: 0.503721\tvalid_1's ndcg@2: 0.576249\tvalid_1's ndcg@3: 0.641742\tvalid_1's ndcg@4: 0.703634\tvalid_1's ndcg@5: 0.734405\n",
      "[165]\ttraining's ndcg@1: 0.991421\ttraining's ndcg@2: 0.984745\ttraining's ndcg@3: 0.978682\ttraining's ndcg@4: 0.976484\ttraining's ndcg@5: 0.979717\tvalid_1's ndcg@1: 0.508669\tvalid_1's ndcg@2: 0.577638\tvalid_1's ndcg@3: 0.64301\tvalid_1's ndcg@4: 0.704857\tvalid_1's ndcg@5: 0.735052\n",
      "[166]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.984945\ttraining's ndcg@3: 0.978865\ttraining's ndcg@4: 0.977188\ttraining's ndcg@5: 0.979954\tvalid_1's ndcg@1: 0.504958\tvalid_1's ndcg@2: 0.57253\tvalid_1's ndcg@3: 0.643367\tvalid_1's ndcg@4: 0.701477\tvalid_1's ndcg@5: 0.733901\n",
      "[167]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985286\ttraining's ndcg@3: 0.979115\ttraining's ndcg@4: 0.977477\ttraining's ndcg@5: 0.980088\tvalid_1's ndcg@1: 0.503545\tvalid_1's ndcg@2: 0.572642\tvalid_1's ndcg@3: 0.643101\tvalid_1's ndcg@4: 0.700762\tvalid_1's ndcg@5: 0.73331\n",
      "[168]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985457\ttraining's ndcg@3: 0.979239\ttraining's ndcg@4: 0.977803\ttraining's ndcg@5: 0.980136\tvalid_1's ndcg@1: 0.506372\tvalid_1's ndcg@2: 0.573265\tvalid_1's ndcg@3: 0.643084\tvalid_1's ndcg@4: 0.701743\tvalid_1's ndcg@5: 0.733216\n",
      "[169]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985712\ttraining's ndcg@3: 0.979489\ttraining's ndcg@4: 0.977844\ttraining's ndcg@5: 0.980296\tvalid_1's ndcg@1: 0.510879\tvalid_1's ndcg@2: 0.57453\tvalid_1's ndcg@3: 0.645099\tvalid_1's ndcg@4: 0.703687\tvalid_1's ndcg@5: 0.734407\n",
      "[170]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985712\ttraining's ndcg@3: 0.979489\ttraining's ndcg@4: 0.977917\ttraining's ndcg@5: 0.980292\tvalid_1's ndcg@1: 0.516506\tvalid_1's ndcg@2: 0.577873\tvalid_1's ndcg@3: 0.648459\tvalid_1's ndcg@4: 0.707755\tvalid_1's ndcg@5: 0.737195\n",
      "[171]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985712\ttraining's ndcg@3: 0.979612\ttraining's ndcg@4: 0.978149\ttraining's ndcg@5: 0.980729\tvalid_1's ndcg@1: 0.516506\tvalid_1's ndcg@2: 0.578058\tvalid_1's ndcg@3: 0.650121\tvalid_1's ndcg@4: 0.706271\tvalid_1's ndcg@5: 0.737016\n",
      "[172]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985712\ttraining's ndcg@3: 0.979705\ttraining's ndcg@4: 0.978162\ttraining's ndcg@5: 0.980826\tvalid_1's ndcg@1: 0.513706\tvalid_1's ndcg@2: 0.577356\tvalid_1's ndcg@3: 0.648321\tvalid_1's ndcg@4: 0.705037\tvalid_1's ndcg@5: 0.73535\n",
      "[173]\ttraining's ndcg@1: 0.992132\ttraining's ndcg@2: 0.985712\ttraining's ndcg@3: 0.979859\ttraining's ndcg@4: 0.978378\ttraining's ndcg@5: 0.980921\tvalid_1's ndcg@1: 0.511586\tvalid_1's ndcg@2: 0.577698\tvalid_1's ndcg@3: 0.648839\tvalid_1's ndcg@4: 0.706639\tvalid_1's ndcg@5: 0.736316\n",
      "[174]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.986082\ttraining's ndcg@3: 0.980043\ttraining's ndcg@4: 0.978555\ttraining's ndcg@5: 0.981068\tvalid_1's ndcg@1: 0.517213\tvalid_1's ndcg@2: 0.578007\tvalid_1's ndcg@3: 0.649814\tvalid_1's ndcg@4: 0.704946\tvalid_1's ndcg@5: 0.737283\n",
      "[175]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.986381\ttraining's ndcg@3: 0.980161\ttraining's ndcg@4: 0.978656\ttraining's ndcg@5: 0.981237\tvalid_1's ndcg@1: 0.514386\tvalid_1's ndcg@2: 0.580013\tvalid_1's ndcg@3: 0.649128\tvalid_1's ndcg@4: 0.703929\tvalid_1's ndcg@5: 0.736011\n",
      "[176]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.986873\ttraining's ndcg@3: 0.980532\ttraining's ndcg@4: 0.978912\ttraining's ndcg@5: 0.981357\tvalid_1's ndcg@1: 0.517213\tvalid_1's ndcg@2: 0.580806\tvalid_1's ndcg@3: 0.650953\tvalid_1's ndcg@4: 0.704717\tvalid_1's ndcg@5: 0.73819\n",
      "[177]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.987044\ttraining's ndcg@3: 0.980565\ttraining's ndcg@4: 0.978924\ttraining's ndcg@5: 0.981356\tvalid_1's ndcg@1: 0.514386\tvalid_1's ndcg@2: 0.577212\tvalid_1's ndcg@3: 0.649443\tvalid_1's ndcg@4: 0.699656\tvalid_1's ndcg@5: 0.735737\n",
      "[178]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.987044\ttraining's ndcg@3: 0.980657\ttraining's ndcg@4: 0.978962\ttraining's ndcg@5: 0.98137\tvalid_1's ndcg@1: 0.514386\tvalid_1's ndcg@2: 0.576638\tvalid_1's ndcg@3: 0.651054\tvalid_1's ndcg@4: 0.701539\tvalid_1's ndcg@5: 0.736983\n",
      "[179]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.987214\ttraining's ndcg@3: 0.980805\ttraining's ndcg@4: 0.979079\ttraining's ndcg@5: 0.981472\tvalid_1's ndcg@1: 0.51792\tvalid_1's ndcg@2: 0.570082\tvalid_1's ndcg@3: 0.646635\tvalid_1's ndcg@4: 0.701048\tvalid_1's ndcg@5: 0.736026\n",
      "[180]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.987214\ttraining's ndcg@3: 0.980805\ttraining's ndcg@4: 0.979137\ttraining's ndcg@5: 0.981556\tvalid_1's ndcg@1: 0.51792\tvalid_1's ndcg@2: 0.575921\tvalid_1's ndcg@3: 0.64835\tvalid_1's ndcg@4: 0.702356\tvalid_1's ndcg@5: 0.737151\n",
      "[181]\ttraining's ndcg@1: 0.992843\ttraining's ndcg@2: 0.987214\ttraining's ndcg@3: 0.98099\ttraining's ndcg@4: 0.979238\ttraining's ndcg@5: 0.981645\tvalid_1's ndcg@1: 0.51792\tvalid_1's ndcg@2: 0.576938\tvalid_1's ndcg@3: 0.650387\tvalid_1's ndcg@4: 0.703576\tvalid_1's ndcg@5: 0.73779\n",
      "[182]\ttraining's ndcg@1: 0.993554\ttraining's ndcg@2: 0.987584\ttraining's ndcg@3: 0.981112\ttraining's ndcg@4: 0.979327\ttraining's ndcg@5: 0.981823\tvalid_1's ndcg@1: 0.512972\tvalid_1's ndcg@2: 0.574531\tvalid_1's ndcg@3: 0.648004\tvalid_1's ndcg@4: 0.700101\tvalid_1's ndcg@5: 0.736121\n",
      "[183]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.987776\ttraining's ndcg@3: 0.981679\ttraining's ndcg@4: 0.979636\ttraining's ndcg@5: 0.982076\tvalid_1's ndcg@1: 0.51792\tvalid_1's ndcg@2: 0.575919\tvalid_1's ndcg@3: 0.651054\tvalid_1's ndcg@4: 0.70298\tvalid_1's ndcg@5: 0.739003\n",
      "[184]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.987776\ttraining's ndcg@3: 0.981679\ttraining's ndcg@4: 0.979688\ttraining's ndcg@5: 0.982081\tvalid_1's ndcg@1: 0.517578\tvalid_1's ndcg@2: 0.575484\tvalid_1's ndcg@3: 0.649061\tvalid_1's ndcg@4: 0.702078\tvalid_1's ndcg@5: 0.73852\n",
      "[185]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.987947\ttraining's ndcg@3: 0.981866\ttraining's ndcg@4: 0.979605\ttraining's ndcg@5: 0.982173\tvalid_1's ndcg@1: 0.515092\tvalid_1's ndcg@2: 0.575126\tvalid_1's ndcg@3: 0.648672\tvalid_1's ndcg@4: 0.700361\tvalid_1's ndcg@5: 0.737672\n",
      "[186]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.987947\ttraining's ndcg@3: 0.98185\ttraining's ndcg@4: 0.979661\ttraining's ndcg@5: 0.982181\tvalid_1's ndcg@1: 0.519338\tvalid_1's ndcg@2: 0.577674\tvalid_1's ndcg@3: 0.649955\tvalid_1's ndcg@4: 0.700128\tvalid_1's ndcg@5: 0.739697\n",
      "[187]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988118\ttraining's ndcg@3: 0.981921\ttraining's ndcg@4: 0.979675\ttraining's ndcg@5: 0.982223\tvalid_1's ndcg@1: 0.529925\tvalid_1's ndcg@2: 0.581834\tvalid_1's ndcg@3: 0.654277\tvalid_1's ndcg@4: 0.703345\tvalid_1's ndcg@5: 0.742689\n",
      "[188]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988126\ttraining's ndcg@3: 0.981992\ttraining's ndcg@4: 0.979782\ttraining's ndcg@5: 0.982322\tvalid_1's ndcg@1: 0.529925\tvalid_1's ndcg@2: 0.58065\tvalid_1's ndcg@3: 0.655649\tvalid_1's ndcg@4: 0.703374\tvalid_1's ndcg@5: 0.74269\n",
      "[189]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988126\ttraining's ndcg@3: 0.981992\ttraining's ndcg@4: 0.980038\ttraining's ndcg@5: 0.982371\tvalid_1's ndcg@1: 0.532797\tvalid_1's ndcg@2: 0.578398\tvalid_1's ndcg@3: 0.655069\tvalid_1's ndcg@4: 0.703044\tvalid_1's ndcg@5: 0.743021\n",
      "[190]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988296\ttraining's ndcg@3: 0.982133\ttraining's ndcg@4: 0.980084\ttraining's ndcg@5: 0.982515\tvalid_1's ndcg@1: 0.532318\tvalid_1's ndcg@2: 0.575738\tvalid_1's ndcg@3: 0.655492\tvalid_1's ndcg@4: 0.702983\tvalid_1's ndcg@5: 0.742347\n",
      "[191]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988467\ttraining's ndcg@3: 0.98235\ttraining's ndcg@4: 0.980313\ttraining's ndcg@5: 0.982553\tvalid_1's ndcg@1: 0.531567\tvalid_1's ndcg@2: 0.576714\tvalid_1's ndcg@3: 0.655586\tvalid_1's ndcg@4: 0.702232\tvalid_1's ndcg@5: 0.741088\n",
      "[192]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988467\ttraining's ndcg@3: 0.98272\ttraining's ndcg@4: 0.980394\ttraining's ndcg@5: 0.982656\tvalid_1's ndcg@1: 0.523791\tvalid_1's ndcg@2: 0.578941\tvalid_1's ndcg@3: 0.653477\tvalid_1's ndcg@4: 0.69953\tvalid_1's ndcg@5: 0.740306\n",
      "[193]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.988637\ttraining's ndcg@3: 0.982814\ttraining's ndcg@4: 0.980498\ttraining's ndcg@5: 0.982656\tvalid_1's ndcg@1: 0.528036\tvalid_1's ndcg@2: 0.580641\tvalid_1's ndcg@3: 0.655093\tvalid_1's ndcg@4: 0.701247\tvalid_1's ndcg@5: 0.741202\n",
      "[194]\ttraining's ndcg@1: 0.994265\ttraining's ndcg@2: 0.9888\ttraining's ndcg@3: 0.983027\ttraining's ndcg@4: 0.980732\ttraining's ndcg@5: 0.982738\tvalid_1's ndcg@1: 0.530157\tvalid_1's ndcg@2: 0.578683\tvalid_1's ndcg@3: 0.654907\tvalid_1's ndcg@4: 0.699832\tvalid_1's ndcg@5: 0.741084\n",
      "[195]\ttraining's ndcg@1: 0.994976\ttraining's ndcg@2: 0.98917\ttraining's ndcg@3: 0.983149\ttraining's ndcg@4: 0.981208\ttraining's ndcg@5: 0.983009\tvalid_1's ndcg@1: 0.530157\tvalid_1's ndcg@2: 0.577494\tvalid_1's ndcg@3: 0.653882\tvalid_1's ndcg@4: 0.699749\tvalid_1's ndcg@5: 0.742242\n",
      "[196]\ttraining's ndcg@1: 0.994976\ttraining's ndcg@2: 0.98917\ttraining's ndcg@3: 0.983241\ttraining's ndcg@4: 0.981246\ttraining's ndcg@5: 0.98303\tvalid_1's ndcg@1: 0.528786\tvalid_1's ndcg@2: 0.57457\tvalid_1's ndcg@3: 0.654823\tvalid_1's ndcg@4: 0.698988\tvalid_1's ndcg@5: 0.741327\n",
      "[197]\ttraining's ndcg@1: 0.994976\ttraining's ndcg@2: 0.98917\ttraining's ndcg@3: 0.983241\ttraining's ndcg@4: 0.98158\ttraining's ndcg@5: 0.983184\tvalid_1's ndcg@1: 0.528786\tvalid_1's ndcg@2: 0.577202\tvalid_1's ndcg@3: 0.653731\tvalid_1's ndcg@4: 0.7012\tvalid_1's ndcg@5: 0.739125\n",
      "[198]\ttraining's ndcg@1: 0.994976\ttraining's ndcg@2: 0.98917\ttraining's ndcg@3: 0.983334\ttraining's ndcg@4: 0.98163\ttraining's ndcg@5: 0.983157\tvalid_1's ndcg@1: 0.524191\tvalid_1's ndcg@2: 0.576426\tvalid_1's ndcg@3: 0.652651\tvalid_1's ndcg@4: 0.700107\tvalid_1's ndcg@5: 0.736112\n",
      "[199]\ttraining's ndcg@1: 0.994976\ttraining's ndcg@2: 0.989511\ttraining's ndcg@3: 0.983615\ttraining's ndcg@4: 0.98172\ttraining's ndcg@5: 0.983295\tvalid_1's ndcg@1: 0.523484\tvalid_1's ndcg@2: 0.57556\tvalid_1's ndcg@3: 0.655598\tvalid_1's ndcg@4: 0.700436\tvalid_1's ndcg@5: 0.738881\n",
      "[200]\ttraining's ndcg@1: 0.994976\ttraining's ndcg@2: 0.989511\ttraining's ndcg@3: 0.983831\ttraining's ndcg@4: 0.981749\ttraining's ndcg@5: 0.983366\tvalid_1's ndcg@1: 0.525605\tvalid_1's ndcg@2: 0.57802\tvalid_1's ndcg@3: 0.655513\tvalid_1's ndcg@4: 0.699514\tvalid_1's ndcg@5: 0.737685\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 7).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "# Split the data into input features (X) and target labels (y)\n",
    "X_train = train_data[columns_to_normalize]\n",
    "y_train = train_data['rank']\n",
    "\n",
    "X_test = test_data[columns_to_normalize]\n",
    "y_test = test_data['rank']\n",
    "\n",
    "train_groups = train_data.groupby('keyword').size().to_frame('size')['size'].to_numpy()\n",
    "test_groups = test_data.groupby('keyword').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "# Create DMatrix for training set\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtrain.set_group(train_groups)\n",
    "\n",
    "# Create DMatrix for test set\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "dtest.set_group(test_groups)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': ['ndcg'],\n",
    "    'random_state': 21,\n",
    "    'lambdarank_num_pair_per_sample': 80,\n",
    "    'lambdarank_pair_method': 'mean',\n",
    "#     'max_depth': 6,\n",
    "    'reg_lambda': 1\n",
    "}\n",
    "# Train the model\n",
    "lgb_train = lgb.Dataset(X_train, y_train, group=train_groups)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, group=test_groups)\n",
    "start_time = time.time()\n",
    "ranker = lgb.train(params, lgb_train, num_boost_round=200, valid_sets=[lgb_train, lgb_test])\n",
    "end_time = time.time()\n",
    "train_elapsed_time = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1c22bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9379779013120884, 0.9549526497812167, 1.0, 0.7220741369783733)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores3a =[]\n",
    "for query, group in test_data.groupby('keyword'):\n",
    "    if len(group['rank'].tolist()) != 1:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "        scores3a.append(nd(np.asarray([ranks]), np.asarray([ranker.predict(features).tolist()])))\n",
    "np.mean(scores3a), np.median(scores3a), max(scores3a), min(scores3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83bcb0",
   "metadata": {},
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "86cc896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10222\n",
      "[LightGBM] [Info] Number of data points in the train set: 6635, number of used features: 71\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[1]\ttraining's ndcg@1: 0.175689\ttraining's ndcg@2: 0.285588\ttraining's ndcg@3: 0.409777\ttraining's ndcg@4: 0.500192\ttraining's ndcg@5: 0.57311\tvalid_1's ndcg@1: 0.171982\tvalid_1's ndcg@2: 0.293576\tvalid_1's ndcg@3: 0.424757\tvalid_1's ndcg@4: 0.509939\tvalid_1's ndcg@5: 0.566962\n",
      "[2]\ttraining's ndcg@1: 0.544935\ttraining's ndcg@2: 0.597089\ttraining's ndcg@3: 0.651442\ttraining's ndcg@4: 0.696236\ttraining's ndcg@5: 0.743803\tvalid_1's ndcg@1: 0.444529\tvalid_1's ndcg@2: 0.529821\tvalid_1's ndcg@3: 0.600928\tvalid_1's ndcg@4: 0.647407\tvalid_1's ndcg@5: 0.698781\n",
      "[3]\ttraining's ndcg@1: 0.612516\ttraining's ndcg@2: 0.662544\ttraining's ndcg@3: 0.705531\ttraining's ndcg@4: 0.744946\ttraining's ndcg@5: 0.781104\tvalid_1's ndcg@1: 0.488956\tvalid_1's ndcg@2: 0.562238\tvalid_1's ndcg@3: 0.628681\tvalid_1's ndcg@4: 0.674001\tvalid_1's ndcg@5: 0.718679\n",
      "[4]\ttraining's ndcg@1: 0.654592\ttraining's ndcg@2: 0.693796\ttraining's ndcg@3: 0.727882\ttraining's ndcg@4: 0.765553\ttraining's ndcg@5: 0.797792\tvalid_1's ndcg@1: 0.485726\tvalid_1's ndcg@2: 0.576547\tvalid_1's ndcg@3: 0.630749\tvalid_1's ndcg@4: 0.682261\tvalid_1's ndcg@5: 0.726979\n",
      "[5]\ttraining's ndcg@1: 0.685263\ttraining's ndcg@2: 0.719663\ttraining's ndcg@3: 0.753467\ttraining's ndcg@4: 0.782422\ttraining's ndcg@5: 0.811232\tvalid_1's ndcg@1: 0.496456\tvalid_1's ndcg@2: 0.563033\tvalid_1's ndcg@3: 0.625676\tvalid_1's ndcg@4: 0.678374\tvalid_1's ndcg@5: 0.727717\n",
      "[6]\ttraining's ndcg@1: 0.711257\ttraining's ndcg@2: 0.734932\ttraining's ndcg@3: 0.76684\ttraining's ndcg@4: 0.797205\ttraining's ndcg@5: 0.821973\tvalid_1's ndcg@1: 0.513739\tvalid_1's ndcg@2: 0.574033\tvalid_1's ndcg@3: 0.630293\tvalid_1's ndcg@4: 0.690315\tvalid_1's ndcg@5: 0.733319\n",
      "[7]\ttraining's ndcg@1: 0.726911\ttraining's ndcg@2: 0.745921\ttraining's ndcg@3: 0.774261\ttraining's ndcg@4: 0.805587\ttraining's ndcg@5: 0.8292\tvalid_1's ndcg@1: 0.54022\tvalid_1's ndcg@2: 0.584847\tvalid_1's ndcg@3: 0.641252\tvalid_1's ndcg@4: 0.700875\tvalid_1's ndcg@5: 0.739956\n",
      "[8]\ttraining's ndcg@1: 0.753449\ttraining's ndcg@2: 0.760403\ttraining's ndcg@3: 0.787877\ttraining's ndcg@4: 0.812993\ttraining's ndcg@5: 0.839179\tvalid_1's ndcg@1: 0.521449\tvalid_1's ndcg@2: 0.564988\tvalid_1's ndcg@3: 0.631698\tvalid_1's ndcg@4: 0.688675\tvalid_1's ndcg@5: 0.732018\n",
      "[9]\ttraining's ndcg@1: 0.767786\ttraining's ndcg@2: 0.775486\ttraining's ndcg@3: 0.798369\ttraining's ndcg@4: 0.822479\ttraining's ndcg@5: 0.846458\tvalid_1's ndcg@1: 0.532029\tvalid_1's ndcg@2: 0.572469\tvalid_1's ndcg@3: 0.63645\tvalid_1's ndcg@4: 0.694549\tvalid_1's ndcg@5: 0.732285\n",
      "[10]\ttraining's ndcg@1: 0.782187\ttraining's ndcg@2: 0.786591\ttraining's ndcg@3: 0.803433\ttraining's ndcg@4: 0.82844\ttraining's ndcg@5: 0.852905\tvalid_1's ndcg@1: 0.534238\tvalid_1's ndcg@2: 0.565809\tvalid_1's ndcg@3: 0.635814\tvalid_1's ndcg@4: 0.69379\tvalid_1's ndcg@5: 0.731043\n",
      "[11]\ttraining's ndcg@1: 0.781957\ttraining's ndcg@2: 0.791036\ttraining's ndcg@3: 0.810593\ttraining's ndcg@4: 0.831904\ttraining's ndcg@5: 0.854976\tvalid_1's ndcg@1: 0.533159\tvalid_1's ndcg@2: 0.56996\tvalid_1's ndcg@3: 0.637409\tvalid_1's ndcg@4: 0.693809\tvalid_1's ndcg@5: 0.732485\n",
      "[12]\ttraining's ndcg@1: 0.786664\ttraining's ndcg@2: 0.797285\ttraining's ndcg@3: 0.8138\ttraining's ndcg@4: 0.834379\ttraining's ndcg@5: 0.857926\tvalid_1's ndcg@1: 0.545627\tvalid_1's ndcg@2: 0.590224\tvalid_1's ndcg@3: 0.644722\tvalid_1's ndcg@4: 0.693112\tvalid_1's ndcg@5: 0.740664\n",
      "[13]\ttraining's ndcg@1: 0.800035\ttraining's ndcg@2: 0.804685\ttraining's ndcg@3: 0.819824\ttraining's ndcg@4: 0.839564\ttraining's ndcg@5: 0.863635\tvalid_1's ndcg@1: 0.550977\tvalid_1's ndcg@2: 0.59115\tvalid_1's ndcg@3: 0.647016\tvalid_1's ndcg@4: 0.698598\tvalid_1's ndcg@5: 0.746901\n",
      "[14]\ttraining's ndcg@1: 0.803686\ttraining's ndcg@2: 0.808137\ttraining's ndcg@3: 0.821734\ttraining's ndcg@4: 0.843576\ttraining's ndcg@5: 0.865974\tvalid_1's ndcg@1: 0.535756\tvalid_1's ndcg@2: 0.593257\tvalid_1's ndcg@3: 0.650768\tvalid_1's ndcg@4: 0.697685\tvalid_1's ndcg@5: 0.74416\n",
      "[15]\ttraining's ndcg@1: 0.810474\ttraining's ndcg@2: 0.81797\ttraining's ndcg@3: 0.829004\ttraining's ndcg@4: 0.849006\ttraining's ndcg@5: 0.871252\tvalid_1's ndcg@1: 0.55861\tvalid_1's ndcg@2: 0.598101\tvalid_1's ndcg@3: 0.657637\tvalid_1's ndcg@4: 0.705848\tvalid_1's ndcg@5: 0.748268\n",
      "[16]\ttraining's ndcg@1: 0.816962\ttraining's ndcg@2: 0.825083\ttraining's ndcg@3: 0.834979\ttraining's ndcg@4: 0.852259\ttraining's ndcg@5: 0.874928\tvalid_1's ndcg@1: 0.544952\tvalid_1's ndcg@2: 0.593053\tvalid_1's ndcg@3: 0.654562\tvalid_1's ndcg@4: 0.703699\tvalid_1's ndcg@5: 0.74661\n",
      "[17]\ttraining's ndcg@1: 0.829939\ttraining's ndcg@2: 0.828428\ttraining's ndcg@3: 0.840502\ttraining's ndcg@4: 0.857298\ttraining's ndcg@5: 0.878466\tvalid_1's ndcg@1: 0.552244\tvalid_1's ndcg@2: 0.592361\tvalid_1's ndcg@3: 0.659553\tvalid_1's ndcg@4: 0.708375\tvalid_1's ndcg@5: 0.748928\n",
      "[18]\ttraining's ndcg@1: 0.832954\ttraining's ndcg@2: 0.833111\ttraining's ndcg@3: 0.842739\ttraining's ndcg@4: 0.862126\ttraining's ndcg@5: 0.883333\tvalid_1's ndcg@1: 0.548619\tvalid_1's ndcg@2: 0.589359\tvalid_1's ndcg@3: 0.665223\tvalid_1's ndcg@4: 0.706426\tvalid_1's ndcg@5: 0.750081\n",
      "[19]\ttraining's ndcg@1: 0.838828\ttraining's ndcg@2: 0.835547\ttraining's ndcg@3: 0.845473\ttraining's ndcg@4: 0.862765\ttraining's ndcg@5: 0.88483\tvalid_1's ndcg@1: 0.559605\tvalid_1's ndcg@2: 0.599497\tvalid_1's ndcg@3: 0.665652\tvalid_1's ndcg@4: 0.714414\tvalid_1's ndcg@5: 0.754358\n",
      "[20]\ttraining's ndcg@1: 0.840611\ttraining's ndcg@2: 0.839943\ttraining's ndcg@3: 0.849912\ttraining's ndcg@4: 0.866121\ttraining's ndcg@5: 0.887555\tvalid_1's ndcg@1: 0.567027\tvalid_1's ndcg@2: 0.596673\tvalid_1's ndcg@3: 0.666931\tvalid_1's ndcg@4: 0.714498\tvalid_1's ndcg@5: 0.757048\n",
      "[21]\ttraining's ndcg@1: 0.850558\ttraining's ndcg@2: 0.846976\ttraining's ndcg@3: 0.854154\ttraining's ndcg@4: 0.871274\ttraining's ndcg@5: 0.89182\tvalid_1's ndcg@1: 0.560685\tvalid_1's ndcg@2: 0.604073\tvalid_1's ndcg@3: 0.665765\tvalid_1's ndcg@4: 0.716986\tvalid_1's ndcg@5: 0.756908\n",
      "[22]\ttraining's ndcg@1: 0.857757\ttraining's ndcg@2: 0.849785\ttraining's ndcg@3: 0.859011\ttraining's ndcg@4: 0.875369\ttraining's ndcg@5: 0.895169\tvalid_1's ndcg@1: 0.551009\tvalid_1's ndcg@2: 0.605254\tvalid_1's ndcg@3: 0.662867\tvalid_1's ndcg@4: 0.711769\tvalid_1's ndcg@5: 0.75625\n",
      "[23]\ttraining's ndcg@1: 0.863437\ttraining's ndcg@2: 0.855076\ttraining's ndcg@3: 0.860134\ttraining's ndcg@4: 0.878184\ttraining's ndcg@5: 0.897253\tvalid_1's ndcg@1: 0.56268\tvalid_1's ndcg@2: 0.608258\tvalid_1's ndcg@3: 0.671206\tvalid_1's ndcg@4: 0.716819\tvalid_1's ndcg@5: 0.759753\n",
      "[24]\ttraining's ndcg@1: 0.870513\ttraining's ndcg@2: 0.860737\ttraining's ndcg@3: 0.862936\ttraining's ndcg@4: 0.880621\ttraining's ndcg@5: 0.900419\tvalid_1's ndcg@1: 0.564734\tvalid_1's ndcg@2: 0.613369\tvalid_1's ndcg@3: 0.663614\tvalid_1's ndcg@4: 0.709584\tvalid_1's ndcg@5: 0.756863\n",
      "[25]\ttraining's ndcg@1: 0.87528\ttraining's ndcg@2: 0.862878\ttraining's ndcg@3: 0.867957\ttraining's ndcg@4: 0.88432\ttraining's ndcg@5: 0.902502\tvalid_1's ndcg@1: 0.55095\tvalid_1's ndcg@2: 0.607908\tvalid_1's ndcg@3: 0.661259\tvalid_1's ndcg@4: 0.708254\tvalid_1's ndcg@5: 0.754259\n",
      "[26]\ttraining's ndcg@1: 0.877419\ttraining's ndcg@2: 0.865868\ttraining's ndcg@3: 0.868821\ttraining's ndcg@4: 0.886486\ttraining's ndcg@5: 0.904052\tvalid_1's ndcg@1: 0.563733\tvalid_1's ndcg@2: 0.613916\tvalid_1's ndcg@3: 0.668734\tvalid_1's ndcg@4: 0.71505\tvalid_1's ndcg@5: 0.759106\n",
      "[27]\ttraining's ndcg@1: 0.878221\ttraining's ndcg@2: 0.871614\ttraining's ndcg@3: 0.872147\ttraining's ndcg@4: 0.888367\ttraining's ndcg@5: 0.90552\tvalid_1's ndcg@1: 0.539872\tvalid_1's ndcg@2: 0.612102\tvalid_1's ndcg@3: 0.660017\tvalid_1's ndcg@4: 0.707497\tvalid_1's ndcg@5: 0.749581\n",
      "[28]\ttraining's ndcg@1: 0.882844\ttraining's ndcg@2: 0.875553\ttraining's ndcg@3: 0.874895\ttraining's ndcg@4: 0.889458\ttraining's ndcg@5: 0.906166\tvalid_1's ndcg@1: 0.546554\tvalid_1's ndcg@2: 0.609316\tvalid_1's ndcg@3: 0.657607\tvalid_1's ndcg@4: 0.708785\tvalid_1's ndcg@5: 0.750577\n",
      "[29]\ttraining's ndcg@1: 0.885999\ttraining's ndcg@2: 0.879988\ttraining's ndcg@3: 0.877571\ttraining's ndcg@4: 0.891275\ttraining's ndcg@5: 0.908084\tvalid_1's ndcg@1: 0.539485\tvalid_1's ndcg@2: 0.610415\tvalid_1's ndcg@3: 0.659066\tvalid_1's ndcg@4: 0.706713\tvalid_1's ndcg@5: 0.745961\n",
      "[30]\ttraining's ndcg@1: 0.890799\ttraining's ndcg@2: 0.882743\ttraining's ndcg@3: 0.881331\ttraining's ndcg@4: 0.893593\ttraining's ndcg@5: 0.909781\tvalid_1's ndcg@1: 0.540344\tvalid_1's ndcg@2: 0.60679\tvalid_1's ndcg@3: 0.659369\tvalid_1's ndcg@4: 0.706978\tvalid_1's ndcg@5: 0.749547\n",
      "[31]\ttraining's ndcg@1: 0.892577\ttraining's ndcg@2: 0.885974\ttraining's ndcg@3: 0.883667\ttraining's ndcg@4: 0.895333\ttraining's ndcg@5: 0.91148\tvalid_1's ndcg@1: 0.546047\tvalid_1's ndcg@2: 0.608447\tvalid_1's ndcg@3: 0.658764\tvalid_1's ndcg@4: 0.708184\tvalid_1's ndcg@5: 0.753279\n",
      "[32]\ttraining's ndcg@1: 0.893288\ttraining's ndcg@2: 0.888954\ttraining's ndcg@3: 0.885229\ttraining's ndcg@4: 0.897064\ttraining's ndcg@5: 0.912947\tvalid_1's ndcg@1: 0.544054\tvalid_1's ndcg@2: 0.606526\tvalid_1's ndcg@3: 0.657971\tvalid_1's ndcg@4: 0.710626\tvalid_1's ndcg@5: 0.751049\n",
      "[33]\ttraining's ndcg@1: 0.897731\ttraining's ndcg@2: 0.891479\ttraining's ndcg@3: 0.887449\ttraining's ndcg@4: 0.898448\ttraining's ndcg@5: 0.914878\tvalid_1's ndcg@1: 0.543747\tvalid_1's ndcg@2: 0.610123\tvalid_1's ndcg@3: 0.658847\tvalid_1's ndcg@4: 0.714663\tvalid_1's ndcg@5: 0.75213\n",
      "[34]\ttraining's ndcg@1: 0.899335\ttraining's ndcg@2: 0.892206\ttraining's ndcg@3: 0.88621\ttraining's ndcg@4: 0.899936\ttraining's ndcg@5: 0.915867\tvalid_1's ndcg@1: 0.534165\tvalid_1's ndcg@2: 0.604785\tvalid_1's ndcg@3: 0.659433\tvalid_1's ndcg@4: 0.714082\tvalid_1's ndcg@5: 0.752909\n",
      "[35]\ttraining's ndcg@1: 0.901623\ttraining's ndcg@2: 0.894137\ttraining's ndcg@3: 0.888843\ttraining's ndcg@4: 0.901538\ttraining's ndcg@5: 0.917248\tvalid_1's ndcg@1: 0.534604\tvalid_1's ndcg@2: 0.599487\tvalid_1's ndcg@3: 0.655588\tvalid_1's ndcg@4: 0.713444\tvalid_1's ndcg@5: 0.752622\n",
      "[36]\ttraining's ndcg@1: 0.903045\ttraining's ndcg@2: 0.897228\ttraining's ndcg@3: 0.891002\ttraining's ndcg@4: 0.903284\ttraining's ndcg@5: 0.918386\tvalid_1's ndcg@1: 0.537631\tvalid_1's ndcg@2: 0.609911\tvalid_1's ndcg@3: 0.658754\tvalid_1's ndcg@4: 0.717814\tvalid_1's ndcg@5: 0.75523\n",
      "[37]\ttraining's ndcg@1: 0.907832\ttraining's ndcg@2: 0.899971\ttraining's ndcg@3: 0.894041\ttraining's ndcg@4: 0.905897\ttraining's ndcg@5: 0.919918\tvalid_1's ndcg@1: 0.551062\tvalid_1's ndcg@2: 0.608296\tvalid_1's ndcg@3: 0.667271\tvalid_1's ndcg@4: 0.721565\tvalid_1's ndcg@5: 0.757256\n",
      "[38]\ttraining's ndcg@1: 0.909254\ttraining's ndcg@2: 0.901638\ttraining's ndcg@3: 0.895922\ttraining's ndcg@4: 0.907237\ttraining's ndcg@5: 0.920575\tvalid_1's ndcg@1: 0.569375\tvalid_1's ndcg@2: 0.617471\tvalid_1's ndcg@3: 0.673942\tvalid_1's ndcg@4: 0.726976\tvalid_1's ndcg@5: 0.761594\n",
      "[39]\ttraining's ndcg@1: 0.910667\ttraining's ndcg@2: 0.90208\ttraining's ndcg@3: 0.89577\ttraining's ndcg@4: 0.907264\ttraining's ndcg@5: 0.920392\tvalid_1's ndcg@1: 0.563212\tvalid_1's ndcg@2: 0.617779\tvalid_1's ndcg@3: 0.673242\tvalid_1's ndcg@4: 0.727079\tvalid_1's ndcg@5: 0.766682\n",
      "[40]\ttraining's ndcg@1: 0.914755\ttraining's ndcg@2: 0.903684\ttraining's ndcg@3: 0.897688\ttraining's ndcg@4: 0.910158\ttraining's ndcg@5: 0.922507\tvalid_1's ndcg@1: 0.553492\tvalid_1's ndcg@2: 0.612415\tvalid_1's ndcg@3: 0.668778\tvalid_1's ndcg@4: 0.722774\tvalid_1's ndcg@5: 0.763249\n",
      "[41]\ttraining's ndcg@1: 0.915466\ttraining's ndcg@2: 0.904678\ttraining's ndcg@3: 0.899634\ttraining's ndcg@4: 0.911163\ttraining's ndcg@5: 0.924298\tvalid_1's ndcg@1: 0.551662\tvalid_1's ndcg@2: 0.612268\tvalid_1's ndcg@3: 0.667842\tvalid_1's ndcg@4: 0.72089\tvalid_1's ndcg@5: 0.764687\n",
      "[42]\ttraining's ndcg@1: 0.916177\ttraining's ndcg@2: 0.907527\ttraining's ndcg@3: 0.902875\ttraining's ndcg@4: 0.912653\ttraining's ndcg@5: 0.925317\tvalid_1's ndcg@1: 0.564363\tvalid_1's ndcg@2: 0.626063\tvalid_1's ndcg@3: 0.674945\tvalid_1's ndcg@4: 0.72672\tvalid_1's ndcg@5: 0.769666\n",
      "[43]\ttraining's ndcg@1: 0.916889\ttraining's ndcg@2: 0.909971\ttraining's ndcg@3: 0.904061\ttraining's ndcg@4: 0.913609\ttraining's ndcg@5: 0.926155\tvalid_1's ndcg@1: 0.57069\tvalid_1's ndcg@2: 0.623035\tvalid_1's ndcg@3: 0.678265\tvalid_1's ndcg@4: 0.727438\tvalid_1's ndcg@5: 0.771461\n",
      "[44]\ttraining's ndcg@1: 0.916711\ttraining's ndcg@2: 0.91086\ttraining's ndcg@3: 0.90561\ttraining's ndcg@4: 0.914672\ttraining's ndcg@5: 0.926755\tvalid_1's ndcg@1: 0.57312\tvalid_1's ndcg@2: 0.629337\tvalid_1's ndcg@3: 0.677506\tvalid_1's ndcg@4: 0.726096\tvalid_1's ndcg@5: 0.773352\n",
      "[45]\ttraining's ndcg@1: 0.91991\ttraining's ndcg@2: 0.913566\ttraining's ndcg@3: 0.906737\ttraining's ndcg@4: 0.916095\ttraining's ndcg@5: 0.927892\tvalid_1's ndcg@1: 0.553539\tvalid_1's ndcg@2: 0.621594\tvalid_1's ndcg@3: 0.672398\tvalid_1's ndcg@4: 0.719753\tvalid_1's ndcg@5: 0.766218\n",
      "[46]\ttraining's ndcg@1: 0.922754\ttraining's ndcg@2: 0.914918\ttraining's ndcg@3: 0.909029\ttraining's ndcg@4: 0.918198\ttraining's ndcg@5: 0.929157\tvalid_1's ndcg@1: 0.553026\tvalid_1's ndcg@2: 0.624351\tvalid_1's ndcg@3: 0.673583\tvalid_1's ndcg@4: 0.717973\tvalid_1's ndcg@5: 0.766215\n",
      "[47]\ttraining's ndcg@1: 0.924709\ttraining's ndcg@2: 0.916383\ttraining's ndcg@3: 0.910576\ttraining's ndcg@4: 0.918777\ttraining's ndcg@5: 0.929975\tvalid_1's ndcg@1: 0.565245\tvalid_1's ndcg@2: 0.630428\tvalid_1's ndcg@3: 0.681847\tvalid_1's ndcg@4: 0.726505\tvalid_1's ndcg@5: 0.770431\n",
      "[48]\ttraining's ndcg@1: 0.926497\ttraining's ndcg@2: 0.917805\ttraining's ndcg@3: 0.911262\ttraining's ndcg@4: 0.920458\ttraining's ndcg@5: 0.930774\tvalid_1's ndcg@1: 0.561012\tvalid_1's ndcg@2: 0.626487\tvalid_1's ndcg@3: 0.676252\tvalid_1's ndcg@4: 0.727396\tvalid_1's ndcg@5: 0.769254\n",
      "[49]\ttraining's ndcg@1: 0.930229\ttraining's ndcg@2: 0.919172\ttraining's ndcg@3: 0.912543\ttraining's ndcg@4: 0.922054\ttraining's ndcg@5: 0.93198\tvalid_1's ndcg@1: 0.562399\tvalid_1's ndcg@2: 0.626774\tvalid_1's ndcg@3: 0.676344\tvalid_1's ndcg@4: 0.725939\tvalid_1's ndcg@5: 0.769621\n",
      "[50]\ttraining's ndcg@1: 0.93024\ttraining's ndcg@2: 0.919809\ttraining's ndcg@3: 0.912534\ttraining's ndcg@4: 0.922596\ttraining's ndcg@5: 0.932381\tvalid_1's ndcg@1: 0.572362\tvalid_1's ndcg@2: 0.628973\tvalid_1's ndcg@3: 0.676642\tvalid_1's ndcg@4: 0.728635\tvalid_1's ndcg@5: 0.771363\n",
      "[51]\ttraining's ndcg@1: 0.93024\ttraining's ndcg@2: 0.920254\ttraining's ndcg@3: 0.913721\ttraining's ndcg@4: 0.922636\ttraining's ndcg@5: 0.933138\tvalid_1's ndcg@1: 0.57007\tvalid_1's ndcg@2: 0.629346\tvalid_1's ndcg@3: 0.676784\tvalid_1's ndcg@4: 0.727028\tvalid_1's ndcg@5: 0.767986\n",
      "[52]\ttraining's ndcg@1: 0.932385\ttraining's ndcg@2: 0.921914\ttraining's ndcg@3: 0.914206\ttraining's ndcg@4: 0.923559\ttraining's ndcg@5: 0.933902\tvalid_1's ndcg@1: 0.576057\tvalid_1's ndcg@2: 0.631399\tvalid_1's ndcg@3: 0.680571\tvalid_1's ndcg@4: 0.727924\tvalid_1's ndcg@5: 0.770375\n",
      "[53]\ttraining's ndcg@1: 0.934351\ttraining's ndcg@2: 0.922903\ttraining's ndcg@3: 0.91538\ttraining's ndcg@4: 0.924934\ttraining's ndcg@5: 0.934557\tvalid_1's ndcg@1: 0.573044\tvalid_1's ndcg@2: 0.625615\tvalid_1's ndcg@3: 0.676299\tvalid_1's ndcg@4: 0.724448\tvalid_1's ndcg@5: 0.767946\n",
      "[54]\ttraining's ndcg@1: 0.935773\ttraining's ndcg@2: 0.924154\ttraining's ndcg@3: 0.917208\ttraining's ndcg@4: 0.925493\ttraining's ndcg@5: 0.935157\tvalid_1's ndcg@1: 0.580648\tvalid_1's ndcg@2: 0.630482\tvalid_1's ndcg@3: 0.68296\tvalid_1's ndcg@4: 0.724724\tvalid_1's ndcg@5: 0.771705\n",
      "[55]\ttraining's ndcg@1: 0.937197\ttraining's ndcg@2: 0.926534\ttraining's ndcg@3: 0.919382\ttraining's ndcg@4: 0.927239\ttraining's ndcg@5: 0.936533\tvalid_1's ndcg@1: 0.572363\tvalid_1's ndcg@2: 0.633338\tvalid_1's ndcg@3: 0.682668\tvalid_1's ndcg@4: 0.7257\tvalid_1's ndcg@5: 0.770708\n",
      "[56]\ttraining's ndcg@1: 0.937197\ttraining's ndcg@2: 0.927217\ttraining's ndcg@3: 0.920464\ttraining's ndcg@4: 0.927778\ttraining's ndcg@5: 0.937074\tvalid_1's ndcg@1: 0.569882\tvalid_1's ndcg@2: 0.633914\tvalid_1's ndcg@3: 0.684539\tvalid_1's ndcg@4: 0.726698\tvalid_1's ndcg@5: 0.77058\n",
      "[57]\ttraining's ndcg@1: 0.937907\ttraining's ndcg@2: 0.928248\ttraining's ndcg@3: 0.920949\ttraining's ndcg@4: 0.927841\ttraining's ndcg@5: 0.937611\tvalid_1's ndcg@1: 0.568719\tvalid_1's ndcg@2: 0.644965\tvalid_1's ndcg@3: 0.688033\tvalid_1's ndcg@4: 0.728761\tvalid_1's ndcg@5: 0.771854\n",
      "[58]\ttraining's ndcg@1: 0.941998\ttraining's ndcg@2: 0.929871\ttraining's ndcg@3: 0.922153\ttraining's ndcg@4: 0.92977\ttraining's ndcg@5: 0.938997\tvalid_1's ndcg@1: 0.566137\tvalid_1's ndcg@2: 0.644653\tvalid_1's ndcg@3: 0.687341\tvalid_1's ndcg@4: 0.725284\tvalid_1's ndcg@5: 0.771041\n",
      "[59]\ttraining's ndcg@1: 0.94449\ttraining's ndcg@2: 0.931525\ttraining's ndcg@3: 0.92378\ttraining's ndcg@4: 0.930762\ttraining's ndcg@5: 0.939863\tvalid_1's ndcg@1: 0.561495\tvalid_1's ndcg@2: 0.640085\tvalid_1's ndcg@3: 0.681723\tvalid_1's ndcg@4: 0.725208\tvalid_1's ndcg@5: 0.769375\n",
      "[60]\ttraining's ndcg@1: 0.945201\ttraining's ndcg@2: 0.932342\ttraining's ndcg@3: 0.924116\ttraining's ndcg@4: 0.931527\ttraining's ndcg@5: 0.940358\tvalid_1's ndcg@1: 0.568272\tvalid_1's ndcg@2: 0.640431\tvalid_1's ndcg@3: 0.685089\tvalid_1's ndcg@4: 0.72626\tvalid_1's ndcg@5: 0.77203\n",
      "[61]\ttraining's ndcg@1: 0.945201\ttraining's ndcg@2: 0.932172\ttraining's ndcg@3: 0.925787\ttraining's ndcg@4: 0.931968\ttraining's ndcg@5: 0.940592\tvalid_1's ndcg@1: 0.579894\tvalid_1's ndcg@2: 0.64327\tvalid_1's ndcg@3: 0.68798\tvalid_1's ndcg@4: 0.729859\tvalid_1's ndcg@5: 0.777159\n",
      "[62]\ttraining's ndcg@1: 0.945378\ttraining's ndcg@2: 0.932797\ttraining's ndcg@3: 0.92679\ttraining's ndcg@4: 0.932346\ttraining's ndcg@5: 0.940905\tvalid_1's ndcg@1: 0.582545\tvalid_1's ndcg@2: 0.643503\tvalid_1's ndcg@3: 0.690795\tvalid_1's ndcg@4: 0.731699\tvalid_1's ndcg@5: 0.775257\n",
      "[63]\ttraining's ndcg@1: 0.947613\ttraining's ndcg@2: 0.933688\ttraining's ndcg@3: 0.928438\ttraining's ndcg@4: 0.933491\ttraining's ndcg@5: 0.941599\tvalid_1's ndcg@1: 0.576136\tvalid_1's ndcg@2: 0.64524\tvalid_1's ndcg@3: 0.686454\tvalid_1's ndcg@4: 0.729349\tvalid_1's ndcg@5: 0.774288\n",
      "[64]\ttraining's ndcg@1: 0.947613\ttraining's ndcg@2: 0.934625\ttraining's ndcg@3: 0.927995\ttraining's ndcg@4: 0.933884\ttraining's ndcg@5: 0.942055\tvalid_1's ndcg@1: 0.571894\tvalid_1's ndcg@2: 0.643111\tvalid_1's ndcg@3: 0.685557\tvalid_1's ndcg@4: 0.728203\tvalid_1's ndcg@5: 0.771999\n",
      "[65]\ttraining's ndcg@1: 0.950457\ttraining's ndcg@2: 0.93568\ttraining's ndcg@3: 0.929239\ttraining's ndcg@4: 0.935158\ttraining's ndcg@5: 0.942879\tvalid_1's ndcg@1: 0.576845\tvalid_1's ndcg@2: 0.642106\tvalid_1's ndcg@3: 0.685606\tvalid_1's ndcg@4: 0.728225\tvalid_1's ndcg@5: 0.773523\n",
      "[66]\ttraining's ndcg@1: 0.951879\ttraining's ndcg@2: 0.935799\ttraining's ndcg@3: 0.930494\ttraining's ndcg@4: 0.935566\ttraining's ndcg@5: 0.943454\tvalid_1's ndcg@1: 0.574018\tvalid_1's ndcg@2: 0.63882\tvalid_1's ndcg@3: 0.684869\tvalid_1's ndcg@4: 0.725986\tvalid_1's ndcg@5: 0.771959\n",
      "[67]\ttraining's ndcg@1: 0.954723\ttraining's ndcg@2: 0.937088\ttraining's ndcg@3: 0.931683\ttraining's ndcg@4: 0.936641\ttraining's ndcg@5: 0.944823\tvalid_1's ndcg@1: 0.572601\tvalid_1's ndcg@2: 0.633166\tvalid_1's ndcg@3: 0.682603\tvalid_1's ndcg@4: 0.725279\tvalid_1's ndcg@5: 0.769921\n",
      "[68]\ttraining's ndcg@1: 0.955078\ttraining's ndcg@2: 0.938104\ttraining's ndcg@3: 0.932142\ttraining's ndcg@4: 0.93747\ttraining's ndcg@5: 0.945361\tvalid_1's ndcg@1: 0.569961\tvalid_1's ndcg@2: 0.635011\tvalid_1's ndcg@3: 0.682071\tvalid_1's ndcg@4: 0.726953\tvalid_1's ndcg@5: 0.770841\n",
      "[69]\ttraining's ndcg@1: 0.955078\ttraining's ndcg@2: 0.938083\ttraining's ndcg@3: 0.932576\ttraining's ndcg@4: 0.937799\ttraining's ndcg@5: 0.945798\tvalid_1's ndcg@1: 0.583233\tvalid_1's ndcg@2: 0.642868\tvalid_1's ndcg@3: 0.685561\tvalid_1's ndcg@4: 0.730547\tvalid_1's ndcg@5: 0.773945\n",
      "[70]\ttraining's ndcg@1: 0.956145\ttraining's ndcg@2: 0.938851\ttraining's ndcg@3: 0.9342\ttraining's ndcg@4: 0.938734\ttraining's ndcg@5: 0.946524\tvalid_1's ndcg@1: 0.589411\tvalid_1's ndcg@2: 0.649446\tvalid_1's ndcg@3: 0.68748\tvalid_1's ndcg@4: 0.735609\tvalid_1's ndcg@5: 0.777148\n",
      "[71]\ttraining's ndcg@1: 0.956145\ttraining's ndcg@2: 0.938979\ttraining's ndcg@3: 0.934529\ttraining's ndcg@4: 0.938672\ttraining's ndcg@5: 0.946671\tvalid_1's ndcg@1: 0.578106\tvalid_1's ndcg@2: 0.642713\tvalid_1's ndcg@3: 0.683754\tvalid_1's ndcg@4: 0.728257\tvalid_1's ndcg@5: 0.773936\n",
      "[72]\ttraining's ndcg@1: 0.956145\ttraining's ndcg@2: 0.939565\ttraining's ndcg@3: 0.934864\ttraining's ndcg@4: 0.93899\ttraining's ndcg@5: 0.946888\tvalid_1's ndcg@1: 0.582348\tvalid_1's ndcg@2: 0.645546\tvalid_1's ndcg@3: 0.686342\tvalid_1's ndcg@4: 0.729919\tvalid_1's ndcg@5: 0.775436\n",
      "[73]\ttraining's ndcg@1: 0.9581\ttraining's ndcg@2: 0.940284\ttraining's ndcg@3: 0.936004\ttraining's ndcg@4: 0.939449\ttraining's ndcg@5: 0.947202\tvalid_1's ndcg@1: 0.582324\tvalid_1's ndcg@2: 0.649952\tvalid_1's ndcg@3: 0.6915\tvalid_1's ndcg@4: 0.733022\tvalid_1's ndcg@5: 0.775584\n",
      "[74]\ttraining's ndcg@1: 0.9581\ttraining's ndcg@2: 0.940327\ttraining's ndcg@3: 0.936054\ttraining's ndcg@4: 0.939488\ttraining's ndcg@5: 0.947581\tvalid_1's ndcg@1: 0.576503\tvalid_1's ndcg@2: 0.646534\tvalid_1's ndcg@3: 0.687252\tvalid_1's ndcg@4: 0.733099\tvalid_1's ndcg@5: 0.77577\n",
      "[75]\ttraining's ndcg@1: 0.958811\ttraining's ndcg@2: 0.940697\ttraining's ndcg@3: 0.936971\ttraining's ndcg@4: 0.94016\ttraining's ndcg@5: 0.948011\tvalid_1's ndcg@1: 0.578624\tvalid_1's ndcg@2: 0.648064\tvalid_1's ndcg@3: 0.687257\tvalid_1's ndcg@4: 0.733618\tvalid_1's ndcg@5: 0.776194\n",
      "[76]\ttraining's ndcg@1: 0.959522\ttraining's ndcg@2: 0.940897\ttraining's ndcg@3: 0.937644\ttraining's ndcg@4: 0.940402\ttraining's ndcg@5: 0.94814\tvalid_1's ndcg@1: 0.584986\tvalid_1's ndcg@2: 0.650527\tvalid_1's ndcg@3: 0.688787\tvalid_1's ndcg@4: 0.73527\tvalid_1's ndcg@5: 0.777163\n",
      "[77]\ttraining's ndcg@1: 0.959522\ttraining's ndcg@2: 0.941152\ttraining's ndcg@3: 0.938265\ttraining's ndcg@4: 0.940634\ttraining's ndcg@5: 0.948786\tvalid_1's ndcg@1: 0.592751\tvalid_1's ndcg@2: 0.650841\tvalid_1's ndcg@3: 0.69231\tvalid_1's ndcg@4: 0.735423\tvalid_1's ndcg@5: 0.779339\n",
      "[78]\ttraining's ndcg@1: 0.961299\ttraining's ndcg@2: 0.942291\ttraining's ndcg@3: 0.939015\ttraining's ndcg@4: 0.941893\ttraining's ndcg@5: 0.949382\tvalid_1's ndcg@1: 0.589923\tvalid_1's ndcg@2: 0.647843\tvalid_1's ndcg@3: 0.686718\tvalid_1's ndcg@4: 0.7325\tvalid_1's ndcg@5: 0.777117\n",
      "[79]\ttraining's ndcg@1: 0.96201\ttraining's ndcg@2: 0.94314\ttraining's ndcg@3: 0.939581\ttraining's ndcg@4: 0.942638\ttraining's ndcg@5: 0.950005\tvalid_1's ndcg@1: 0.584622\tvalid_1's ndcg@2: 0.646355\tvalid_1's ndcg@3: 0.682383\tvalid_1's ndcg@4: 0.734555\tvalid_1's ndcg@5: 0.7757\n",
      "[80]\ttraining's ndcg@1: 0.962721\ttraining's ndcg@2: 0.943681\ttraining's ndcg@3: 0.940154\ttraining's ndcg@4: 0.943231\ttraining's ndcg@5: 0.95023\tvalid_1's ndcg@1: 0.584091\tvalid_1's ndcg@2: 0.646546\tvalid_1's ndcg@3: 0.685165\tvalid_1's ndcg@4: 0.736082\tvalid_1's ndcg@5: 0.775924\n",
      "[81]\ttraining's ndcg@1: 0.962721\ttraining's ndcg@2: 0.944494\ttraining's ndcg@3: 0.941125\ttraining's ndcg@4: 0.943628\ttraining's ndcg@5: 0.950503\tvalid_1's ndcg@1: 0.589393\tvalid_1's ndcg@2: 0.649051\tvalid_1's ndcg@3: 0.688835\tvalid_1's ndcg@4: 0.736683\tvalid_1's ndcg@5: 0.778053\n",
      "[82]\ttraining's ndcg@1: 0.963432\ttraining's ndcg@2: 0.945279\ttraining's ndcg@3: 0.941405\ttraining's ndcg@4: 0.944123\ttraining's ndcg@5: 0.951205\tvalid_1's ndcg@1: 0.58851\tvalid_1's ndcg@2: 0.645919\tvalid_1's ndcg@3: 0.68813\tvalid_1's ndcg@4: 0.734907\tvalid_1's ndcg@5: 0.776482\n",
      "[83]\ttraining's ndcg@1: 0.963432\ttraining's ndcg@2: 0.946215\ttraining's ndcg@3: 0.941804\ttraining's ndcg@4: 0.944486\ttraining's ndcg@5: 0.951466\tvalid_1's ndcg@1: 0.590815\tvalid_1's ndcg@2: 0.640028\tvalid_1's ndcg@3: 0.688034\tvalid_1's ndcg@4: 0.734452\tvalid_1's ndcg@5: 0.775837\n",
      "[84]\ttraining's ndcg@1: 0.964498\ttraining's ndcg@2: 0.947622\ttraining's ndcg@3: 0.942262\ttraining's ndcg@4: 0.944947\ttraining's ndcg@5: 0.95186\tvalid_1's ndcg@1: 0.593289\tvalid_1's ndcg@2: 0.640635\tvalid_1's ndcg@3: 0.691518\tvalid_1's ndcg@4: 0.73688\tvalid_1's ndcg@5: 0.777126\n",
      "[85]\ttraining's ndcg@1: 0.966265\ttraining's ndcg@2: 0.948118\ttraining's ndcg@3: 0.942949\ttraining's ndcg@4: 0.946283\ttraining's ndcg@5: 0.95258\tvalid_1's ndcg@1: 0.587634\tvalid_1's ndcg@2: 0.638887\tvalid_1's ndcg@3: 0.688356\tvalid_1's ndcg@4: 0.732858\tvalid_1's ndcg@5: 0.775015\n",
      "[86]\ttraining's ndcg@1: 0.966976\ttraining's ndcg@2: 0.949171\ttraining's ndcg@3: 0.943355\ttraining's ndcg@4: 0.946886\ttraining's ndcg@5: 0.953215\tvalid_1's ndcg@1: 0.5728\tvalid_1's ndcg@2: 0.638121\tvalid_1's ndcg@3: 0.685638\tvalid_1's ndcg@4: 0.735038\tvalid_1's ndcg@5: 0.772323\n",
      "[87]\ttraining's ndcg@1: 0.967687\ttraining's ndcg@2: 0.949883\ttraining's ndcg@3: 0.944423\ttraining's ndcg@4: 0.947767\ttraining's ndcg@5: 0.953956\tvalid_1's ndcg@1: 0.580929\tvalid_1's ndcg@2: 0.637093\tvalid_1's ndcg@3: 0.686102\tvalid_1's ndcg@4: 0.731731\tvalid_1's ndcg@5: 0.773183\n",
      "[88]\ttraining's ndcg@1: 0.967687\ttraining's ndcg@2: 0.95048\ttraining's ndcg@3: 0.945281\ttraining's ndcg@4: 0.948332\ttraining's ndcg@5: 0.954309\tvalid_1's ndcg@1: 0.58728\tvalid_1's ndcg@2: 0.635356\tvalid_1's ndcg@3: 0.689718\tvalid_1's ndcg@4: 0.733936\tvalid_1's ndcg@5: 0.772712\n",
      "[89]\ttraining's ndcg@1: 0.96981\ttraining's ndcg@2: 0.952131\ttraining's ndcg@3: 0.946351\ttraining's ndcg@4: 0.949663\ttraining's ndcg@5: 0.955718\tvalid_1's ndcg@1: 0.575274\tvalid_1's ndcg@2: 0.634784\tvalid_1's ndcg@3: 0.68357\tvalid_1's ndcg@4: 0.732409\tvalid_1's ndcg@5: 0.769624\n",
      "[90]\ttraining's ndcg@1: 0.96981\ttraining's ndcg@2: 0.952302\ttraining's ndcg@3: 0.946522\ttraining's ndcg@4: 0.949753\ttraining's ndcg@5: 0.955713\tvalid_1's ndcg@1: 0.575274\tvalid_1's ndcg@2: 0.626339\tvalid_1's ndcg@3: 0.68087\tvalid_1's ndcg@4: 0.730162\tvalid_1's ndcg@5: 0.768061\n",
      "[91]\ttraining's ndcg@1: 0.96981\ttraining's ndcg@2: 0.952392\ttraining's ndcg@3: 0.946492\ttraining's ndcg@4: 0.950136\ttraining's ndcg@5: 0.956102\tvalid_1's ndcg@1: 0.577395\tvalid_1's ndcg@2: 0.628441\tvalid_1's ndcg@3: 0.682837\tvalid_1's ndcg@4: 0.731952\tvalid_1's ndcg@5: 0.768868\n",
      "[92]\ttraining's ndcg@1: 0.971232\ttraining's ndcg@2: 0.952706\ttraining's ndcg@3: 0.948446\ttraining's ndcg@4: 0.951155\ttraining's ndcg@5: 0.95655\tvalid_1's ndcg@1: 0.578102\tvalid_1's ndcg@2: 0.632698\tvalid_1's ndcg@3: 0.681608\tvalid_1's ndcg@4: 0.732143\tvalid_1's ndcg@5: 0.769665\n",
      "[93]\ttraining's ndcg@1: 0.971232\ttraining's ndcg@2: 0.952701\ttraining's ndcg@3: 0.948454\ttraining's ndcg@4: 0.951389\ttraining's ndcg@5: 0.956947\tvalid_1's ndcg@1: 0.584464\tvalid_1's ndcg@2: 0.63708\tvalid_1's ndcg@3: 0.681659\tvalid_1's ndcg@4: 0.734321\tvalid_1's ndcg@5: 0.771589\n",
      "[94]\ttraining's ndcg@1: 0.971232\ttraining's ndcg@2: 0.952957\ttraining's ndcg@3: 0.948791\ttraining's ndcg@4: 0.951734\ttraining's ndcg@5: 0.95729\tvalid_1's ndcg@1: 0.584464\tvalid_1's ndcg@2: 0.634112\tvalid_1's ndcg@3: 0.6829\tvalid_1's ndcg@4: 0.735322\tvalid_1's ndcg@5: 0.771671\n",
      "[95]\ttraining's ndcg@1: 0.971588\ttraining's ndcg@2: 0.953576\ttraining's ndcg@3: 0.94909\ttraining's ndcg@4: 0.951929\ttraining's ndcg@5: 0.957492\tvalid_1's ndcg@1: 0.584618\tvalid_1's ndcg@2: 0.636362\tvalid_1's ndcg@3: 0.681793\tvalid_1's ndcg@4: 0.733998\tvalid_1's ndcg@5: 0.77109\n",
      "[96]\ttraining's ndcg@1: 0.971588\ttraining's ndcg@2: 0.954088\ttraining's ndcg@3: 0.949587\ttraining's ndcg@4: 0.951823\ttraining's ndcg@5: 0.957591\tvalid_1's ndcg@1: 0.584618\tvalid_1's ndcg@2: 0.635341\tvalid_1's ndcg@3: 0.68548\tvalid_1's ndcg@4: 0.734447\tvalid_1's ndcg@5: 0.771079\n",
      "[97]\ttraining's ndcg@1: 0.971943\ttraining's ndcg@2: 0.95452\ttraining's ndcg@3: 0.950217\ttraining's ndcg@4: 0.95212\ttraining's ndcg@5: 0.958245\tvalid_1's ndcg@1: 0.583584\tvalid_1's ndcg@2: 0.638696\tvalid_1's ndcg@3: 0.681464\tvalid_1's ndcg@4: 0.734723\tvalid_1's ndcg@5: 0.770093\n",
      "[98]\ttraining's ndcg@1: 0.971943\ttraining's ndcg@2: 0.954614\ttraining's ndcg@3: 0.950681\ttraining's ndcg@4: 0.95281\ttraining's ndcg@5: 0.95866\tvalid_1's ndcg@1: 0.582523\tvalid_1's ndcg@2: 0.633928\tvalid_1's ndcg@3: 0.679524\tvalid_1's ndcg@4: 0.733568\tvalid_1's ndcg@5: 0.769608\n",
      "[99]\ttraining's ndcg@1: 0.971943\ttraining's ndcg@2: 0.954784\ttraining's ndcg@3: 0.951429\ttraining's ndcg@4: 0.953419\ttraining's ndcg@5: 0.958797\tvalid_1's ndcg@1: 0.583207\tvalid_1's ndcg@2: 0.632\tvalid_1's ndcg@3: 0.682543\tvalid_1's ndcg@4: 0.732227\tvalid_1's ndcg@5: 0.770296\n",
      "[100]\ttraining's ndcg@1: 0.971943\ttraining's ndcg@2: 0.954955\ttraining's ndcg@3: 0.951884\ttraining's ndcg@4: 0.953779\ttraining's ndcg@5: 0.959103\tvalid_1's ndcg@1: 0.581087\tvalid_1's ndcg@2: 0.629747\tvalid_1's ndcg@3: 0.6815\tvalid_1's ndcg@4: 0.731733\tvalid_1's ndcg@5: 0.769222\n",
      "[101]\ttraining's ndcg@1: 0.971943\ttraining's ndcg@2: 0.956234\ttraining's ndcg@3: 0.95252\ttraining's ndcg@4: 0.953961\ttraining's ndcg@5: 0.959319\tvalid_1's ndcg@1: 0.579673\tvalid_1's ndcg@2: 0.629372\tvalid_1's ndcg@3: 0.679516\tvalid_1's ndcg@4: 0.729333\tvalid_1's ndcg@5: 0.768507\n",
      "[102]\ttraining's ndcg@1: 0.971943\ttraining's ndcg@2: 0.956319\ttraining's ndcg@3: 0.953242\ttraining's ndcg@4: 0.954341\ttraining's ndcg@5: 0.959452\tvalid_1's ndcg@1: 0.588867\tvalid_1's ndcg@2: 0.629752\tvalid_1's ndcg@3: 0.679615\tvalid_1's ndcg@4: 0.729315\tvalid_1's ndcg@5: 0.769401\n",
      "[103]\ttraining's ndcg@1: 0.972655\ttraining's ndcg@2: 0.957287\ttraining's ndcg@3: 0.953915\ttraining's ndcg@4: 0.955217\ttraining's ndcg@5: 0.960351\tvalid_1's ndcg@1: 0.593454\tvalid_1's ndcg@2: 0.632226\tvalid_1's ndcg@3: 0.684421\tvalid_1's ndcg@4: 0.731476\tvalid_1's ndcg@5: 0.769767\n",
      "[104]\ttraining's ndcg@1: 0.973366\ttraining's ndcg@2: 0.957486\ttraining's ndcg@3: 0.9542\ttraining's ndcg@4: 0.955207\ttraining's ndcg@5: 0.960635\tvalid_1's ndcg@1: 0.580376\tvalid_1's ndcg@2: 0.629573\tvalid_1's ndcg@3: 0.683535\tvalid_1's ndcg@4: 0.72588\tvalid_1's ndcg@5: 0.766444\n",
      "[105]\ttraining's ndcg@1: 0.973366\ttraining's ndcg@2: 0.957812\ttraining's ndcg@3: 0.954633\ttraining's ndcg@4: 0.955872\ttraining's ndcg@5: 0.960947\tvalid_1's ndcg@1: 0.580553\tvalid_1's ndcg@2: 0.630046\tvalid_1's ndcg@3: 0.683599\tvalid_1's ndcg@4: 0.726553\tvalid_1's ndcg@5: 0.766521\n",
      "[106]\ttraining's ndcg@1: 0.973366\ttraining's ndcg@2: 0.958579\ttraining's ndcg@3: 0.955199\ttraining's ndcg@4: 0.956071\ttraining's ndcg@5: 0.961297\tvalid_1's ndcg@1: 0.590803\tvalid_1's ndcg@2: 0.631567\tvalid_1's ndcg@3: 0.685057\tvalid_1's ndcg@4: 0.728133\tvalid_1's ndcg@5: 0.768441\n",
      "[107]\ttraining's ndcg@1: 0.973366\ttraining's ndcg@2: 0.959198\ttraining's ndcg@3: 0.955542\ttraining's ndcg@4: 0.95659\ttraining's ndcg@5: 0.961375\tvalid_1's ndcg@1: 0.593449\tvalid_1's ndcg@2: 0.629919\tvalid_1's ndcg@3: 0.681366\tvalid_1's ndcg@4: 0.727113\tvalid_1's ndcg@5: 0.769116\n",
      "[108]\ttraining's ndcg@1: 0.974077\ttraining's ndcg@2: 0.960165\ttraining's ndcg@3: 0.955839\ttraining's ndcg@4: 0.957115\ttraining's ndcg@5: 0.961592\tvalid_1's ndcg@1: 0.589683\tvalid_1's ndcg@2: 0.628956\tvalid_1's ndcg@3: 0.679087\tvalid_1's ndcg@4: 0.726887\tvalid_1's ndcg@5: 0.76802\n",
      "[109]\ttraining's ndcg@1: 0.974077\ttraining's ndcg@2: 0.960171\ttraining's ndcg@3: 0.956989\ttraining's ndcg@4: 0.95736\ttraining's ndcg@5: 0.961924\tvalid_1's ndcg@1: 0.59227\tvalid_1's ndcg@2: 0.629827\tvalid_1's ndcg@3: 0.680575\tvalid_1's ndcg@4: 0.725339\tvalid_1's ndcg@5: 0.768418\n",
      "[110]\ttraining's ndcg@1: 0.974077\ttraining's ndcg@2: 0.960892\ttraining's ndcg@3: 0.957188\ttraining's ndcg@4: 0.957458\ttraining's ndcg@5: 0.962115\tvalid_1's ndcg@1: 0.589473\tvalid_1's ndcg@2: 0.631213\tvalid_1's ndcg@3: 0.677248\tvalid_1's ndcg@4: 0.725713\tvalid_1's ndcg@5: 0.770612\n",
      "[111]\ttraining's ndcg@1: 0.974788\ttraining's ndcg@2: 0.96139\ttraining's ndcg@3: 0.957529\ttraining's ndcg@4: 0.957738\ttraining's ndcg@5: 0.962271\tvalid_1's ndcg@1: 0.586637\tvalid_1's ndcg@2: 0.628297\tvalid_1's ndcg@3: 0.675505\tvalid_1's ndcg@4: 0.726819\tvalid_1's ndcg@5: 0.767818\n",
      "[112]\ttraining's ndcg@1: 0.974788\ttraining's ndcg@2: 0.961561\ttraining's ndcg@3: 0.957835\ttraining's ndcg@4: 0.958138\ttraining's ndcg@5: 0.962448\tvalid_1's ndcg@1: 0.577922\tvalid_1's ndcg@2: 0.626434\tvalid_1's ndcg@3: 0.676715\tvalid_1's ndcg@4: 0.721942\tvalid_1's ndcg@5: 0.766684\n",
      "[113]\ttraining's ndcg@1: 0.974788\ttraining's ndcg@2: 0.961553\ttraining's ndcg@3: 0.957965\ttraining's ndcg@4: 0.958286\ttraining's ndcg@5: 0.962487\tvalid_1's ndcg@1: 0.578858\tvalid_1's ndcg@2: 0.626788\tvalid_1's ndcg@3: 0.68188\tvalid_1's ndcg@4: 0.723636\tvalid_1's ndcg@5: 0.768969\n",
      "[114]\ttraining's ndcg@1: 0.975499\ttraining's ndcg@2: 0.962264\ttraining's ndcg@3: 0.958213\ttraining's ndcg@4: 0.958884\ttraining's ndcg@5: 0.962807\tvalid_1's ndcg@1: 0.577239\tvalid_1's ndcg@2: 0.628425\tvalid_1's ndcg@3: 0.679824\tvalid_1's ndcg@4: 0.725899\tvalid_1's ndcg@5: 0.767825\n",
      "[115]\ttraining's ndcg@1: 0.975499\ttraining's ndcg@2: 0.962711\ttraining's ndcg@3: 0.958452\ttraining's ndcg@4: 0.958774\ttraining's ndcg@5: 0.963349\tvalid_1's ndcg@1: 0.577239\tvalid_1's ndcg@2: 0.629231\tvalid_1's ndcg@3: 0.68327\tvalid_1's ndcg@4: 0.728948\tvalid_1's ndcg@5: 0.768909\n",
      "[116]\ttraining's ndcg@1: 0.975499\ttraining's ndcg@2: 0.962711\ttraining's ndcg@3: 0.958753\ttraining's ndcg@4: 0.958774\ttraining's ndcg@5: 0.963329\tvalid_1's ndcg@1: 0.580067\tvalid_1's ndcg@2: 0.628963\tvalid_1's ndcg@3: 0.6813\tvalid_1's ndcg@4: 0.727333\tvalid_1's ndcg@5: 0.769435\n",
      "[117]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.962975\ttraining's ndcg@3: 0.959249\ttraining's ndcg@4: 0.959184\ttraining's ndcg@5: 0.963558\tvalid_1's ndcg@1: 0.580067\tvalid_1's ndcg@2: 0.629855\tvalid_1's ndcg@3: 0.68239\tvalid_1's ndcg@4: 0.723412\tvalid_1's ndcg@5: 0.769453\n",
      "[118]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.962983\ttraining's ndcg@3: 0.959413\ttraining's ndcg@4: 0.95947\ttraining's ndcg@5: 0.96366\tvalid_1's ndcg@1: 0.580244\tvalid_1's ndcg@2: 0.629055\tvalid_1's ndcg@3: 0.682274\tvalid_1's ndcg@4: 0.723377\tvalid_1's ndcg@5: 0.769566\n",
      "[119]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.96358\ttraining's ndcg@3: 0.959696\ttraining's ndcg@4: 0.960135\ttraining's ndcg@5: 0.963851\tvalid_1's ndcg@1: 0.583781\tvalid_1's ndcg@2: 0.629984\tvalid_1's ndcg@3: 0.679852\tvalid_1's ndcg@4: 0.727403\tvalid_1's ndcg@5: 0.771051\n",
      "[120]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.964011\ttraining's ndcg@3: 0.959697\ttraining's ndcg@4: 0.96001\ttraining's ndcg@5: 0.963936\tvalid_1's ndcg@1: 0.586432\tvalid_1's ndcg@2: 0.630048\tvalid_1's ndcg@3: 0.681353\tvalid_1's ndcg@4: 0.726147\tvalid_1's ndcg@5: 0.771507\n",
      "[121]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.964182\ttraining's ndcg@3: 0.960219\ttraining's ndcg@4: 0.9602\ttraining's ndcg@5: 0.964858\tvalid_1's ndcg@1: 0.586432\tvalid_1's ndcg@2: 0.633249\tvalid_1's ndcg@3: 0.681684\tvalid_1's ndcg@4: 0.729424\tvalid_1's ndcg@5: 0.773044\n",
      "[122]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.965179\ttraining's ndcg@3: 0.96081\ttraining's ndcg@4: 0.961035\ttraining's ndcg@5: 0.965291\tvalid_1's ndcg@1: 0.586432\tvalid_1's ndcg@2: 0.63485\tvalid_1's ndcg@3: 0.68456\tvalid_1's ndcg@4: 0.726432\tvalid_1's ndcg@5: 0.772576\n",
      "[123]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.965195\ttraining's ndcg@3: 0.960739\ttraining's ndcg@4: 0.961657\ttraining's ndcg@5: 0.965322\tvalid_1's ndcg@1: 0.578828\tvalid_1's ndcg@2: 0.631974\tvalid_1's ndcg@3: 0.681795\tvalid_1's ndcg@4: 0.725643\tvalid_1's ndcg@5: 0.770984\n",
      "[124]\ttraining's ndcg@1: 0.97621\ttraining's ndcg@2: 0.965512\ttraining's ndcg@3: 0.961035\ttraining's ndcg@4: 0.961913\ttraining's ndcg@5: 0.965965\tvalid_1's ndcg@1: 0.578298\tvalid_1's ndcg@2: 0.632587\tvalid_1's ndcg@3: 0.68205\tvalid_1's ndcg@4: 0.727608\tvalid_1's ndcg@5: 0.771172\n",
      "[125]\ttraining's ndcg@1: 0.976921\ttraining's ndcg@2: 0.966221\ttraining's ndcg@3: 0.96216\ttraining's ndcg@4: 0.962692\ttraining's ndcg@5: 0.966656\tvalid_1's ndcg@1: 0.585022\tvalid_1's ndcg@2: 0.627598\tvalid_1's ndcg@3: 0.681949\tvalid_1's ndcg@4: 0.724896\tvalid_1's ndcg@5: 0.76946\n",
      "[126]\ttraining's ndcg@1: 0.976921\ttraining's ndcg@2: 0.966221\ttraining's ndcg@3: 0.96267\ttraining's ndcg@4: 0.963609\ttraining's ndcg@5: 0.966688\tvalid_1's ndcg@1: 0.581487\tvalid_1's ndcg@2: 0.627792\tvalid_1's ndcg@3: 0.679611\tvalid_1's ndcg@4: 0.725598\tvalid_1's ndcg@5: 0.770206\n",
      "[127]\ttraining's ndcg@1: 0.976921\ttraining's ndcg@2: 0.966434\ttraining's ndcg@3: 0.963327\ttraining's ndcg@4: 0.963923\ttraining's ndcg@5: 0.967236\tvalid_1's ndcg@1: 0.58131\tvalid_1's ndcg@2: 0.625962\tvalid_1's ndcg@3: 0.679934\tvalid_1's ndcg@4: 0.726037\tvalid_1's ndcg@5: 0.770098\n",
      "[128]\ttraining's ndcg@1: 0.977633\ttraining's ndcg@2: 0.967188\ttraining's ndcg@3: 0.964157\ttraining's ndcg@4: 0.964693\ttraining's ndcg@5: 0.967612\tvalid_1's ndcg@1: 0.588357\tvalid_1's ndcg@2: 0.628194\tvalid_1's ndcg@3: 0.681873\tvalid_1's ndcg@4: 0.729214\tvalid_1's ndcg@5: 0.770497\n",
      "[129]\ttraining's ndcg@1: 0.977633\ttraining's ndcg@2: 0.967262\ttraining's ndcg@3: 0.964604\ttraining's ndcg@4: 0.964483\ttraining's ndcg@5: 0.967654\tvalid_1's ndcg@1: 0.591895\tvalid_1's ndcg@2: 0.629493\tvalid_1's ndcg@3: 0.682654\tvalid_1's ndcg@4: 0.728648\tvalid_1's ndcg@5: 0.771277\n",
      "[130]\ttraining's ndcg@1: 0.977633\ttraining's ndcg@2: 0.96868\ttraining's ndcg@3: 0.964903\ttraining's ndcg@4: 0.964977\ttraining's ndcg@5: 0.968112\tvalid_1's ndcg@1: 0.580783\tvalid_1's ndcg@2: 0.632479\tvalid_1's ndcg@3: 0.685627\tvalid_1's ndcg@4: 0.731057\tvalid_1's ndcg@5: 0.770627\n",
      "[131]\ttraining's ndcg@1: 0.978344\ttraining's ndcg@2: 0.969306\ttraining's ndcg@3: 0.965597\ttraining's ndcg@4: 0.965416\ttraining's ndcg@5: 0.96838\tvalid_1's ndcg@1: 0.586438\tvalid_1's ndcg@2: 0.633737\tvalid_1's ndcg@3: 0.686907\tvalid_1's ndcg@4: 0.732177\tvalid_1's ndcg@5: 0.77042\n",
      "[132]\ttraining's ndcg@1: 0.978344\ttraining's ndcg@2: 0.969721\ttraining's ndcg@3: 0.965861\ttraining's ndcg@4: 0.965709\ttraining's ndcg@5: 0.968612\tvalid_1's ndcg@1: 0.592507\tvalid_1's ndcg@2: 0.634867\tvalid_1's ndcg@3: 0.690638\tvalid_1's ndcg@4: 0.73354\tvalid_1's ndcg@5: 0.773978\n",
      "[133]\ttraining's ndcg@1: 0.978966\ttraining's ndcg@2: 0.970386\ttraining's ndcg@3: 0.966489\ttraining's ndcg@4: 0.966111\ttraining's ndcg@5: 0.969284\tvalid_1's ndcg@1: 0.598162\tvalid_1's ndcg@2: 0.637429\tvalid_1's ndcg@3: 0.691328\tvalid_1's ndcg@4: 0.735837\tvalid_1's ndcg@5: 0.77426\n",
      "[134]\ttraining's ndcg@1: 0.978966\ttraining's ndcg@2: 0.970706\ttraining's ndcg@3: 0.966612\ttraining's ndcg@4: 0.966242\ttraining's ndcg@5: 0.969409\tvalid_1's ndcg@1: 0.595334\tvalid_1's ndcg@2: 0.633445\tvalid_1's ndcg@3: 0.689998\tvalid_1's ndcg@4: 0.732024\tvalid_1's ndcg@5: 0.772744\n",
      "[135]\ttraining's ndcg@1: 0.978966\ttraining's ndcg@2: 0.970706\ttraining's ndcg@3: 0.966828\ttraining's ndcg@4: 0.966303\ttraining's ndcg@5: 0.969742\tvalid_1's ndcg@1: 0.594628\tvalid_1's ndcg@2: 0.632271\tvalid_1's ndcg@3: 0.690394\tvalid_1's ndcg@4: 0.734003\tvalid_1's ndcg@5: 0.773419\n",
      "[136]\ttraining's ndcg@1: 0.978966\ttraining's ndcg@2: 0.970706\ttraining's ndcg@3: 0.967136\ttraining's ndcg@4: 0.966449\ttraining's ndcg@5: 0.969884\tvalid_1's ndcg@1: 0.594628\tvalid_1's ndcg@2: 0.631477\tvalid_1's ndcg@3: 0.691286\tvalid_1's ndcg@4: 0.735228\tvalid_1's ndcg@5: 0.773396\n",
      "[137]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.970905\ttraining's ndcg@3: 0.967473\ttraining's ndcg@4: 0.966835\ttraining's ndcg@5: 0.970085\tvalid_1's ndcg@1: 0.590861\tvalid_1's ndcg@2: 0.633687\tvalid_1's ndcg@3: 0.689256\tvalid_1's ndcg@4: 0.734797\tvalid_1's ndcg@5: 0.773073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.970905\ttraining's ndcg@3: 0.967628\ttraining's ndcg@4: 0.966836\ttraining's ndcg@5: 0.970247\tvalid_1's ndcg@1: 0.588385\tvalid_1's ndcg@2: 0.634942\tvalid_1's ndcg@3: 0.688592\tvalid_1's ndcg@4: 0.734972\tvalid_1's ndcg@5: 0.772777\n",
      "[139]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.970905\ttraining's ndcg@3: 0.967707\ttraining's ndcg@4: 0.967065\ttraining's ndcg@5: 0.97046\tvalid_1's ndcg@1: 0.593861\tvalid_1's ndcg@2: 0.631561\tvalid_1's ndcg@3: 0.688572\tvalid_1's ndcg@4: 0.736688\tvalid_1's ndcg@5: 0.774554\n",
      "[140]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.971438\ttraining's ndcg@3: 0.968208\ttraining's ndcg@4: 0.967334\ttraining's ndcg@5: 0.970748\tvalid_1's ndcg@1: 0.594093\tvalid_1's ndcg@2: 0.632481\tvalid_1's ndcg@3: 0.689067\tvalid_1's ndcg@4: 0.739258\tvalid_1's ndcg@5: 0.77416\n",
      "[141]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.971779\ttraining's ndcg@3: 0.968303\ttraining's ndcg@4: 0.967231\ttraining's ndcg@5: 0.970825\tvalid_1's ndcg@1: 0.594093\tvalid_1's ndcg@2: 0.630827\tvalid_1's ndcg@3: 0.688677\tvalid_1's ndcg@4: 0.739532\tvalid_1's ndcg@5: 0.773911\n",
      "[142]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.971811\ttraining's ndcg@3: 0.968309\ttraining's ndcg@4: 0.967269\ttraining's ndcg@5: 0.970738\tvalid_1's ndcg@1: 0.587502\tvalid_1's ndcg@2: 0.628675\tvalid_1's ndcg@3: 0.68836\tvalid_1's ndcg@4: 0.73691\tvalid_1's ndcg@5: 0.773588\n",
      "[143]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.972323\ttraining's ndcg@3: 0.968576\ttraining's ndcg@4: 0.967404\ttraining's ndcg@5: 0.970917\tvalid_1's ndcg@1: 0.58738\tvalid_1's ndcg@2: 0.623646\tvalid_1's ndcg@3: 0.688438\tvalid_1's ndcg@4: 0.736006\tvalid_1's ndcg@5: 0.772779\n",
      "[144]\ttraining's ndcg@1: 0.979677\ttraining's ndcg@2: 0.972323\ttraining's ndcg@3: 0.968576\ttraining's ndcg@4: 0.967337\ttraining's ndcg@5: 0.971211\tvalid_1's ndcg@1: 0.587377\tvalid_1's ndcg@2: 0.627206\tvalid_1's ndcg@3: 0.688866\tvalid_1's ndcg@4: 0.736623\tvalid_1's ndcg@5: 0.773972\n",
      "[145]\ttraining's ndcg@1: 0.980388\ttraining's ndcg@2: 0.972842\ttraining's ndcg@3: 0.968819\ttraining's ndcg@4: 0.967706\ttraining's ndcg@5: 0.971449\tvalid_1's ndcg@1: 0.590735\tvalid_1's ndcg@2: 0.629632\tvalid_1's ndcg@3: 0.68964\tvalid_1's ndcg@4: 0.737497\tvalid_1's ndcg@5: 0.774922\n",
      "[146]\ttraining's ndcg@1: 0.980388\ttraining's ndcg@2: 0.972842\ttraining's ndcg@3: 0.968819\ttraining's ndcg@4: 0.967717\ttraining's ndcg@5: 0.97153\tvalid_1's ndcg@1: 0.590735\tvalid_1's ndcg@2: 0.628402\tvalid_1's ndcg@3: 0.689123\tvalid_1's ndcg@4: 0.737746\tvalid_1's ndcg@5: 0.773854\n",
      "[147]\ttraining's ndcg@1: 0.980388\ttraining's ndcg@2: 0.972842\ttraining's ndcg@3: 0.96934\ttraining's ndcg@4: 0.967819\ttraining's ndcg@5: 0.971633\tvalid_1's ndcg@1: 0.585789\tvalid_1's ndcg@2: 0.629515\tvalid_1's ndcg@3: 0.68744\tvalid_1's ndcg@4: 0.735674\tvalid_1's ndcg@5: 0.771341\n",
      "[148]\ttraining's ndcg@1: 0.980388\ttraining's ndcg@2: 0.973525\ttraining's ndcg@3: 0.969577\ttraining's ndcg@4: 0.967997\ttraining's ndcg@5: 0.971773\tvalid_1's ndcg@1: 0.580308\tvalid_1's ndcg@2: 0.625806\tvalid_1's ndcg@3: 0.685868\tvalid_1's ndcg@4: 0.73319\tvalid_1's ndcg@5: 0.769752\n",
      "[149]\ttraining's ndcg@1: 0.98181\ttraining's ndcg@2: 0.97418\ttraining's ndcg@3: 0.970099\ttraining's ndcg@4: 0.969016\ttraining's ndcg@5: 0.972312\tvalid_1's ndcg@1: 0.575006\tvalid_1's ndcg@2: 0.622155\tvalid_1's ndcg@3: 0.682994\tvalid_1's ndcg@4: 0.731466\tvalid_1's ndcg@5: 0.768664\n",
      "[150]\ttraining's ndcg@1: 0.98181\ttraining's ndcg@2: 0.974692\ttraining's ndcg@3: 0.970642\ttraining's ndcg@4: 0.969526\ttraining's ndcg@5: 0.972589\tvalid_1's ndcg@1: 0.595311\tvalid_1's ndcg@2: 0.62941\tvalid_1's ndcg@3: 0.689607\tvalid_1's ndcg@4: 0.738295\tvalid_1's ndcg@5: 0.774872\n",
      "[151]\ttraining's ndcg@1: 0.98181\ttraining's ndcg@2: 0.974692\ttraining's ndcg@3: 0.97069\ttraining's ndcg@4: 0.969506\ttraining's ndcg@5: 0.972656\tvalid_1's ndcg@1: 0.583081\tvalid_1's ndcg@2: 0.627773\tvalid_1's ndcg@3: 0.686731\tvalid_1's ndcg@4: 0.733197\tvalid_1's ndcg@5: 0.771315\n",
      "[152]\ttraining's ndcg@1: 0.982521\ttraining's ndcg@2: 0.975318\ttraining's ndcg@3: 0.970923\ttraining's ndcg@4: 0.969634\ttraining's ndcg@5: 0.972965\tvalid_1's ndcg@1: 0.582017\tvalid_1's ndcg@2: 0.637987\tvalid_1's ndcg@3: 0.687039\tvalid_1's ndcg@4: 0.733417\tvalid_1's ndcg@5: 0.772636\n",
      "[153]\ttraining's ndcg@1: 0.983232\ttraining's ndcg@2: 0.975517\ttraining's ndcg@3: 0.971197\ttraining's ndcg@4: 0.969888\ttraining's ndcg@5: 0.973132\tvalid_1's ndcg@1: 0.582548\tvalid_1's ndcg@2: 0.634533\tvalid_1's ndcg@3: 0.686492\tvalid_1's ndcg@4: 0.734495\tvalid_1's ndcg@5: 0.772861\n",
      "[154]\ttraining's ndcg@1: 0.983232\ttraining's ndcg@2: 0.97637\ttraining's ndcg@3: 0.971807\ttraining's ndcg@4: 0.970539\ttraining's ndcg@5: 0.973583\tvalid_1's ndcg@1: 0.58891\tvalid_1's ndcg@2: 0.638353\tvalid_1's ndcg@3: 0.690074\tvalid_1's ndcg@4: 0.735145\tvalid_1's ndcg@5: 0.773816\n",
      "[155]\ttraining's ndcg@1: 0.983232\ttraining's ndcg@2: 0.97654\ttraining's ndcg@3: 0.971772\ttraining's ndcg@4: 0.970542\ttraining's ndcg@5: 0.973623\tvalid_1's ndcg@1: 0.59283\tvalid_1's ndcg@2: 0.638315\tvalid_1's ndcg@3: 0.692386\tvalid_1's ndcg@4: 0.738378\tvalid_1's ndcg@5: 0.774085\n",
      "[156]\ttraining's ndcg@1: 0.983232\ttraining's ndcg@2: 0.976711\ttraining's ndcg@3: 0.971777\ttraining's ndcg@4: 0.970634\ttraining's ndcg@5: 0.973807\tvalid_1's ndcg@1: 0.590127\tvalid_1's ndcg@2: 0.636787\tvalid_1's ndcg@3: 0.687125\tvalid_1's ndcg@4: 0.737084\tvalid_1's ndcg@5: 0.773671\n",
      "[157]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.976911\ttraining's ndcg@3: 0.9721\ttraining's ndcg@4: 0.970816\ttraining's ndcg@5: 0.974042\tvalid_1's ndcg@1: 0.590709\tvalid_1's ndcg@2: 0.636957\tvalid_1's ndcg@3: 0.692527\tvalid_1's ndcg@4: 0.736415\tvalid_1's ndcg@5: 0.774261\n",
      "[158]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.977423\ttraining's ndcg@3: 0.972198\ttraining's ndcg@4: 0.971412\ttraining's ndcg@5: 0.974569\tvalid_1's ndcg@1: 0.592478\tvalid_1's ndcg@2: 0.639997\tvalid_1's ndcg@3: 0.692542\tvalid_1's ndcg@4: 0.737602\tvalid_1's ndcg@5: 0.775997\n",
      "[159]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.978106\ttraining's ndcg@3: 0.972404\ttraining's ndcg@4: 0.971573\ttraining's ndcg@5: 0.975102\tvalid_1's ndcg@1: 0.591948\tvalid_1's ndcg@2: 0.634846\tvalid_1's ndcg@3: 0.688558\tvalid_1's ndcg@4: 0.736392\tvalid_1's ndcg@5: 0.774485\n",
      "[160]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.97828\ttraining's ndcg@3: 0.973062\ttraining's ndcg@4: 0.97174\ttraining's ndcg@5: 0.975345\tvalid_1's ndcg@1: 0.58912\tvalid_1's ndcg@2: 0.637391\tvalid_1's ndcg@3: 0.689524\tvalid_1's ndcg@4: 0.736522\tvalid_1's ndcg@5: 0.775041\n",
      "[161]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.97845\ttraining's ndcg@3: 0.97332\ttraining's ndcg@4: 0.971968\ttraining's ndcg@5: 0.975428\tvalid_1's ndcg@1: 0.59036\tvalid_1's ndcg@2: 0.632737\tvalid_1's ndcg@3: 0.687052\tvalid_1's ndcg@4: 0.736719\tvalid_1's ndcg@5: 0.774802\n",
      "[162]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.979007\ttraining's ndcg@3: 0.973671\ttraining's ndcg@4: 0.972365\ttraining's ndcg@5: 0.975791\tvalid_1's ndcg@1: 0.5847\tvalid_1's ndcg@2: 0.630724\tvalid_1's ndcg@3: 0.687699\tvalid_1's ndcg@4: 0.734423\tvalid_1's ndcg@5: 0.773365\n",
      "[163]\ttraining's ndcg@1: 0.983943\ttraining's ndcg@2: 0.979519\ttraining's ndcg@3: 0.974061\ttraining's ndcg@4: 0.972485\ttraining's ndcg@5: 0.976015\tvalid_1's ndcg@1: 0.589296\tvalid_1's ndcg@2: 0.632014\tvalid_1's ndcg@3: 0.688049\tvalid_1's ndcg@4: 0.735769\tvalid_1's ndcg@5: 0.774404\n",
      "[164]\ttraining's ndcg@1: 0.984654\ttraining's ndcg@2: 0.979929\ttraining's ndcg@3: 0.974391\ttraining's ndcg@4: 0.973137\ttraining's ndcg@5: 0.976325\tvalid_1's ndcg@1: 0.592124\tvalid_1's ndcg@2: 0.634122\tvalid_1's ndcg@3: 0.687242\tvalid_1's ndcg@4: 0.734634\tvalid_1's ndcg@5: 0.774176\n",
      "[165]\ttraining's ndcg@1: 0.985365\ttraining's ndcg@2: 0.980619\ttraining's ndcg@3: 0.975022\ttraining's ndcg@4: 0.973418\ttraining's ndcg@5: 0.976602\tvalid_1's ndcg@1: 0.592124\tvalid_1's ndcg@2: 0.635652\tvalid_1's ndcg@3: 0.686627\tvalid_1's ndcg@4: 0.736086\tvalid_1's ndcg@5: 0.77491\n",
      "[166]\ttraining's ndcg@1: 0.985365\ttraining's ndcg@2: 0.980619\ttraining's ndcg@3: 0.97513\ttraining's ndcg@4: 0.973426\ttraining's ndcg@5: 0.97663\tvalid_1's ndcg@1: 0.595491\tvalid_1's ndcg@2: 0.635067\tvalid_1's ndcg@3: 0.688413\tvalid_1's ndcg@4: 0.736012\tvalid_1's ndcg@5: 0.776009\n",
      "[167]\ttraining's ndcg@1: 0.986076\ttraining's ndcg@2: 0.980989\ttraining's ndcg@3: 0.975499\ttraining's ndcg@4: 0.973737\ttraining's ndcg@5: 0.976972\tvalid_1's ndcg@1: 0.59266\tvalid_1's ndcg@2: 0.634273\tvalid_1's ndcg@3: 0.689498\tvalid_1's ndcg@4: 0.739502\tvalid_1's ndcg@5: 0.777111\n",
      "[168]\ttraining's ndcg@1: 0.986788\ttraining's ndcg@2: 0.981149\ttraining's ndcg@3: 0.975668\ttraining's ndcg@4: 0.974065\ttraining's ndcg@5: 0.977196\tvalid_1's ndcg@1: 0.595311\tvalid_1's ndcg@2: 0.638285\tvalid_1's ndcg@3: 0.69296\tvalid_1's ndcg@4: 0.737889\tvalid_1's ndcg@5: 0.778561\n",
      "[169]\ttraining's ndcg@1: 0.986788\ttraining's ndcg@2: 0.981359\ttraining's ndcg@3: 0.97578\ttraining's ndcg@4: 0.974177\ttraining's ndcg@5: 0.977554\tvalid_1's ndcg@1: 0.587177\tvalid_1's ndcg@2: 0.634136\tvalid_1's ndcg@3: 0.691012\tvalid_1's ndcg@4: 0.73719\tvalid_1's ndcg@5: 0.775635\n",
      "[170]\ttraining's ndcg@1: 0.986788\ttraining's ndcg@2: 0.98153\ttraining's ndcg@3: 0.975887\ttraining's ndcg@4: 0.974257\ttraining's ndcg@5: 0.977664\tvalid_1's ndcg@1: 0.59849\tvalid_1's ndcg@2: 0.644223\tvalid_1's ndcg@3: 0.693044\tvalid_1's ndcg@4: 0.739593\tvalid_1's ndcg@5: 0.779816\n",
      "[171]\ttraining's ndcg@1: 0.986788\ttraining's ndcg@2: 0.98153\ttraining's ndcg@3: 0.975973\ttraining's ndcg@4: 0.974344\ttraining's ndcg@5: 0.977716\tvalid_1's ndcg@1: 0.594955\tvalid_1's ndcg@2: 0.646301\tvalid_1's ndcg@3: 0.692873\tvalid_1's ndcg@4: 0.740014\tvalid_1's ndcg@5: 0.779532\n",
      "[172]\ttraining's ndcg@1: 0.986788\ttraining's ndcg@2: 0.982212\ttraining's ndcg@3: 0.976216\ttraining's ndcg@4: 0.974535\ttraining's ndcg@5: 0.977876\tvalid_1's ndcg@1: 0.587381\tvalid_1's ndcg@2: 0.637603\tvalid_1's ndcg@3: 0.689008\tvalid_1's ndcg@4: 0.736468\tvalid_1's ndcg@5: 0.775141\n",
      "[173]\ttraining's ndcg@1: 0.986788\ttraining's ndcg@2: 0.982468\ttraining's ndcg@3: 0.97625\ttraining's ndcg@4: 0.974923\ttraining's ndcg@5: 0.978103\tvalid_1's ndcg@1: 0.585783\tvalid_1's ndcg@2: 0.634958\tvalid_1's ndcg@3: 0.6868\tvalid_1's ndcg@4: 0.735868\tvalid_1's ndcg@5: 0.774753\n",
      "[174]\ttraining's ndcg@1: 0.987499\ttraining's ndcg@2: 0.982667\ttraining's ndcg@3: 0.976404\ttraining's ndcg@4: 0.975056\ttraining's ndcg@5: 0.978425\tvalid_1's ndcg@1: 0.588614\tvalid_1's ndcg@2: 0.636168\tvalid_1's ndcg@3: 0.684104\tvalid_1's ndcg@4: 0.736176\tvalid_1's ndcg@5: 0.775687\n",
      "[175]\ttraining's ndcg@1: 0.987499\ttraining's ndcg@2: 0.98271\ttraining's ndcg@3: 0.976413\ttraining's ndcg@4: 0.975436\ttraining's ndcg@5: 0.978453\tvalid_1's ndcg@1: 0.585963\tvalid_1's ndcg@2: 0.632881\tvalid_1's ndcg@3: 0.682943\tvalid_1's ndcg@4: 0.73447\tvalid_1's ndcg@5: 0.772726\n",
      "[176]\ttraining's ndcg@1: 0.987499\ttraining's ndcg@2: 0.983352\ttraining's ndcg@3: 0.976565\ttraining's ndcg@4: 0.975512\ttraining's ndcg@5: 0.978383\tvalid_1's ndcg@1: 0.582959\tvalid_1's ndcg@2: 0.630714\tvalid_1's ndcg@3: 0.683642\tvalid_1's ndcg@4: 0.735158\tvalid_1's ndcg@5: 0.771474\n",
      "[177]\ttraining's ndcg@1: 0.987499\ttraining's ndcg@2: 0.983352\ttraining's ndcg@3: 0.97675\ttraining's ndcg@4: 0.975569\ttraining's ndcg@5: 0.978626\tvalid_1's ndcg@1: 0.582959\tvalid_1's ndcg@2: 0.63241\tvalid_1's ndcg@3: 0.684792\tvalid_1's ndcg@4: 0.735018\tvalid_1's ndcg@5: 0.772237\n",
      "[178]\ttraining's ndcg@1: 0.989276\ttraining's ndcg@2: 0.983851\ttraining's ndcg@3: 0.977229\ttraining's ndcg@4: 0.9763\ttraining's ndcg@5: 0.979221\tvalid_1's ndcg@1: 0.579192\tvalid_1's ndcg@2: 0.627531\tvalid_1's ndcg@3: 0.680106\tvalid_1's ndcg@4: 0.732809\tvalid_1's ndcg@5: 0.7708\n",
      "[179]\ttraining's ndcg@1: 0.989276\ttraining's ndcg@2: 0.983851\ttraining's ndcg@3: 0.97731\ttraining's ndcg@4: 0.976397\ttraining's ndcg@5: 0.979248\tvalid_1's ndcg@1: 0.57748\tvalid_1's ndcg@2: 0.631699\tvalid_1's ndcg@3: 0.67875\tvalid_1's ndcg@4: 0.733301\tvalid_1's ndcg@5: 0.770863\n",
      "[180]\ttraining's ndcg@1: 0.989276\ttraining's ndcg@2: 0.984021\ttraining's ndcg@3: 0.977342\ttraining's ndcg@4: 0.976479\ttraining's ndcg@5: 0.97935\tvalid_1's ndcg@1: 0.582428\tvalid_1's ndcg@2: 0.63419\tvalid_1's ndcg@3: 0.681087\tvalid_1's ndcg@4: 0.734841\tvalid_1's ndcg@5: 0.773415\n",
      "[181]\ttraining's ndcg@1: 0.989276\ttraining's ndcg@2: 0.984021\ttraining's ndcg@3: 0.977342\ttraining's ndcg@4: 0.976505\ttraining's ndcg@5: 0.979324\tvalid_1's ndcg@1: 0.585255\tvalid_1's ndcg@2: 0.636703\tvalid_1's ndcg@3: 0.682568\tvalid_1's ndcg@4: 0.73514\tvalid_1's ndcg@5: 0.774168\n",
      "[182]\ttraining's ndcg@1: 0.989276\ttraining's ndcg@2: 0.984021\ttraining's ndcg@3: 0.977343\ttraining's ndcg@4: 0.976804\ttraining's ndcg@5: 0.979363\tvalid_1's ndcg@1: 0.581013\tvalid_1's ndcg@2: 0.632603\tvalid_1's ndcg@3: 0.680053\tvalid_1's ndcg@4: 0.733237\tvalid_1's ndcg@5: 0.772181\n",
      "[183]\ttraining's ndcg@1: 0.989276\ttraining's ndcg@2: 0.984021\ttraining's ndcg@3: 0.977528\ttraining's ndcg@4: 0.977024\ttraining's ndcg@5: 0.979845\tvalid_1's ndcg@1: 0.581013\tvalid_1's ndcg@2: 0.634769\tvalid_1's ndcg@3: 0.681692\tvalid_1's ndcg@4: 0.733843\tvalid_1's ndcg@5: 0.772334\n",
      "[184]\ttraining's ndcg@1: 0.989987\ttraining's ndcg@2: 0.984391\ttraining's ndcg@3: 0.977928\ttraining's ndcg@4: 0.97733\ttraining's ndcg@5: 0.980313\tvalid_1's ndcg@1: 0.583841\tvalid_1's ndcg@2: 0.6382\tvalid_1's ndcg@3: 0.681521\tvalid_1's ndcg@4: 0.735042\tvalid_1's ndcg@5: 0.773343\n",
      "[185]\ttraining's ndcg@1: 0.990698\ttraining's ndcg@2: 0.984591\ttraining's ndcg@3: 0.978148\ttraining's ndcg@4: 0.977539\ttraining's ndcg@5: 0.980533\tvalid_1's ndcg@1: 0.586849\tvalid_1's ndcg@2: 0.637768\tvalid_1's ndcg@3: 0.681851\tvalid_1's ndcg@4: 0.735654\tvalid_1's ndcg@5: 0.774372\n",
      "[186]\ttraining's ndcg@1: 0.990698\ttraining's ndcg@2: 0.984631\ttraining's ndcg@3: 0.978705\ttraining's ndcg@4: 0.977703\ttraining's ndcg@5: 0.980802\tvalid_1's ndcg@1: 0.596746\tvalid_1's ndcg@2: 0.640206\tvalid_1's ndcg@3: 0.686751\tvalid_1's ndcg@4: 0.738617\tvalid_1's ndcg@5: 0.777053\n",
      "[187]\ttraining's ndcg@1: 0.990698\ttraining's ndcg@2: 0.985057\ttraining's ndcg@3: 0.979681\ttraining's ndcg@4: 0.978292\ttraining's ndcg@5: 0.98103\tvalid_1's ndcg@1: 0.591798\tvalid_1's ndcg@2: 0.641191\tvalid_1's ndcg@3: 0.684398\tvalid_1's ndcg@4: 0.737266\tvalid_1's ndcg@5: 0.776219\n",
      "[188]\ttraining's ndcg@1: 0.99141\ttraining's ndcg@2: 0.985769\ttraining's ndcg@3: 0.97996\ttraining's ndcg@4: 0.978773\ttraining's ndcg@5: 0.981369\tvalid_1's ndcg@1: 0.589147\tvalid_1's ndcg@2: 0.637904\tvalid_1's ndcg@3: 0.683435\tvalid_1's ndcg@4: 0.736226\tvalid_1's ndcg@5: 0.77419\n",
      "[189]\ttraining's ndcg@1: 0.99141\ttraining's ndcg@2: 0.985939\ttraining's ndcg@3: 0.980055\ttraining's ndcg@4: 0.978815\ttraining's ndcg@5: 0.981417\tvalid_1's ndcg@1: 0.584726\tvalid_1's ndcg@2: 0.63732\tvalid_1's ndcg@3: 0.684513\tvalid_1's ndcg@4: 0.735131\tvalid_1's ndcg@5: 0.773692\n",
      "[190]\ttraining's ndcg@1: 0.99141\ttraining's ndcg@2: 0.98611\ttraining's ndcg@3: 0.980095\ttraining's ndcg@4: 0.978896\ttraining's ndcg@5: 0.981509\tvalid_1's ndcg@1: 0.584726\tvalid_1's ndcg@2: 0.638952\tvalid_1's ndcg@3: 0.682306\tvalid_1's ndcg@4: 0.735494\tvalid_1's ndcg@5: 0.774306\n",
      "[191]\ttraining's ndcg@1: 0.99141\ttraining's ndcg@2: 0.98628\ttraining's ndcg@3: 0.980436\ttraining's ndcg@4: 0.978978\ttraining's ndcg@5: 0.981603\tvalid_1's ndcg@1: 0.59301\tvalid_1's ndcg@2: 0.640768\tvalid_1's ndcg@3: 0.682612\tvalid_1's ndcg@4: 0.73643\tvalid_1's ndcg@5: 0.77609\n",
      "[192]\ttraining's ndcg@1: 0.99141\ttraining's ndcg@2: 0.986707\ttraining's ndcg@3: 0.98092\ttraining's ndcg@4: 0.979129\ttraining's ndcg@5: 0.981871\tvalid_1's ndcg@1: 0.597251\tvalid_1's ndcg@2: 0.639584\tvalid_1's ndcg@3: 0.685147\tvalid_1's ndcg@4: 0.737145\tvalid_1's ndcg@5: 0.776929\n",
      "[193]\ttraining's ndcg@1: 0.992121\ttraining's ndcg@2: 0.987077\ttraining's ndcg@3: 0.981349\ttraining's ndcg@4: 0.979313\ttraining's ndcg@5: 0.982084\tvalid_1's ndcg@1: 0.597251\tvalid_1's ndcg@2: 0.638566\tvalid_1's ndcg@3: 0.683297\tvalid_1's ndcg@4: 0.736844\tvalid_1's ndcg@5: 0.777347\n",
      "[194]\ttraining's ndcg@1: 0.992121\ttraining's ndcg@2: 0.987333\ttraining's ndcg@3: 0.981459\ttraining's ndcg@4: 0.979631\ttraining's ndcg@5: 0.982111\tvalid_1's ndcg@1: 0.593014\tvalid_1's ndcg@2: 0.640939\tvalid_1's ndcg@3: 0.685694\tvalid_1's ndcg@4: 0.736237\tvalid_1's ndcg@5: 0.777843\n",
      "[195]\ttraining's ndcg@1: 0.992121\ttraining's ndcg@2: 0.988186\ttraining's ndcg@3: 0.981933\ttraining's ndcg@4: 0.979989\ttraining's ndcg@5: 0.982395\tvalid_1's ndcg@1: 0.597782\tvalid_1's ndcg@2: 0.645414\tvalid_1's ndcg@3: 0.687204\tvalid_1's ndcg@4: 0.742442\tvalid_1's ndcg@5: 0.779216\n",
      "[196]\ttraining's ndcg@1: 0.992121\ttraining's ndcg@2: 0.988357\ttraining's ndcg@3: 0.981965\ttraining's ndcg@4: 0.980034\ttraining's ndcg@5: 0.982475\tvalid_1's ndcg@1: 0.595661\tvalid_1's ndcg@2: 0.641766\tvalid_1's ndcg@3: 0.685652\tvalid_1's ndcg@4: 0.741507\tvalid_1's ndcg@5: 0.778092\n",
      "[197]\ttraining's ndcg@1: 0.992121\ttraining's ndcg@2: 0.988528\ttraining's ndcg@3: 0.982059\ttraining's ndcg@4: 0.980204\ttraining's ndcg@5: 0.982612\tvalid_1's ndcg@1: 0.596346\tvalid_1's ndcg@2: 0.639539\tvalid_1's ndcg@3: 0.687363\tvalid_1's ndcg@4: 0.740932\tvalid_1's ndcg@5: 0.778144\n",
      "[198]\ttraining's ndcg@1: 0.992121\ttraining's ndcg@2: 0.988528\ttraining's ndcg@3: 0.982336\ttraining's ndcg@4: 0.980267\ttraining's ndcg@5: 0.982671\tvalid_1's ndcg@1: 0.600587\tvalid_1's ndcg@2: 0.640476\tvalid_1's ndcg@3: 0.690489\tvalid_1's ndcg@4: 0.742826\tvalid_1's ndcg@5: 0.779334\n",
      "[199]\ttraining's ndcg@1: 0.992832\ttraining's ndcg@2: 0.989154\ttraining's ndcg@3: 0.982662\ttraining's ndcg@4: 0.980632\ttraining's ndcg@5: 0.982806\tvalid_1's ndcg@1: 0.600587\tvalid_1's ndcg@2: 0.645986\tvalid_1's ndcg@3: 0.689162\tvalid_1's ndcg@4: 0.741454\tvalid_1's ndcg@5: 0.779753\n",
      "[200]\ttraining's ndcg@1: 0.992832\ttraining's ndcg@2: 0.989154\ttraining's ndcg@3: 0.982728\ttraining's ndcg@4: 0.980576\ttraining's ndcg@5: 0.982772\tvalid_1's ndcg@1: 0.600587\tvalid_1's ndcg@2: 0.640476\tvalid_1's ndcg@3: 0.687651\tvalid_1's ndcg@4: 0.73993\tvalid_1's ndcg@5: 0.778227\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 1).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "# Split the data into input features (X) and target labels (y)\n",
    "X_train = train_data[columns_to_normalize]\n",
    "y_train = train_data['rank']\n",
    "\n",
    "X_test = test_data[columns_to_normalize]\n",
    "y_test = test_data['rank']\n",
    "\n",
    "train_groups = train_data.groupby('keyword').size().to_frame('size')['size'].to_numpy()\n",
    "test_groups = test_data.groupby('keyword').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "# Create DMatrix for training set\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtrain.set_group(train_groups)\n",
    "\n",
    "# Create DMatrix for test set\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "dtest.set_group(test_groups)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': ['ndcg'],\n",
    "    'random_state': 21,\n",
    "    'lambdarank_num_pair_per_sample': 80,\n",
    "    'lambdarank_pair_method': 'mean',\n",
    "#     'max_depth': 60,\n",
    "    'reg_lambda': 1\n",
    "}\n",
    "# Train the model\n",
    "lgb_train = lgb.Dataset(X_train, y_train, group=train_groups)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, group=test_groups)\n",
    "ranker = lgb.train(params, lgb_train, num_boost_round=200, valid_sets=[lgb_train, lgb_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c475f4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9483330029218707, 0.9626745644463237, 1.0, 0.7614636371595962)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores3b =[]\n",
    "for query, group in test_data.groupby('keyword'):\n",
    "    if len(group['rank'].tolist()) != 1:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "        scores3b.append(nd(np.asarray([ranks]), np.asarray([ranker.predict(features).tolist()])))\n",
    "np.mean(scores3b), np.median(scores3b), max(scores3b), min(scores3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2397650",
   "metadata": {},
   "source": [
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c18bff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10265\n",
      "[LightGBM] [Info] Number of data points in the train set: 6640, number of used features: 71\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_num_pair_per_sample\n",
      "[LightGBM] [Warning] Unknown parameter: lambdarank_pair_method\n",
      "[1]\ttraining's ndcg@1: 0.240596\ttraining's ndcg@2: 0.360808\ttraining's ndcg@3: 0.443287\ttraining's ndcg@4: 0.526239\ttraining's ndcg@5: 0.597769\tvalid_1's ndcg@1: 0.226644\tvalid_1's ndcg@2: 0.327317\tvalid_1's ndcg@3: 0.413747\tvalid_1's ndcg@4: 0.497441\tvalid_1's ndcg@5: 0.56722\n",
      "[2]\ttraining's ndcg@1: 0.561415\ttraining's ndcg@2: 0.609766\ttraining's ndcg@3: 0.662656\ttraining's ndcg@4: 0.711881\ttraining's ndcg@5: 0.756601\tvalid_1's ndcg@1: 0.434819\tvalid_1's ndcg@2: 0.493808\tvalid_1's ndcg@3: 0.565299\tvalid_1's ndcg@4: 0.632909\tvalid_1's ndcg@5: 0.680262\n",
      "[3]\ttraining's ndcg@1: 0.642348\ttraining's ndcg@2: 0.684253\ttraining's ndcg@3: 0.716041\ttraining's ndcg@4: 0.757958\ttraining's ndcg@5: 0.792801\tvalid_1's ndcg@1: 0.437692\tvalid_1's ndcg@2: 0.508249\tvalid_1's ndcg@3: 0.580792\tvalid_1's ndcg@4: 0.64567\tvalid_1's ndcg@5: 0.691576\n",
      "[4]\ttraining's ndcg@1: 0.680328\ttraining's ndcg@2: 0.703949\ttraining's ndcg@3: 0.737548\ttraining's ndcg@4: 0.777575\ttraining's ndcg@5: 0.805657\tvalid_1's ndcg@1: 0.421051\tvalid_1's ndcg@2: 0.528491\tvalid_1's ndcg@3: 0.594422\tvalid_1's ndcg@4: 0.651934\tvalid_1's ndcg@5: 0.695241\n",
      "[5]\ttraining's ndcg@1: 0.696595\ttraining's ndcg@2: 0.719184\ttraining's ndcg@3: 0.753143\ttraining's ndcg@4: 0.787547\ttraining's ndcg@5: 0.81565\tvalid_1's ndcg@1: 0.462639\tvalid_1's ndcg@2: 0.539662\tvalid_1's ndcg@3: 0.604307\tvalid_1's ndcg@4: 0.661123\tvalid_1's ndcg@5: 0.707754\n",
      "[6]\ttraining's ndcg@1: 0.723673\ttraining's ndcg@2: 0.740104\ttraining's ndcg@3: 0.775275\ttraining's ndcg@4: 0.802143\ttraining's ndcg@5: 0.827577\tvalid_1's ndcg@1: 0.491802\tvalid_1's ndcg@2: 0.560922\tvalid_1's ndcg@3: 0.625546\tvalid_1's ndcg@4: 0.674342\tvalid_1's ndcg@5: 0.719562\n",
      "[7]\ttraining's ndcg@1: 0.735991\ttraining's ndcg@2: 0.748707\ttraining's ndcg@3: 0.781877\ttraining's ndcg@4: 0.805484\ttraining's ndcg@5: 0.831994\tvalid_1's ndcg@1: 0.497595\tvalid_1's ndcg@2: 0.558783\tvalid_1's ndcg@3: 0.628407\tvalid_1's ndcg@4: 0.677243\tvalid_1's ndcg@5: 0.72109\n",
      "[8]\ttraining's ndcg@1: 0.758016\ttraining's ndcg@2: 0.758537\ttraining's ndcg@3: 0.786925\ttraining's ndcg@4: 0.813685\ttraining's ndcg@5: 0.839183\tvalid_1's ndcg@1: 0.469772\tvalid_1's ndcg@2: 0.557747\tvalid_1's ndcg@3: 0.626448\tvalid_1's ndcg@4: 0.668967\tvalid_1's ndcg@5: 0.716062\n",
      "[9]\ttraining's ndcg@1: 0.758283\ttraining's ndcg@2: 0.764985\ttraining's ndcg@3: 0.794053\ttraining's ndcg@4: 0.819026\ttraining's ndcg@5: 0.842306\tvalid_1's ndcg@1: 0.470773\tvalid_1's ndcg@2: 0.55085\tvalid_1's ndcg@3: 0.619572\tvalid_1's ndcg@4: 0.669536\tvalid_1's ndcg@5: 0.718609\n",
      "[10]\ttraining's ndcg@1: 0.7726\ttraining's ndcg@2: 0.776072\ttraining's ndcg@3: 0.803014\ttraining's ndcg@4: 0.824932\ttraining's ndcg@5: 0.848589\tvalid_1's ndcg@1: 0.463326\tvalid_1's ndcg@2: 0.553908\tvalid_1's ndcg@3: 0.625841\tvalid_1's ndcg@4: 0.67404\tvalid_1's ndcg@5: 0.716761\n",
      "[11]\ttraining's ndcg@1: 0.783138\ttraining's ndcg@2: 0.78375\ttraining's ndcg@3: 0.807204\ttraining's ndcg@4: 0.832565\ttraining's ndcg@5: 0.853725\tvalid_1's ndcg@1: 0.478611\tvalid_1's ndcg@2: 0.553565\tvalid_1's ndcg@3: 0.620992\tvalid_1's ndcg@4: 0.673379\tvalid_1's ndcg@5: 0.720316\n",
      "[12]\ttraining's ndcg@1: 0.795463\ttraining's ndcg@2: 0.793752\ttraining's ndcg@3: 0.81594\ttraining's ndcg@4: 0.836741\ttraining's ndcg@5: 0.857955\tvalid_1's ndcg@1: 0.491668\tvalid_1's ndcg@2: 0.562377\tvalid_1's ndcg@3: 0.627378\tvalid_1's ndcg@4: 0.683255\tvalid_1's ndcg@5: 0.72818\n",
      "[13]\ttraining's ndcg@1: 0.804889\ttraining's ndcg@2: 0.805052\ttraining's ndcg@3: 0.822661\ttraining's ndcg@4: 0.845317\ttraining's ndcg@5: 0.863982\tvalid_1's ndcg@1: 0.490565\tvalid_1's ndcg@2: 0.558287\tvalid_1's ndcg@3: 0.629298\tvalid_1's ndcg@4: 0.683202\tvalid_1's ndcg@5: 0.728407\n",
      "[14]\ttraining's ndcg@1: 0.810936\ttraining's ndcg@2: 0.807391\ttraining's ndcg@3: 0.830492\ttraining's ndcg@4: 0.84758\ttraining's ndcg@5: 0.868077\tvalid_1's ndcg@1: 0.504075\tvalid_1's ndcg@2: 0.551815\tvalid_1's ndcg@3: 0.633969\tvalid_1's ndcg@4: 0.685955\tvalid_1's ndcg@5: 0.727495\n",
      "[15]\ttraining's ndcg@1: 0.814447\ttraining's ndcg@2: 0.812759\ttraining's ndcg@3: 0.833611\ttraining's ndcg@4: 0.850147\ttraining's ndcg@5: 0.871211\tvalid_1's ndcg@1: 0.519074\tvalid_1's ndcg@2: 0.57648\tvalid_1's ndcg@3: 0.635831\tvalid_1's ndcg@4: 0.698105\tvalid_1's ndcg@5: 0.734615\n",
      "[16]\ttraining's ndcg@1: 0.819069\ttraining's ndcg@2: 0.819699\ttraining's ndcg@3: 0.837469\ttraining's ndcg@4: 0.854521\ttraining's ndcg@5: 0.876277\tvalid_1's ndcg@1: 0.540947\tvalid_1's ndcg@2: 0.579648\tvalid_1's ndcg@3: 0.640047\tvalid_1's ndcg@4: 0.698294\tvalid_1's ndcg@5: 0.739723\n",
      "[17]\ttraining's ndcg@1: 0.827982\ttraining's ndcg@2: 0.825555\ttraining's ndcg@3: 0.843403\ttraining's ndcg@4: 0.858024\ttraining's ndcg@5: 0.879739\tvalid_1's ndcg@1: 0.542495\tvalid_1's ndcg@2: 0.575883\tvalid_1's ndcg@3: 0.642769\tvalid_1's ndcg@4: 0.702133\tvalid_1's ndcg@5: 0.741405\n",
      "[18]\ttraining's ndcg@1: 0.832983\ttraining's ndcg@2: 0.830257\ttraining's ndcg@3: 0.846684\ttraining's ndcg@4: 0.862895\ttraining's ndcg@5: 0.881697\tvalid_1's ndcg@1: 0.522891\tvalid_1's ndcg@2: 0.563658\tvalid_1's ndcg@3: 0.638205\tvalid_1's ndcg@4: 0.697152\tvalid_1's ndcg@5: 0.737028\n",
      "[19]\ttraining's ndcg@1: 0.837353\ttraining's ndcg@2: 0.833159\ttraining's ndcg@3: 0.848912\ttraining's ndcg@4: 0.864484\ttraining's ndcg@5: 0.884357\tvalid_1's ndcg@1: 0.527888\tvalid_1's ndcg@2: 0.577748\tvalid_1's ndcg@3: 0.638461\tvalid_1's ndcg@4: 0.702794\tvalid_1's ndcg@5: 0.739429\n",
      "[20]\ttraining's ndcg@1: 0.842865\ttraining's ndcg@2: 0.837255\ttraining's ndcg@3: 0.851763\ttraining's ndcg@4: 0.868422\ttraining's ndcg@5: 0.887115\tvalid_1's ndcg@1: 0.522726\tvalid_1's ndcg@2: 0.572933\tvalid_1's ndcg@3: 0.635718\tvalid_1's ndcg@4: 0.698164\tvalid_1's ndcg@5: 0.736458\n",
      "[21]\ttraining's ndcg@1: 0.847062\ttraining's ndcg@2: 0.841986\ttraining's ndcg@3: 0.857518\ttraining's ndcg@4: 0.873338\ttraining's ndcg@5: 0.890228\tvalid_1's ndcg@1: 0.532671\tvalid_1's ndcg@2: 0.58177\tvalid_1's ndcg@3: 0.640624\tvalid_1's ndcg@4: 0.701838\tvalid_1's ndcg@5: 0.73853\n",
      "[22]\ttraining's ndcg@1: 0.850792\ttraining's ndcg@2: 0.844749\ttraining's ndcg@3: 0.859143\ttraining's ndcg@4: 0.875971\ttraining's ndcg@5: 0.892185\tvalid_1's ndcg@1: 0.530418\tvalid_1's ndcg@2: 0.586462\tvalid_1's ndcg@3: 0.651462\tvalid_1's ndcg@4: 0.708598\tvalid_1's ndcg@5: 0.738646\n",
      "[23]\ttraining's ndcg@1: 0.856346\ttraining's ndcg@2: 0.848808\ttraining's ndcg@3: 0.863879\ttraining's ndcg@4: 0.877915\ttraining's ndcg@5: 0.894161\tvalid_1's ndcg@1: 0.510547\tvalid_1's ndcg@2: 0.576306\tvalid_1's ndcg@3: 0.64374\tvalid_1's ndcg@4: 0.699955\tvalid_1's ndcg@5: 0.734048\n",
      "[24]\ttraining's ndcg@1: 0.85868\ttraining's ndcg@2: 0.851782\ttraining's ndcg@3: 0.864365\ttraining's ndcg@4: 0.87943\ttraining's ndcg@5: 0.89516\tvalid_1's ndcg@1: 0.517294\tvalid_1's ndcg@2: 0.58066\tvalid_1's ndcg@3: 0.64799\tvalid_1's ndcg@4: 0.702592\tvalid_1's ndcg@5: 0.736969\n",
      "[25]\ttraining's ndcg@1: 0.86418\ttraining's ndcg@2: 0.858445\ttraining's ndcg@3: 0.869571\ttraining's ndcg@4: 0.883033\ttraining's ndcg@5: 0.898317\tvalid_1's ndcg@1: 0.530644\tvalid_1's ndcg@2: 0.58848\tvalid_1's ndcg@3: 0.645627\tvalid_1's ndcg@4: 0.705885\tvalid_1's ndcg@5: 0.740969\n",
      "[26]\ttraining's ndcg@1: 0.867769\ttraining's ndcg@2: 0.861406\ttraining's ndcg@3: 0.872772\ttraining's ndcg@4: 0.884431\ttraining's ndcg@5: 0.899957\tvalid_1's ndcg@1: 0.514492\tvalid_1's ndcg@2: 0.586832\tvalid_1's ndcg@3: 0.647842\tvalid_1's ndcg@4: 0.69889\tvalid_1's ndcg@5: 0.739037\n",
      "[27]\ttraining's ndcg@1: 0.871591\ttraining's ndcg@2: 0.864498\ttraining's ndcg@3: 0.874019\ttraining's ndcg@4: 0.886596\ttraining's ndcg@5: 0.901347\tvalid_1's ndcg@1: 0.515463\tvalid_1's ndcg@2: 0.586286\tvalid_1's ndcg@3: 0.649022\tvalid_1's ndcg@4: 0.69963\tvalid_1's ndcg@5: 0.741319\n",
      "[28]\ttraining's ndcg@1: 0.876744\ttraining's ndcg@2: 0.868061\ttraining's ndcg@3: 0.877173\ttraining's ndcg@4: 0.888813\ttraining's ndcg@5: 0.903447\tvalid_1's ndcg@1: 0.522733\tvalid_1's ndcg@2: 0.58053\tvalid_1's ndcg@3: 0.645588\tvalid_1's ndcg@4: 0.705217\tvalid_1's ndcg@5: 0.740605\n",
      "[29]\ttraining's ndcg@1: 0.881704\ttraining's ndcg@2: 0.870137\ttraining's ndcg@3: 0.879736\ttraining's ndcg@4: 0.891177\ttraining's ndcg@5: 0.905854\tvalid_1's ndcg@1: 0.528319\tvalid_1's ndcg@2: 0.584642\tvalid_1's ndcg@3: 0.649613\tvalid_1's ndcg@4: 0.707839\tvalid_1's ndcg@5: 0.73987\n",
      "[30]\ttraining's ndcg@1: 0.884549\ttraining's ndcg@2: 0.871402\ttraining's ndcg@3: 0.880653\ttraining's ndcg@4: 0.891601\ttraining's ndcg@5: 0.905749\tvalid_1's ndcg@1: 0.537769\tvalid_1's ndcg@2: 0.590122\tvalid_1's ndcg@3: 0.656921\tvalid_1's ndcg@4: 0.707658\tvalid_1's ndcg@5: 0.745279\n",
      "[31]\ttraining's ndcg@1: 0.88776\ttraining's ndcg@2: 0.875235\ttraining's ndcg@3: 0.883136\ttraining's ndcg@4: 0.894078\ttraining's ndcg@5: 0.907255\tvalid_1's ndcg@1: 0.528668\tvalid_1's ndcg@2: 0.597423\tvalid_1's ndcg@3: 0.651364\tvalid_1's ndcg@4: 0.708247\tvalid_1's ndcg@5: 0.745437\n",
      "[32]\ttraining's ndcg@1: 0.890247\ttraining's ndcg@2: 0.87749\ttraining's ndcg@3: 0.885568\ttraining's ndcg@4: 0.895638\ttraining's ndcg@5: 0.909627\tvalid_1's ndcg@1: 0.534985\tvalid_1's ndcg@2: 0.597721\tvalid_1's ndcg@3: 0.655364\tvalid_1's ndcg@4: 0.707313\tvalid_1's ndcg@5: 0.745735\n",
      "[33]\ttraining's ndcg@1: 0.892379\ttraining's ndcg@2: 0.879494\ttraining's ndcg@3: 0.887101\ttraining's ndcg@4: 0.896795\ttraining's ndcg@5: 0.910281\tvalid_1's ndcg@1: 0.543475\tvalid_1's ndcg@2: 0.594751\tvalid_1's ndcg@3: 0.655948\tvalid_1's ndcg@4: 0.709268\tvalid_1's ndcg@5: 0.746861\n",
      "[34]\ttraining's ndcg@1: 0.896428\ttraining's ndcg@2: 0.881099\ttraining's ndcg@3: 0.88819\ttraining's ndcg@4: 0.897901\ttraining's ndcg@5: 0.911703\tvalid_1's ndcg@1: 0.535301\tvalid_1's ndcg@2: 0.60208\tvalid_1's ndcg@3: 0.659897\tvalid_1's ndcg@4: 0.714134\tvalid_1's ndcg@5: 0.747101\n",
      "[35]\ttraining's ndcg@1: 0.901761\ttraining's ndcg@2: 0.886808\ttraining's ndcg@3: 0.891406\ttraining's ndcg@4: 0.900345\ttraining's ndcg@5: 0.913953\tvalid_1's ndcg@1: 0.521868\tvalid_1's ndcg@2: 0.600004\tvalid_1's ndcg@3: 0.657743\tvalid_1's ndcg@4: 0.70795\tvalid_1's ndcg@5: 0.744661\n",
      "[36]\ttraining's ndcg@1: 0.901664\ttraining's ndcg@2: 0.888785\ttraining's ndcg@3: 0.892471\ttraining's ndcg@4: 0.901013\ttraining's ndcg@5: 0.915128\tvalid_1's ndcg@1: 0.53309\tvalid_1's ndcg@2: 0.601947\tvalid_1's ndcg@3: 0.659516\tvalid_1's ndcg@4: 0.711852\tvalid_1's ndcg@5: 0.746476\n",
      "[37]\ttraining's ndcg@1: 0.903528\ttraining's ndcg@2: 0.889505\ttraining's ndcg@3: 0.893631\ttraining's ndcg@4: 0.902463\ttraining's ndcg@5: 0.915686\tvalid_1's ndcg@1: 0.525883\tvalid_1's ndcg@2: 0.597257\tvalid_1's ndcg@3: 0.655516\tvalid_1's ndcg@4: 0.710428\tvalid_1's ndcg@5: 0.745256\n",
      "[38]\ttraining's ndcg@1: 0.903539\ttraining's ndcg@2: 0.892083\ttraining's ndcg@3: 0.894287\ttraining's ndcg@4: 0.904063\ttraining's ndcg@5: 0.916202\tvalid_1's ndcg@1: 0.52899\tvalid_1's ndcg@2: 0.593315\tvalid_1's ndcg@3: 0.65727\tvalid_1's ndcg@4: 0.706861\tvalid_1's ndcg@5: 0.746124\n",
      "[39]\ttraining's ndcg@1: 0.903539\ttraining's ndcg@2: 0.893713\ttraining's ndcg@3: 0.896095\ttraining's ndcg@4: 0.904756\ttraining's ndcg@5: 0.917512\tvalid_1's ndcg@1: 0.527924\tvalid_1's ndcg@2: 0.594551\tvalid_1's ndcg@3: 0.655906\tvalid_1's ndcg@4: 0.709291\tvalid_1's ndcg@5: 0.745869\n",
      "[40]\ttraining's ndcg@1: 0.904161\ttraining's ndcg@2: 0.895824\ttraining's ndcg@3: 0.896795\ttraining's ndcg@4: 0.905459\ttraining's ndcg@5: 0.918163\tvalid_1's ndcg@1: 0.523771\tvalid_1's ndcg@2: 0.590695\tvalid_1's ndcg@3: 0.656499\tvalid_1's ndcg@4: 0.698535\tvalid_1's ndcg@5: 0.74463\n",
      "[41]\ttraining's ndcg@1: 0.904961\ttraining's ndcg@2: 0.89704\ttraining's ndcg@3: 0.898734\ttraining's ndcg@4: 0.906561\ttraining's ndcg@5: 0.918668\tvalid_1's ndcg@1: 0.520458\tvalid_1's ndcg@2: 0.577782\tvalid_1's ndcg@3: 0.652411\tvalid_1's ndcg@4: 0.695829\tvalid_1's ndcg@5: 0.741677\n",
      "[42]\ttraining's ndcg@1: 0.906105\ttraining's ndcg@2: 0.899344\ttraining's ndcg@3: 0.899851\ttraining's ndcg@4: 0.9079\ttraining's ndcg@5: 0.920398\tvalid_1's ndcg@1: 0.523197\tvalid_1's ndcg@2: 0.585879\tvalid_1's ndcg@3: 0.653137\tvalid_1's ndcg@4: 0.700575\tvalid_1's ndcg@5: 0.743071\n",
      "[43]\ttraining's ndcg@1: 0.909393\ttraining's ndcg@2: 0.901212\ttraining's ndcg@3: 0.903288\ttraining's ndcg@4: 0.909396\ttraining's ndcg@5: 0.922499\tvalid_1's ndcg@1: 0.526105\tvalid_1's ndcg@2: 0.579944\tvalid_1's ndcg@3: 0.656135\tvalid_1's ndcg@4: 0.706196\tvalid_1's ndcg@5: 0.745475\n",
      "[44]\ttraining's ndcg@1: 0.912428\ttraining's ndcg@2: 0.903046\ttraining's ndcg@3: 0.904614\ttraining's ndcg@4: 0.91121\ttraining's ndcg@5: 0.923595\tvalid_1's ndcg@1: 0.520277\tvalid_1's ndcg@2: 0.581378\tvalid_1's ndcg@3: 0.650838\tvalid_1's ndcg@4: 0.704599\tvalid_1's ndcg@5: 0.742808\n",
      "[45]\ttraining's ndcg@1: 0.913406\ttraining's ndcg@2: 0.904589\ttraining's ndcg@3: 0.906012\ttraining's ndcg@4: 0.912097\ttraining's ndcg@5: 0.923954\tvalid_1's ndcg@1: 0.520896\tvalid_1's ndcg@2: 0.571374\tvalid_1's ndcg@3: 0.651109\tvalid_1's ndcg@4: 0.708907\tvalid_1's ndcg@5: 0.744226\n",
      "[46]\ttraining's ndcg@1: 0.913495\ttraining's ndcg@2: 0.905104\ttraining's ndcg@3: 0.907039\ttraining's ndcg@4: 0.912369\ttraining's ndcg@5: 0.924124\tvalid_1's ndcg@1: 0.527304\tvalid_1's ndcg@2: 0.585111\tvalid_1's ndcg@3: 0.654157\tvalid_1's ndcg@4: 0.711872\tvalid_1's ndcg@5: 0.748473\n",
      "[47]\ttraining's ndcg@1: 0.913506\ttraining's ndcg@2: 0.906594\ttraining's ndcg@3: 0.907597\ttraining's ndcg@4: 0.912442\ttraining's ndcg@5: 0.924702\tvalid_1's ndcg@1: 0.53667\tvalid_1's ndcg@2: 0.590495\tvalid_1's ndcg@3: 0.655546\tvalid_1's ndcg@4: 0.712268\tvalid_1's ndcg@5: 0.750654\n",
      "[48]\ttraining's ndcg@1: 0.913511\ttraining's ndcg@2: 0.907907\ttraining's ndcg@3: 0.908401\ttraining's ndcg@4: 0.91318\ttraining's ndcg@5: 0.9248\tvalid_1's ndcg@1: 0.538675\tvalid_1's ndcg@2: 0.586867\tvalid_1's ndcg@3: 0.652865\tvalid_1's ndcg@4: 0.711921\tvalid_1's ndcg@5: 0.750301\n",
      "[49]\ttraining's ndcg@1: 0.915555\ttraining's ndcg@2: 0.908323\ttraining's ndcg@3: 0.910038\ttraining's ndcg@4: 0.914422\ttraining's ndcg@5: 0.925526\tvalid_1's ndcg@1: 0.542519\tvalid_1's ndcg@2: 0.587512\tvalid_1's ndcg@3: 0.658471\tvalid_1's ndcg@4: 0.714\tvalid_1's ndcg@5: 0.753301\n",
      "[50]\ttraining's ndcg@1: 0.91911\ttraining's ndcg@2: 0.91077\ttraining's ndcg@3: 0.911551\ttraining's ndcg@4: 0.916191\ttraining's ndcg@5: 0.927105\tvalid_1's ndcg@1: 0.533218\tvalid_1's ndcg@2: 0.591533\tvalid_1's ndcg@3: 0.65847\tvalid_1's ndcg@4: 0.714119\tvalid_1's ndcg@5: 0.753358\n",
      "[51]\ttraining's ndcg@1: 0.920532\ttraining's ndcg@2: 0.911553\ttraining's ndcg@3: 0.911559\ttraining's ndcg@4: 0.917035\ttraining's ndcg@5: 0.927853\tvalid_1's ndcg@1: 0.533707\tvalid_1's ndcg@2: 0.586795\tvalid_1's ndcg@3: 0.654202\tvalid_1's ndcg@4: 0.710775\tvalid_1's ndcg@5: 0.749082\n",
      "[52]\ttraining's ndcg@1: 0.922665\ttraining's ndcg@2: 0.913081\ttraining's ndcg@3: 0.91269\ttraining's ndcg@4: 0.918419\ttraining's ndcg@5: 0.928823\tvalid_1's ndcg@1: 0.536897\tvalid_1's ndcg@2: 0.586022\tvalid_1's ndcg@3: 0.654675\tvalid_1's ndcg@4: 0.71277\tvalid_1's ndcg@5: 0.752444\n",
      "[53]\ttraining's ndcg@1: 0.923731\ttraining's ndcg@2: 0.914595\ttraining's ndcg@3: 0.913759\ttraining's ndcg@4: 0.919214\ttraining's ndcg@5: 0.929393\tvalid_1's ndcg@1: 0.534458\tvalid_1's ndcg@2: 0.591017\tvalid_1's ndcg@3: 0.652725\tvalid_1's ndcg@4: 0.711744\tvalid_1's ndcg@5: 0.753178\n",
      "[54]\ttraining's ndcg@1: 0.925764\ttraining's ndcg@2: 0.914953\ttraining's ndcg@3: 0.914288\ttraining's ndcg@4: 0.92007\ttraining's ndcg@5: 0.930211\tvalid_1's ndcg@1: 0.539406\tvalid_1's ndcg@2: 0.59116\tvalid_1's ndcg@3: 0.652914\tvalid_1's ndcg@4: 0.71156\tvalid_1's ndcg@5: 0.753468\n",
      "[55]\ttraining's ndcg@1: 0.925848\ttraining's ndcg@2: 0.915552\ttraining's ndcg@3: 0.915316\ttraining's ndcg@4: 0.920732\ttraining's ndcg@5: 0.93131\tvalid_1's ndcg@1: 0.538081\tvalid_1's ndcg@2: 0.592116\tvalid_1's ndcg@3: 0.653021\tvalid_1's ndcg@4: 0.713618\tvalid_1's ndcg@5: 0.751845\n",
      "[56]\ttraining's ndcg@1: 0.926736\ttraining's ndcg@2: 0.916139\ttraining's ndcg@3: 0.916418\ttraining's ndcg@4: 0.921239\ttraining's ndcg@5: 0.931801\tvalid_1's ndcg@1: 0.537371\tvalid_1's ndcg@2: 0.598043\tvalid_1's ndcg@3: 0.655098\tvalid_1's ndcg@4: 0.714903\tvalid_1's ndcg@5: 0.752398\n",
      "[57]\ttraining's ndcg@1: 0.928869\ttraining's ndcg@2: 0.917335\ttraining's ndcg@3: 0.917903\ttraining's ndcg@4: 0.922156\ttraining's ndcg@5: 0.9325\tvalid_1's ndcg@1: 0.527105\tvalid_1's ndcg@2: 0.592886\tvalid_1's ndcg@3: 0.650857\tvalid_1's ndcg@4: 0.711304\tvalid_1's ndcg@5: 0.748981\n",
      "[58]\ttraining's ndcg@1: 0.92959\ttraining's ndcg@2: 0.918008\ttraining's ndcg@3: 0.918706\ttraining's ndcg@4: 0.923202\ttraining's ndcg@5: 0.933315\tvalid_1's ndcg@1: 0.537217\tvalid_1's ndcg@2: 0.590571\tvalid_1's ndcg@3: 0.65229\tvalid_1's ndcg@4: 0.712016\tvalid_1's ndcg@5: 0.750352\n",
      "[59]\ttraining's ndcg@1: 0.92959\ttraining's ndcg@2: 0.918901\ttraining's ndcg@3: 0.919743\ttraining's ndcg@4: 0.923836\ttraining's ndcg@5: 0.933388\tvalid_1's ndcg@1: 0.539691\tvalid_1's ndcg@2: 0.592599\tvalid_1's ndcg@3: 0.65652\tvalid_1's ndcg@4: 0.711968\tvalid_1's ndcg@5: 0.752683\n",
      "[60]\ttraining's ndcg@1: 0.931014\ttraining's ndcg@2: 0.920537\ttraining's ndcg@3: 0.920787\ttraining's ndcg@4: 0.924918\ttraining's ndcg@5: 0.934347\tvalid_1's ndcg@1: 0.5344\tvalid_1's ndcg@2: 0.588171\tvalid_1's ndcg@3: 0.653842\tvalid_1's ndcg@4: 0.711473\tvalid_1's ndcg@5: 0.751118\n",
      "[61]\ttraining's ndcg@1: 0.933224\ttraining's ndcg@2: 0.92151\ttraining's ndcg@3: 0.92213\ttraining's ndcg@4: 0.926047\ttraining's ndcg@5: 0.935014\tvalid_1's ndcg@1: 0.521088\tvalid_1's ndcg@2: 0.584352\tvalid_1's ndcg@3: 0.649659\tvalid_1's ndcg@4: 0.708205\tvalid_1's ndcg@5: 0.747962\n",
      "[62]\ttraining's ndcg@1: 0.934479\ttraining's ndcg@2: 0.922854\ttraining's ndcg@3: 0.922733\ttraining's ndcg@4: 0.926372\ttraining's ndcg@5: 0.936073\tvalid_1's ndcg@1: 0.522112\tvalid_1's ndcg@2: 0.582343\tvalid_1's ndcg@3: 0.653509\tvalid_1's ndcg@4: 0.708131\tvalid_1's ndcg@5: 0.748863\n",
      "[63]\ttraining's ndcg@1: 0.93519\ttraining's ndcg@2: 0.924183\ttraining's ndcg@3: 0.922679\ttraining's ndcg@4: 0.927403\ttraining's ndcg@5: 0.937013\tvalid_1's ndcg@1: 0.532649\tvalid_1's ndcg@2: 0.586724\tvalid_1's ndcg@3: 0.656295\tvalid_1's ndcg@4: 0.70842\tvalid_1's ndcg@5: 0.750832\n",
      "[64]\ttraining's ndcg@1: 0.93519\ttraining's ndcg@2: 0.924439\ttraining's ndcg@3: 0.922738\ttraining's ndcg@4: 0.927729\ttraining's ndcg@5: 0.937477\tvalid_1's ndcg@1: 0.53612\tvalid_1's ndcg@2: 0.594345\tvalid_1's ndcg@3: 0.66064\tvalid_1's ndcg@4: 0.710349\tvalid_1's ndcg@5: 0.753841\n",
      "[65]\ttraining's ndcg@1: 0.93784\ttraining's ndcg@2: 0.926371\ttraining's ndcg@3: 0.923795\ttraining's ndcg@4: 0.929158\ttraining's ndcg@5: 0.938334\tvalid_1's ndcg@1: 0.541068\tvalid_1's ndcg@2: 0.59811\tvalid_1's ndcg@3: 0.664537\tvalid_1's ndcg@4: 0.712375\tvalid_1's ndcg@5: 0.75612\n",
      "[66]\ttraining's ndcg@1: 0.939345\ttraining's ndcg@2: 0.926964\ttraining's ndcg@3: 0.925068\ttraining's ndcg@4: 0.929693\ttraining's ndcg@5: 0.939339\tvalid_1's ndcg@1: 0.546017\tvalid_1's ndcg@2: 0.600582\tvalid_1's ndcg@3: 0.665536\tvalid_1's ndcg@4: 0.717167\tvalid_1's ndcg@5: 0.756484\n",
      "[67]\ttraining's ndcg@1: 0.939434\ttraining's ndcg@2: 0.92899\ttraining's ndcg@3: 0.925981\ttraining's ndcg@4: 0.930353\ttraining's ndcg@5: 0.939769\tvalid_1's ndcg@1: 0.548579\tvalid_1's ndcg@2: 0.603279\tvalid_1's ndcg@3: 0.6669\tvalid_1's ndcg@4: 0.718187\tvalid_1's ndcg@5: 0.758826\n",
      "[68]\ttraining's ndcg@1: 0.939434\ttraining's ndcg@2: 0.92916\ttraining's ndcg@3: 0.925858\ttraining's ndcg@4: 0.930791\ttraining's ndcg@5: 0.939572\tvalid_1's ndcg@1: 0.552483\tvalid_1's ndcg@2: 0.601663\tvalid_1's ndcg@3: 0.667389\tvalid_1's ndcg@4: 0.717461\tvalid_1's ndcg@5: 0.756249\n",
      "[69]\ttraining's ndcg@1: 0.942189\ttraining's ndcg@2: 0.929933\ttraining's ndcg@3: 0.926642\ttraining's ndcg@4: 0.931723\ttraining's ndcg@5: 0.9409\tvalid_1's ndcg@1: 0.54743\tvalid_1's ndcg@2: 0.600068\tvalid_1's ndcg@3: 0.668426\tvalid_1's ndcg@4: 0.718675\tvalid_1's ndcg@5: 0.756046\n",
      "[70]\ttraining's ndcg@1: 0.942189\ttraining's ndcg@2: 0.930317\ttraining's ndcg@3: 0.926699\ttraining's ndcg@4: 0.932046\ttraining's ndcg@5: 0.941177\tvalid_1's ndcg@1: 0.545303\tvalid_1's ndcg@2: 0.597818\tvalid_1's ndcg@3: 0.666784\tvalid_1's ndcg@4: 0.714962\tvalid_1's ndcg@5: 0.756089\n",
      "[71]\ttraining's ndcg@1: 0.943167\ttraining's ndcg@2: 0.93217\ttraining's ndcg@3: 0.927892\ttraining's ndcg@4: 0.933453\ttraining's ndcg@5: 0.942117\tvalid_1's ndcg@1: 0.55425\tvalid_1's ndcg@2: 0.593968\tvalid_1's ndcg@3: 0.665736\tvalid_1's ndcg@4: 0.718737\tvalid_1's ndcg@5: 0.756327\n",
      "[72]\ttraining's ndcg@1: 0.944678\ttraining's ndcg@2: 0.932424\ttraining's ndcg@3: 0.92888\ttraining's ndcg@4: 0.933976\ttraining's ndcg@5: 0.942674\tvalid_1's ndcg@1: 0.545526\tvalid_1's ndcg@2: 0.597661\tvalid_1's ndcg@3: 0.664651\tvalid_1's ndcg@4: 0.714695\tvalid_1's ndcg@5: 0.755202\n",
      "[73]\ttraining's ndcg@1: 0.945566\ttraining's ndcg@2: 0.933547\ttraining's ndcg@3: 0.930616\ttraining's ndcg@4: 0.934647\ttraining's ndcg@5: 0.943142\tvalid_1's ndcg@1: 0.544824\tvalid_1's ndcg@2: 0.598572\tvalid_1's ndcg@3: 0.666861\tvalid_1's ndcg@4: 0.719578\tvalid_1's ndcg@5: 0.755709\n",
      "[74]\ttraining's ndcg@1: 0.946278\ttraining's ndcg@2: 0.933661\ttraining's ndcg@3: 0.931371\ttraining's ndcg@4: 0.934901\ttraining's ndcg@5: 0.943264\tvalid_1's ndcg@1: 0.541909\tvalid_1's ndcg@2: 0.601337\tvalid_1's ndcg@3: 0.666662\tvalid_1's ndcg@4: 0.716156\tvalid_1's ndcg@5: 0.756174\n",
      "[75]\ttraining's ndcg@1: 0.946989\ttraining's ndcg@2: 0.934458\ttraining's ndcg@3: 0.932315\ttraining's ndcg@4: 0.93599\ttraining's ndcg@5: 0.944038\tvalid_1's ndcg@1: 0.541921\tvalid_1's ndcg@2: 0.598709\tvalid_1's ndcg@3: 0.665538\tvalid_1's ndcg@4: 0.71539\tvalid_1's ndcg@5: 0.753639\n",
      "[76]\ttraining's ndcg@1: 0.947789\ttraining's ndcg@2: 0.93496\ttraining's ndcg@3: 0.932742\ttraining's ndcg@4: 0.936589\ttraining's ndcg@5: 0.945599\tvalid_1's ndcg@1: 0.542319\tvalid_1's ndcg@2: 0.594751\tvalid_1's ndcg@3: 0.668236\tvalid_1's ndcg@4: 0.71642\tvalid_1's ndcg@5: 0.754291\n",
      "[77]\ttraining's ndcg@1: 0.948055\ttraining's ndcg@2: 0.935738\ttraining's ndcg@3: 0.933629\ttraining's ndcg@4: 0.937869\ttraining's ndcg@5: 0.946039\tvalid_1's ndcg@1: 0.541844\tvalid_1's ndcg@2: 0.597296\tvalid_1's ndcg@3: 0.665652\tvalid_1's ndcg@4: 0.717287\tvalid_1's ndcg@5: 0.75514\n",
      "[78]\ttraining's ndcg@1: 0.949122\ttraining's ndcg@2: 0.935952\ttraining's ndcg@3: 0.934838\ttraining's ndcg@4: 0.938467\ttraining's ndcg@5: 0.946659\tvalid_1's ndcg@1: 0.537476\tvalid_1's ndcg@2: 0.592249\tvalid_1's ndcg@3: 0.66183\tvalid_1's ndcg@4: 0.710983\tvalid_1's ndcg@5: 0.753506\n",
      "[79]\ttraining's ndcg@1: 0.95161\ttraining's ndcg@2: 0.93695\ttraining's ndcg@3: 0.935863\ttraining's ndcg@4: 0.939237\ttraining's ndcg@5: 0.947251\tvalid_1's ndcg@1: 0.538062\tvalid_1's ndcg@2: 0.593092\tvalid_1's ndcg@3: 0.664448\tvalid_1's ndcg@4: 0.71371\tvalid_1's ndcg@5: 0.756449\n",
      "[80]\ttraining's ndcg@1: 0.952322\ttraining's ndcg@2: 0.938559\ttraining's ndcg@3: 0.937026\ttraining's ndcg@4: 0.940135\ttraining's ndcg@5: 0.947743\tvalid_1's ndcg@1: 0.522597\tvalid_1's ndcg@2: 0.58943\tvalid_1's ndcg@3: 0.661889\tvalid_1's ndcg@4: 0.709811\tvalid_1's ndcg@5: 0.753407\n",
      "[81]\ttraining's ndcg@1: 0.952322\ttraining's ndcg@2: 0.938729\ttraining's ndcg@3: 0.937363\ttraining's ndcg@4: 0.940465\ttraining's ndcg@5: 0.948095\tvalid_1's ndcg@1: 0.522907\tvalid_1's ndcg@2: 0.591528\tvalid_1's ndcg@3: 0.662902\tvalid_1's ndcg@4: 0.711222\tvalid_1's ndcg@5: 0.754141\n",
      "[82]\ttraining's ndcg@1: 0.953032\ttraining's ndcg@2: 0.939781\ttraining's ndcg@3: 0.937696\ttraining's ndcg@4: 0.940819\ttraining's ndcg@5: 0.948215\tvalid_1's ndcg@1: 0.51787\tvalid_1's ndcg@2: 0.591681\tvalid_1's ndcg@3: 0.659238\tvalid_1's ndcg@4: 0.710529\tvalid_1's ndcg@5: 0.752252\n",
      "[83]\ttraining's ndcg@1: 0.953032\ttraining's ndcg@2: 0.940676\ttraining's ndcg@3: 0.938124\ttraining's ndcg@4: 0.941094\ttraining's ndcg@5: 0.948635\tvalid_1's ndcg@1: 0.523086\tvalid_1's ndcg@2: 0.59217\tvalid_1's ndcg@3: 0.66082\tvalid_1's ndcg@4: 0.71232\tvalid_1's ndcg@5: 0.751865\n",
      "[84]\ttraining's ndcg@1: 0.953032\ttraining's ndcg@2: 0.941502\ttraining's ndcg@3: 0.938583\ttraining's ndcg@4: 0.941297\ttraining's ndcg@5: 0.948904\tvalid_1's ndcg@1: 0.517425\tvalid_1's ndcg@2: 0.592004\tvalid_1's ndcg@3: 0.660221\tvalid_1's ndcg@4: 0.710112\tvalid_1's ndcg@5: 0.750955\n",
      "[85]\ttraining's ndcg@1: 0.953743\ttraining's ndcg@2: 0.942235\ttraining's ndcg@3: 0.939021\ttraining's ndcg@4: 0.941527\ttraining's ndcg@5: 0.949224\tvalid_1's ndcg@1: 0.514023\tvalid_1's ndcg@2: 0.591473\tvalid_1's ndcg@3: 0.66119\tvalid_1's ndcg@4: 0.70966\tvalid_1's ndcg@5: 0.751324\n",
      "[86]\ttraining's ndcg@1: 0.953743\ttraining's ndcg@2: 0.942661\ttraining's ndcg@3: 0.939233\ttraining's ndcg@4: 0.941983\ttraining's ndcg@5: 0.949833\tvalid_1's ndcg@1: 0.505532\tvalid_1's ndcg@2: 0.589217\tvalid_1's ndcg@3: 0.656221\tvalid_1's ndcg@4: 0.707105\tvalid_1's ndcg@5: 0.750586\n",
      "[87]\ttraining's ndcg@1: 0.953743\ttraining's ndcg@2: 0.943394\ttraining's ndcg@3: 0.939299\ttraining's ndcg@4: 0.942649\ttraining's ndcg@5: 0.949837\tvalid_1's ndcg@1: 0.515209\tvalid_1's ndcg@2: 0.591742\tvalid_1's ndcg@3: 0.659556\tvalid_1's ndcg@4: 0.71059\tvalid_1's ndcg@5: 0.752662\n",
      "[88]\ttraining's ndcg@1: 0.954454\ttraining's ndcg@2: 0.944431\ttraining's ndcg@3: 0.93974\ttraining's ndcg@4: 0.943146\ttraining's ndcg@5: 0.950319\tvalid_1's ndcg@1: 0.509819\tvalid_1's ndcg@2: 0.591412\tvalid_1's ndcg@3: 0.656937\tvalid_1's ndcg@4: 0.710213\tvalid_1's ndcg@5: 0.750929\n",
      "[89]\ttraining's ndcg@1: 0.954454\ttraining's ndcg@2: 0.944601\ttraining's ndcg@3: 0.940004\ttraining's ndcg@4: 0.943462\ttraining's ndcg@5: 0.950665\tvalid_1's ndcg@1: 0.517459\tvalid_1's ndcg@2: 0.595591\tvalid_1's ndcg@3: 0.657808\tvalid_1's ndcg@4: 0.712184\tvalid_1's ndcg@5: 0.752705\n",
      "[90]\ttraining's ndcg@1: 0.955521\ttraining's ndcg@2: 0.945625\ttraining's ndcg@3: 0.940654\ttraining's ndcg@4: 0.943997\ttraining's ndcg@5: 0.951176\tvalid_1's ndcg@1: 0.522366\tvalid_1's ndcg@2: 0.593751\tvalid_1's ndcg@3: 0.661375\tvalid_1's ndcg@4: 0.71269\tvalid_1's ndcg@5: 0.752977\n",
      "[91]\ttraining's ndcg@1: 0.956587\ttraining's ndcg@2: 0.945967\ttraining's ndcg@3: 0.941435\ttraining's ndcg@4: 0.944987\ttraining's ndcg@5: 0.951576\tvalid_1's ndcg@1: 0.522984\tvalid_1's ndcg@2: 0.597317\tvalid_1's ndcg@3: 0.662698\tvalid_1's ndcg@4: 0.707274\tvalid_1's ndcg@5: 0.753733\n",
      "[92]\ttraining's ndcg@1: 0.9573\ttraining's ndcg@2: 0.946508\ttraining's ndcg@3: 0.942473\ttraining's ndcg@4: 0.945236\ttraining's ndcg@5: 0.952233\tvalid_1's ndcg@1: 0.520502\tvalid_1's ndcg@2: 0.594665\tvalid_1's ndcg@3: 0.660095\tvalid_1's ndcg@4: 0.704698\tvalid_1's ndcg@5: 0.751021\n",
      "[93]\ttraining's ndcg@1: 0.9573\ttraining's ndcg@2: 0.946849\ttraining's ndcg@3: 0.942607\ttraining's ndcg@4: 0.946244\ttraining's ndcg@5: 0.953295\tvalid_1's ndcg@1: 0.521027\tvalid_1's ndcg@2: 0.594133\tvalid_1's ndcg@3: 0.660839\tvalid_1's ndcg@4: 0.707116\tvalid_1's ndcg@5: 0.751182\n",
      "[94]\ttraining's ndcg@1: 0.9573\ttraining's ndcg@2: 0.94702\ttraining's ndcg@3: 0.94306\ttraining's ndcg@4: 0.947094\ttraining's ndcg@5: 0.953561\tvalid_1's ndcg@1: 0.51714\tvalid_1's ndcg@2: 0.590602\tvalid_1's ndcg@3: 0.662222\tvalid_1's ndcg@4: 0.706371\tvalid_1's ndcg@5: 0.749604\n",
      "[95]\ttraining's ndcg@1: 0.958722\ttraining's ndcg@2: 0.947762\ttraining's ndcg@3: 0.944088\ttraining's ndcg@4: 0.947579\ttraining's ndcg@5: 0.954168\tvalid_1's ndcg@1: 0.526697\tvalid_1's ndcg@2: 0.598372\tvalid_1's ndcg@3: 0.664362\tvalid_1's ndcg@4: 0.709668\tvalid_1's ndcg@5: 0.753242\n",
      "[96]\ttraining's ndcg@1: 0.959434\ttraining's ndcg@2: 0.948301\ttraining's ndcg@3: 0.945071\ttraining's ndcg@4: 0.94797\ttraining's ndcg@5: 0.955009\tvalid_1's ndcg@1: 0.520435\tvalid_1's ndcg@2: 0.595238\tvalid_1's ndcg@3: 0.662514\tvalid_1's ndcg@4: 0.708077\tvalid_1's ndcg@5: 0.751473\n",
      "[97]\ttraining's ndcg@1: 0.959434\ttraining's ndcg@2: 0.948599\ttraining's ndcg@3: 0.945553\ttraining's ndcg@4: 0.948964\ttraining's ndcg@5: 0.955166\tvalid_1's ndcg@1: 0.523083\tvalid_1's ndcg@2: 0.594969\tvalid_1's ndcg@3: 0.662614\tvalid_1's ndcg@4: 0.710116\tvalid_1's ndcg@5: 0.754388\n",
      "[98]\ttraining's ndcg@1: 0.960144\ttraining's ndcg@2: 0.949641\ttraining's ndcg@3: 0.946913\ttraining's ndcg@4: 0.949747\ttraining's ndcg@5: 0.955737\tvalid_1's ndcg@1: 0.519725\tvalid_1's ndcg@2: 0.59375\tvalid_1's ndcg@3: 0.662803\tvalid_1's ndcg@4: 0.709711\tvalid_1's ndcg@5: 0.753977\n",
      "[99]\ttraining's ndcg@1: 0.960144\ttraining's ndcg@2: 0.950025\ttraining's ndcg@3: 0.94734\ttraining's ndcg@4: 0.950115\ttraining's ndcg@5: 0.956109\tvalid_1's ndcg@1: 0.525919\tvalid_1's ndcg@2: 0.596332\tvalid_1's ndcg@3: 0.666393\tvalid_1's ndcg@4: 0.712535\tvalid_1's ndcg@5: 0.75526\n",
      "[100]\ttraining's ndcg@1: 0.960855\ttraining's ndcg@2: 0.950566\ttraining's ndcg@3: 0.947826\ttraining's ndcg@4: 0.950547\ttraining's ndcg@5: 0.956353\tvalid_1's ndcg@1: 0.528039\tvalid_1's ndcg@2: 0.597759\tvalid_1's ndcg@3: 0.664455\tvalid_1's ndcg@4: 0.71182\tvalid_1's ndcg@5: 0.755172\n",
      "[101]\ttraining's ndcg@1: 0.960855\ttraining's ndcg@2: 0.950822\ttraining's ndcg@3: 0.948206\ttraining's ndcg@4: 0.950564\ttraining's ndcg@5: 0.956564\tvalid_1's ndcg@1: 0.523221\tvalid_1's ndcg@2: 0.59505\tvalid_1's ndcg@3: 0.666524\tvalid_1's ndcg@4: 0.710322\tvalid_1's ndcg@5: 0.753425\n",
      "[102]\ttraining's ndcg@1: 0.961566\ttraining's ndcg@2: 0.951192\ttraining's ndcg@3: 0.948993\ttraining's ndcg@4: 0.95102\ttraining's ndcg@5: 0.957058\tvalid_1's ndcg@1: 0.518052\tvalid_1's ndcg@2: 0.597354\tvalid_1's ndcg@3: 0.668004\tvalid_1's ndcg@4: 0.712381\tvalid_1's ndcg@5: 0.755061\n",
      "[103]\ttraining's ndcg@1: 0.963366\ttraining's ndcg@2: 0.951869\ttraining's ndcg@3: 0.949535\ttraining's ndcg@4: 0.951789\ttraining's ndcg@5: 0.95766\tvalid_1's ndcg@1: 0.523949\tvalid_1's ndcg@2: 0.598841\tvalid_1's ndcg@3: 0.66961\tvalid_1's ndcg@4: 0.712859\tvalid_1's ndcg@5: 0.756138\n",
      "[104]\ttraining's ndcg@1: 0.963366\ttraining's ndcg@2: 0.952295\ttraining's ndcg@3: 0.949925\ttraining's ndcg@4: 0.952166\ttraining's ndcg@5: 0.957979\tvalid_1's ndcg@1: 0.512512\tvalid_1's ndcg@2: 0.595435\tvalid_1's ndcg@3: 0.66418\tvalid_1's ndcg@4: 0.708693\tvalid_1's ndcg@5: 0.752147\n",
      "[105]\ttraining's ndcg@1: 0.964077\ttraining's ndcg@2: 0.952921\ttraining's ndcg@3: 0.95028\ttraining's ndcg@4: 0.952763\ttraining's ndcg@5: 0.95839\tvalid_1's ndcg@1: 0.515299\tvalid_1's ndcg@2: 0.596917\tvalid_1's ndcg@3: 0.66297\tvalid_1's ndcg@4: 0.708674\tvalid_1's ndcg@5: 0.752313\n",
      "[106]\ttraining's ndcg@1: 0.964077\ttraining's ndcg@2: 0.953262\ttraining's ndcg@3: 0.951565\ttraining's ndcg@4: 0.953939\ttraining's ndcg@5: 0.959461\tvalid_1's ndcg@1: 0.512777\tvalid_1's ndcg@2: 0.594005\tvalid_1's ndcg@3: 0.662095\tvalid_1's ndcg@4: 0.707443\tvalid_1's ndcg@5: 0.749873\n",
      "[107]\ttraining's ndcg@1: 0.964798\ttraining's ndcg@2: 0.953636\ttraining's ndcg@3: 0.951877\ttraining's ndcg@4: 0.954325\ttraining's ndcg@5: 0.960115\tvalid_1's ndcg@1: 0.518742\tvalid_1's ndcg@2: 0.597716\tvalid_1's ndcg@3: 0.666652\tvalid_1's ndcg@4: 0.711632\tvalid_1's ndcg@5: 0.750472\n",
      "[108]\ttraining's ndcg@1: 0.96622\ttraining's ndcg@2: 0.954972\ttraining's ndcg@3: 0.952929\ttraining's ndcg@4: 0.955085\ttraining's ndcg@5: 0.96069\tvalid_1's ndcg@1: 0.522381\tvalid_1's ndcg@2: 0.599713\tvalid_1's ndcg@3: 0.66587\tvalid_1's ndcg@4: 0.711617\tvalid_1's ndcg@5: 0.753366\n",
      "[109]\ttraining's ndcg@1: 0.96622\ttraining's ndcg@2: 0.95582\ttraining's ndcg@3: 0.953667\ttraining's ndcg@4: 0.955877\ttraining's ndcg@5: 0.960944\tvalid_1's ndcg@1: 0.521726\tvalid_1's ndcg@2: 0.598556\tvalid_1's ndcg@3: 0.662991\tvalid_1's ndcg@4: 0.712368\tvalid_1's ndcg@5: 0.750106\n",
      "[110]\ttraining's ndcg@1: 0.966932\ttraining's ndcg@2: 0.956191\ttraining's ndcg@3: 0.954098\ttraining's ndcg@4: 0.956773\ttraining's ndcg@5: 0.961851\tvalid_1's ndcg@1: 0.527132\tvalid_1's ndcg@2: 0.601388\tvalid_1's ndcg@3: 0.664485\tvalid_1's ndcg@4: 0.712858\tvalid_1's ndcg@5: 0.752131\n",
      "[111]\ttraining's ndcg@1: 0.967643\ttraining's ndcg@2: 0.956475\ttraining's ndcg@3: 0.954057\ttraining's ndcg@4: 0.957231\ttraining's ndcg@5: 0.96204\tvalid_1's ndcg@1: 0.511264\tvalid_1's ndcg@2: 0.594757\tvalid_1's ndcg@3: 0.66031\tvalid_1's ndcg@4: 0.706926\tvalid_1's ndcg@5: 0.747313\n",
      "[112]\ttraining's ndcg@1: 0.967643\ttraining's ndcg@2: 0.957756\ttraining's ndcg@3: 0.955064\ttraining's ndcg@4: 0.957498\ttraining's ndcg@5: 0.96252\tvalid_1's ndcg@1: 0.521381\tvalid_1's ndcg@2: 0.597469\tvalid_1's ndcg@3: 0.661219\tvalid_1's ndcg@4: 0.709308\tvalid_1's ndcg@5: 0.748843\n",
      "[113]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.957955\ttraining's ndcg@3: 0.955369\ttraining's ndcg@4: 0.957854\ttraining's ndcg@5: 0.962785\tvalid_1's ndcg@1: 0.523151\tvalid_1's ndcg@2: 0.600779\tvalid_1's ndcg@3: 0.66404\tvalid_1's ndcg@4: 0.710491\tvalid_1's ndcg@5: 0.75109\n",
      "[114]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.959149\ttraining's ndcg@3: 0.9568\ttraining's ndcg@4: 0.958412\ttraining's ndcg@5: 0.963215\tvalid_1's ndcg@1: 0.525183\tvalid_1's ndcg@2: 0.601476\tvalid_1's ndcg@3: 0.664654\tvalid_1's ndcg@4: 0.711664\tvalid_1's ndcg@5: 0.753484\n",
      "[115]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.959405\ttraining's ndcg@3: 0.957512\ttraining's ndcg@4: 0.958644\ttraining's ndcg@5: 0.963451\tvalid_1's ndcg@1: 0.527648\tvalid_1's ndcg@2: 0.598949\tvalid_1's ndcg@3: 0.665958\tvalid_1's ndcg@4: 0.710777\tvalid_1's ndcg@5: 0.754472\n",
      "[116]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.95949\ttraining's ndcg@3: 0.957925\ttraining's ndcg@4: 0.958975\ttraining's ndcg@5: 0.963636\tvalid_1's ndcg@1: 0.518718\tvalid_1's ndcg@2: 0.594895\tvalid_1's ndcg@3: 0.661226\tvalid_1's ndcg@4: 0.705751\tvalid_1's ndcg@5: 0.751681\n",
      "[117]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.960258\ttraining's ndcg@3: 0.95806\ttraining's ndcg@4: 0.959011\ttraining's ndcg@5: 0.963932\tvalid_1's ndcg@1: 0.511349\tvalid_1's ndcg@2: 0.592571\tvalid_1's ndcg@3: 0.657502\tvalid_1's ndcg@4: 0.703027\tvalid_1's ndcg@5: 0.750823\n",
      "[118]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.960258\ttraining's ndcg@3: 0.958363\ttraining's ndcg@4: 0.959073\ttraining's ndcg@5: 0.96419\tvalid_1's ndcg@1: 0.519479\tvalid_1's ndcg@2: 0.592619\tvalid_1's ndcg@3: 0.659652\tvalid_1's ndcg@4: 0.705902\tvalid_1's ndcg@5: 0.751584\n",
      "[119]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.960557\ttraining's ndcg@3: 0.958482\ttraining's ndcg@4: 0.959226\ttraining's ndcg@5: 0.964187\tvalid_1's ndcg@1: 0.517212\tvalid_1's ndcg@2: 0.592915\tvalid_1's ndcg@3: 0.659817\tvalid_1's ndcg@4: 0.707175\tvalid_1's ndcg@5: 0.751772\n",
      "[120]\ttraining's ndcg@1: 0.968354\ttraining's ndcg@2: 0.960557\ttraining's ndcg@3: 0.959091\ttraining's ndcg@4: 0.959229\ttraining's ndcg@5: 0.964235\tvalid_1's ndcg@1: 0.529537\tvalid_1's ndcg@2: 0.596186\tvalid_1's ndcg@3: 0.662597\tvalid_1's ndcg@4: 0.712157\tvalid_1's ndcg@5: 0.753493\n",
      "[121]\ttraining's ndcg@1: 0.969064\ttraining's ndcg@2: 0.96178\ttraining's ndcg@3: 0.959484\ttraining's ndcg@4: 0.959774\ttraining's ndcg@5: 0.96476\tvalid_1's ndcg@1: 0.536208\tvalid_1's ndcg@2: 0.59757\tvalid_1's ndcg@3: 0.665559\tvalid_1's ndcg@4: 0.713725\tvalid_1's ndcg@5: 0.755434\n",
      "[122]\ttraining's ndcg@1: 0.969064\ttraining's ndcg@2: 0.96178\ttraining's ndcg@3: 0.959515\ttraining's ndcg@4: 0.95992\ttraining's ndcg@5: 0.964885\tvalid_1's ndcg@1: 0.538771\tvalid_1's ndcg@2: 0.600458\tvalid_1's ndcg@3: 0.668146\tvalid_1's ndcg@4: 0.714553\tvalid_1's ndcg@5: 0.755135\n",
      "[123]\ttraining's ndcg@1: 0.969064\ttraining's ndcg@2: 0.961983\ttraining's ndcg@3: 0.960179\ttraining's ndcg@4: 0.959982\ttraining's ndcg@5: 0.965041\tvalid_1's ndcg@1: 0.524444\tvalid_1's ndcg@2: 0.596055\tvalid_1's ndcg@3: 0.664645\tvalid_1's ndcg@4: 0.710168\tvalid_1's ndcg@5: 0.750921\n",
      "[124]\ttraining's ndcg@1: 0.969064\ttraining's ndcg@2: 0.962239\ttraining's ndcg@3: 0.9603\ttraining's ndcg@4: 0.960157\ttraining's ndcg@5: 0.965473\tvalid_1's ndcg@1: 0.529922\tvalid_1's ndcg@2: 0.601238\tvalid_1's ndcg@3: 0.668674\tvalid_1's ndcg@4: 0.710178\tvalid_1's ndcg@5: 0.752937\n",
      "[125]\ttraining's ndcg@1: 0.969775\ttraining's ndcg@2: 0.962865\ttraining's ndcg@3: 0.960899\ttraining's ndcg@4: 0.960539\ttraining's ndcg@5: 0.965753\tvalid_1's ndcg@1: 0.529917\tvalid_1's ndcg@2: 0.597836\tvalid_1's ndcg@3: 0.664164\tvalid_1's ndcg@4: 0.709701\tvalid_1's ndcg@5: 0.754298\n",
      "[126]\ttraining's ndcg@1: 0.969775\ttraining's ndcg@2: 0.963376\ttraining's ndcg@3: 0.961153\ttraining's ndcg@4: 0.960817\ttraining's ndcg@5: 0.96615\tvalid_1's ndcg@1: 0.531958\tvalid_1's ndcg@2: 0.59977\tvalid_1's ndcg@3: 0.664998\tvalid_1's ndcg@4: 0.711272\tvalid_1's ndcg@5: 0.754533\n",
      "[127]\ttraining's ndcg@1: 0.972622\ttraining's ndcg@2: 0.964602\ttraining's ndcg@3: 0.962142\ttraining's ndcg@4: 0.96145\ttraining's ndcg@5: 0.966731\tvalid_1's ndcg@1: 0.522957\tvalid_1's ndcg@2: 0.59502\tvalid_1's ndcg@3: 0.66286\tvalid_1's ndcg@4: 0.709361\tvalid_1's ndcg@5: 0.751795\n",
      "[128]\ttraining's ndcg@1: 0.974756\ttraining's ndcg@2: 0.965734\ttraining's ndcg@3: 0.962976\ttraining's ndcg@4: 0.963119\ttraining's ndcg@5: 0.967819\tvalid_1's ndcg@1: 0.524989\tvalid_1's ndcg@2: 0.593207\tvalid_1's ndcg@3: 0.664854\tvalid_1's ndcg@4: 0.710503\tvalid_1's ndcg@5: 0.752606\n",
      "[129]\ttraining's ndcg@1: 0.975467\ttraining's ndcg@2: 0.966402\ttraining's ndcg@3: 0.963678\ttraining's ndcg@4: 0.96347\ttraining's ndcg@5: 0.968236\tvalid_1's ndcg@1: 0.527213\tvalid_1's ndcg@2: 0.591649\tvalid_1's ndcg@3: 0.665542\tvalid_1's ndcg@4: 0.710525\tvalid_1's ndcg@5: 0.752086\n",
      "[130]\ttraining's ndcg@1: 0.976179\ttraining's ndcg@2: 0.967157\ttraining's ndcg@3: 0.963994\ttraining's ndcg@4: 0.964526\ttraining's ndcg@5: 0.96846\tvalid_1's ndcg@1: 0.53091\tvalid_1's ndcg@2: 0.593359\tvalid_1's ndcg@3: 0.665945\tvalid_1's ndcg@4: 0.710399\tvalid_1's ndcg@5: 0.75411\n",
      "[131]\ttraining's ndcg@1: 0.976889\ttraining's ndcg@2: 0.968038\ttraining's ndcg@3: 0.9642\ttraining's ndcg@4: 0.964951\ttraining's ndcg@5: 0.969079\tvalid_1's ndcg@1: 0.53719\tvalid_1's ndcg@2: 0.597504\tvalid_1's ndcg@3: 0.670548\tvalid_1's ndcg@4: 0.70878\tvalid_1's ndcg@5: 0.754995\n",
      "[132]\ttraining's ndcg@1: 0.976889\ttraining's ndcg@2: 0.968167\ttraining's ndcg@3: 0.964178\ttraining's ndcg@4: 0.964897\ttraining's ndcg@5: 0.969214\tvalid_1's ndcg@1: 0.540026\tvalid_1's ndcg@2: 0.597178\tvalid_1's ndcg@3: 0.668455\tvalid_1's ndcg@4: 0.711155\tvalid_1's ndcg@5: 0.757426\n",
      "[133]\ttraining's ndcg@1: 0.976889\ttraining's ndcg@2: 0.968337\ttraining's ndcg@3: 0.964435\ttraining's ndcg@4: 0.965039\ttraining's ndcg@5: 0.969292\tvalid_1's ndcg@1: 0.528985\tvalid_1's ndcg@2: 0.600335\tvalid_1's ndcg@3: 0.665225\tvalid_1's ndcg@4: 0.708476\tvalid_1's ndcg@5: 0.755816\n",
      "[134]\ttraining's ndcg@1: 0.9776\ttraining's ndcg@2: 0.968792\ttraining's ndcg@3: 0.965545\ttraining's ndcg@4: 0.965401\ttraining's ndcg@5: 0.969736\tvalid_1's ndcg@1: 0.538209\tvalid_1's ndcg@2: 0.600637\tvalid_1's ndcg@3: 0.669054\tvalid_1's ndcg@4: 0.711043\tvalid_1's ndcg@5: 0.756188\n",
      "[135]\ttraining's ndcg@1: 0.9776\ttraining's ndcg@2: 0.969858\ttraining's ndcg@3: 0.965867\ttraining's ndcg@4: 0.965839\ttraining's ndcg@5: 0.969969\tvalid_1's ndcg@1: 0.534674\tvalid_1's ndcg@2: 0.59992\tvalid_1's ndcg@3: 0.668886\tvalid_1's ndcg@4: 0.711687\tvalid_1's ndcg@5: 0.755068\n",
      "[136]\ttraining's ndcg@1: 0.978312\ttraining's ndcg@2: 0.970058\ttraining's ndcg@3: 0.966204\ttraining's ndcg@4: 0.966055\ttraining's ndcg@5: 0.970253\tvalid_1's ndcg@1: 0.530079\tvalid_1's ndcg@2: 0.601156\tvalid_1's ndcg@3: 0.668433\tvalid_1's ndcg@4: 0.712264\tvalid_1's ndcg@5: 0.753552\n",
      "[137]\ttraining's ndcg@1: 0.978312\ttraining's ndcg@2: 0.970527\ttraining's ndcg@3: 0.966509\ttraining's ndcg@4: 0.966401\ttraining's ndcg@5: 0.970328\tvalid_1's ndcg@1: 0.521254\tvalid_1's ndcg@2: 0.598588\tvalid_1's ndcg@3: 0.668074\tvalid_1's ndcg@4: 0.708848\tvalid_1's ndcg@5: 0.751363\n",
      "[138]\ttraining's ndcg@1: 0.978312\ttraining's ndcg@2: 0.970847\ttraining's ndcg@3: 0.96657\ttraining's ndcg@4: 0.966421\ttraining's ndcg@5: 0.970369\tvalid_1's ndcg@1: 0.52258\tvalid_1's ndcg@2: 0.598281\tvalid_1's ndcg@3: 0.664355\tvalid_1's ndcg@4: 0.71153\tvalid_1's ndcg@5: 0.751877\n",
      "[139]\ttraining's ndcg@1: 0.978312\ttraining's ndcg@2: 0.971017\ttraining's ndcg@3: 0.966633\ttraining's ndcg@4: 0.9666\ttraining's ndcg@5: 0.970192\tvalid_1's ndcg@1: 0.526334\tvalid_1's ndcg@2: 0.597094\tvalid_1's ndcg@3: 0.666025\tvalid_1's ndcg@4: 0.711905\tvalid_1's ndcg@5: 0.752862\n",
      "[140]\ttraining's ndcg@1: 0.979023\ttraining's ndcg@2: 0.971388\ttraining's ndcg@3: 0.966786\ttraining's ndcg@4: 0.966805\ttraining's ndcg@5: 0.970686\tvalid_1's ndcg@1: 0.527748\tvalid_1's ndcg@2: 0.596389\tvalid_1's ndcg@3: 0.667357\tvalid_1's ndcg@4: 0.712001\tvalid_1's ndcg@5: 0.75394\n",
      "[141]\ttraining's ndcg@1: 0.979023\ttraining's ndcg@2: 0.971388\ttraining's ndcg@3: 0.966786\ttraining's ndcg@4: 0.966899\ttraining's ndcg@5: 0.970834\tvalid_1's ndcg@1: 0.518469\tvalid_1's ndcg@2: 0.595684\tvalid_1's ndcg@3: 0.666519\tvalid_1's ndcg@4: 0.710283\tvalid_1's ndcg@5: 0.752296\n",
      "[142]\ttraining's ndcg@1: 0.979734\ttraining's ndcg@2: 0.971758\ttraining's ndcg@3: 0.967001\ttraining's ndcg@4: 0.966971\ttraining's ndcg@5: 0.971093\tvalid_1's ndcg@1: 0.521642\tvalid_1's ndcg@2: 0.597252\tvalid_1's ndcg@3: 0.666609\tvalid_1's ndcg@4: 0.710809\tvalid_1's ndcg@5: 0.753794\n",
      "[143]\ttraining's ndcg@1: 0.980445\ttraining's ndcg@2: 0.971958\ttraining's ndcg@3: 0.967395\ttraining's ndcg@4: 0.967321\ttraining's ndcg@5: 0.971363\tvalid_1's ndcg@1: 0.534232\tvalid_1's ndcg@2: 0.59994\tvalid_1's ndcg@3: 0.669508\tvalid_1's ndcg@4: 0.713634\tvalid_1's ndcg@5: 0.754024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144]\ttraining's ndcg@1: 0.981156\ttraining's ndcg@2: 0.972797\ttraining's ndcg@3: 0.967729\ttraining's ndcg@4: 0.96773\ttraining's ndcg@5: 0.97174\tvalid_1's ndcg@1: 0.519938\tvalid_1's ndcg@2: 0.598138\tvalid_1's ndcg@3: 0.665721\tvalid_1's ndcg@4: 0.709918\tvalid_1's ndcg@5: 0.751226\n",
      "[145]\ttraining's ndcg@1: 0.981156\ttraining's ndcg@2: 0.972968\ttraining's ndcg@3: 0.968689\ttraining's ndcg@4: 0.968299\ttraining's ndcg@5: 0.971963\tvalid_1's ndcg@1: 0.516459\tvalid_1's ndcg@2: 0.594356\tvalid_1's ndcg@3: 0.667618\tvalid_1's ndcg@4: 0.70848\tvalid_1's ndcg@5: 0.751643\n",
      "[146]\ttraining's ndcg@1: 0.981867\ttraining's ndcg@2: 0.973125\ttraining's ndcg@3: 0.969033\ttraining's ndcg@4: 0.968477\ttraining's ndcg@5: 0.972327\tvalid_1's ndcg@1: 0.525329\tvalid_1's ndcg@2: 0.598305\tvalid_1's ndcg@3: 0.666525\tvalid_1's ndcg@4: 0.710456\tvalid_1's ndcg@5: 0.752462\n",
      "[147]\ttraining's ndcg@1: 0.981867\ttraining's ndcg@2: 0.973892\ttraining's ndcg@3: 0.96958\ttraining's ndcg@4: 0.968613\ttraining's ndcg@5: 0.972523\tvalid_1's ndcg@1: 0.517754\tvalid_1's ndcg@2: 0.596685\tvalid_1's ndcg@3: 0.663525\tvalid_1's ndcg@4: 0.71002\tvalid_1's ndcg@5: 0.752112\n",
      "[148]\ttraining's ndcg@1: 0.981867\ttraining's ndcg@2: 0.974233\ttraining's ndcg@3: 0.970185\ttraining's ndcg@4: 0.968812\ttraining's ndcg@5: 0.97271\tvalid_1's ndcg@1: 0.517688\tvalid_1's ndcg@2: 0.597583\tvalid_1's ndcg@3: 0.662643\tvalid_1's ndcg@4: 0.710771\tvalid_1's ndcg@5: 0.753144\n",
      "[149]\ttraining's ndcg@1: 0.981867\ttraining's ndcg@2: 0.974233\ttraining's ndcg@3: 0.970358\ttraining's ndcg@4: 0.969039\ttraining's ndcg@5: 0.973187\tvalid_1's ndcg@1: 0.51184\tvalid_1's ndcg@2: 0.594585\tvalid_1's ndcg@3: 0.660885\tvalid_1's ndcg@4: 0.709075\tvalid_1's ndcg@5: 0.750009\n",
      "[150]\ttraining's ndcg@1: 0.982933\ttraining's ndcg@2: 0.975044\ttraining's ndcg@3: 0.970971\ttraining's ndcg@4: 0.969518\ttraining's ndcg@5: 0.973662\tvalid_1's ndcg@1: 0.519848\tvalid_1's ndcg@2: 0.597475\tvalid_1's ndcg@3: 0.66398\tvalid_1's ndcg@4: 0.711418\tvalid_1's ndcg@5: 0.752994\n",
      "[151]\ttraining's ndcg@1: 0.982933\ttraining's ndcg@2: 0.975215\ttraining's ndcg@3: 0.971713\ttraining's ndcg@4: 0.969798\ttraining's ndcg@5: 0.973764\tvalid_1's ndcg@1: 0.511133\tvalid_1's ndcg@2: 0.594853\tvalid_1's ndcg@3: 0.660688\tvalid_1's ndcg@4: 0.709374\tvalid_1's ndcg@5: 0.752971\n",
      "[152]\ttraining's ndcg@1: 0.983644\ttraining's ndcg@2: 0.97644\ttraining's ndcg@3: 0.97209\ttraining's ndcg@4: 0.970191\ttraining's ndcg@5: 0.974236\tvalid_1's ndcg@1: 0.519848\tvalid_1's ndcg@2: 0.598153\tvalid_1's ndcg@3: 0.664722\tvalid_1's ndcg@4: 0.712209\tvalid_1's ndcg@5: 0.755805\n",
      "[153]\ttraining's ndcg@1: 0.983644\ttraining's ndcg@2: 0.97661\ttraining's ndcg@3: 0.972153\ttraining's ndcg@4: 0.97038\ttraining's ndcg@5: 0.974612\tvalid_1's ndcg@1: 0.51702\tvalid_1's ndcg@2: 0.600496\tvalid_1's ndcg@3: 0.663491\tvalid_1's ndcg@4: 0.712198\tvalid_1's ndcg@5: 0.755887\n",
      "[154]\ttraining's ndcg@1: 0.983644\ttraining's ndcg@2: 0.977037\ttraining's ndcg@3: 0.972697\ttraining's ndcg@4: 0.970705\ttraining's ndcg@5: 0.974825\tvalid_1's ndcg@1: 0.518428\tvalid_1's ndcg@2: 0.596833\tvalid_1's ndcg@3: 0.663357\tvalid_1's ndcg@4: 0.711707\tvalid_1's ndcg@5: 0.753957\n",
      "[155]\ttraining's ndcg@1: 0.983644\ttraining's ndcg@2: 0.977378\ttraining's ndcg@3: 0.972738\ttraining's ndcg@4: 0.971051\ttraining's ndcg@5: 0.974861\tvalid_1's ndcg@1: 0.52038\tvalid_1's ndcg@2: 0.598992\tvalid_1's ndcg@3: 0.666066\tvalid_1's ndcg@4: 0.712497\tvalid_1's ndcg@5: 0.755513\n",
      "[156]\ttraining's ndcg@1: 0.983644\ttraining's ndcg@2: 0.977378\ttraining's ndcg@3: 0.972785\ttraining's ndcg@4: 0.971215\ttraining's ndcg@5: 0.974778\tvalid_1's ndcg@1: 0.520444\tvalid_1's ndcg@2: 0.598925\tvalid_1's ndcg@3: 0.666001\tvalid_1's ndcg@4: 0.711571\tvalid_1's ndcg@5: 0.755074\n",
      "[157]\ttraining's ndcg@1: 0.984355\ttraining's ndcg@2: 0.977748\ttraining's ndcg@3: 0.97303\ttraining's ndcg@4: 0.971439\ttraining's ndcg@5: 0.975232\tvalid_1's ndcg@1: 0.520444\tvalid_1's ndcg@2: 0.598925\tvalid_1's ndcg@3: 0.666811\tvalid_1's ndcg@4: 0.709671\tvalid_1's ndcg@5: 0.754788\n",
      "[158]\ttraining's ndcg@1: 0.985778\ttraining's ndcg@2: 0.978701\ttraining's ndcg@3: 0.97374\ttraining's ndcg@4: 0.972143\ttraining's ndcg@5: 0.975839\tvalid_1's ndcg@1: 0.528352\tvalid_1's ndcg@2: 0.602962\tvalid_1's ndcg@3: 0.666726\tvalid_1's ndcg@4: 0.712753\tvalid_1's ndcg@5: 0.757882\n",
      "[159]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.978858\ttraining's ndcg@3: 0.974161\ttraining's ndcg@4: 0.972427\ttraining's ndcg@5: 0.976037\tvalid_1's ndcg@1: 0.528705\tvalid_1's ndcg@2: 0.604333\tvalid_1's ndcg@3: 0.663376\tvalid_1's ndcg@4: 0.714313\tvalid_1's ndcg@5: 0.756667\n",
      "[160]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.979029\ttraining's ndcg@3: 0.974347\ttraining's ndcg@4: 0.97252\ttraining's ndcg@5: 0.976231\tvalid_1's ndcg@1: 0.51792\tvalid_1's ndcg@2: 0.6019\tvalid_1's ndcg@3: 0.662994\tvalid_1's ndcg@4: 0.715281\tvalid_1's ndcg@5: 0.755447\n",
      "[161]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.979199\ttraining's ndcg@3: 0.97438\ttraining's ndcg@4: 0.972628\ttraining's ndcg@5: 0.97622\tvalid_1's ndcg@1: 0.519334\tvalid_1's ndcg@2: 0.604\tvalid_1's ndcg@3: 0.664232\tvalid_1's ndcg@4: 0.717596\tvalid_1's ndcg@5: 0.756329\n",
      "[162]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.979199\ttraining's ndcg@3: 0.974669\ttraining's ndcg@4: 0.972686\ttraining's ndcg@5: 0.976292\tvalid_1's ndcg@1: 0.519687\tvalid_1's ndcg@2: 0.603418\tvalid_1's ndcg@3: 0.664155\tvalid_1's ndcg@4: 0.717834\tvalid_1's ndcg@5: 0.757365\n",
      "[163]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.980862\ttraining's ndcg@3: 0.975416\ttraining's ndcg@4: 0.973099\ttraining's ndcg@5: 0.976622\tvalid_1's ndcg@1: 0.529631\tvalid_1's ndcg@2: 0.607238\tvalid_1's ndcg@3: 0.66672\tvalid_1's ndcg@4: 0.718117\tvalid_1's ndcg@5: 0.761507\n",
      "[164]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.981032\ttraining's ndcg@3: 0.975539\ttraining's ndcg@4: 0.973271\ttraining's ndcg@5: 0.976662\tvalid_1's ndcg@1: 0.524849\tvalid_1's ndcg@2: 0.605895\tvalid_1's ndcg@3: 0.668135\tvalid_1's ndcg@4: 0.71851\tvalid_1's ndcg@5: 0.758753\n",
      "[165]\ttraining's ndcg@1: 0.986489\ttraining's ndcg@2: 0.981288\ttraining's ndcg@3: 0.97568\ttraining's ndcg@4: 0.973416\ttraining's ndcg@5: 0.976887\tvalid_1's ndcg@1: 0.528502\tvalid_1's ndcg@2: 0.610312\tvalid_1's ndcg@3: 0.671556\tvalid_1's ndcg@4: 0.719312\tvalid_1's ndcg@5: 0.761995\n",
      "[166]\ttraining's ndcg@1: 0.9872\ttraining's ndcg@2: 0.981658\ttraining's ndcg@3: 0.976204\ttraining's ndcg@4: 0.973677\ttraining's ndcg@5: 0.977204\tvalid_1's ndcg@1: 0.527441\tvalid_1's ndcg@2: 0.609845\tvalid_1's ndcg@3: 0.668672\tvalid_1's ndcg@4: 0.720262\tvalid_1's ndcg@5: 0.762044\n",
      "[167]\ttraining's ndcg@1: 0.987913\ttraining's ndcg@2: 0.981859\ttraining's ndcg@3: 0.976387\ttraining's ndcg@4: 0.973905\ttraining's ndcg@5: 0.977365\tvalid_1's ndcg@1: 0.524827\tvalid_1's ndcg@2: 0.610216\tvalid_1's ndcg@3: 0.669062\tvalid_1's ndcg@4: 0.720019\tvalid_1's ndcg@5: 0.761112\n",
      "[168]\ttraining's ndcg@1: 0.988624\ttraining's ndcg@2: 0.982314\ttraining's ndcg@3: 0.976618\ttraining's ndcg@4: 0.974128\ttraining's ndcg@5: 0.977628\tvalid_1's ndcg@1: 0.528718\tvalid_1's ndcg@2: 0.606388\tvalid_1's ndcg@3: 0.671646\tvalid_1's ndcg@4: 0.719519\tvalid_1's ndcg@5: 0.761416\n",
      "[169]\ttraining's ndcg@1: 0.988624\ttraining's ndcg@2: 0.982741\ttraining's ndcg@3: 0.976791\ttraining's ndcg@4: 0.974399\ttraining's ndcg@5: 0.97774\tvalid_1's ndcg@1: 0.523413\tvalid_1's ndcg@2: 0.607784\tvalid_1's ndcg@3: 0.670894\tvalid_1's ndcg@4: 0.718285\tvalid_1's ndcg@5: 0.760606\n",
      "[170]\ttraining's ndcg@1: 0.988624\ttraining's ndcg@2: 0.982911\ttraining's ndcg@3: 0.976824\ttraining's ndcg@4: 0.974475\ttraining's ndcg@5: 0.977843\tvalid_1's ndcg@1: 0.530485\tvalid_1's ndcg@2: 0.606717\tvalid_1's ndcg@3: 0.673231\tvalid_1's ndcg@4: 0.720899\tvalid_1's ndcg@5: 0.761662\n",
      "[171]\ttraining's ndcg@1: 0.989335\ttraining's ndcg@2: 0.983409\ttraining's ndcg@3: 0.977093\ttraining's ndcg@4: 0.974741\ttraining's ndcg@5: 0.977969\tvalid_1's ndcg@1: 0.530485\tvalid_1's ndcg@2: 0.610533\tvalid_1's ndcg@3: 0.671802\tvalid_1's ndcg@4: 0.722438\tvalid_1's ndcg@5: 0.762604\n",
      "[172]\ttraining's ndcg@1: 0.990046\ttraining's ndcg@2: 0.983694\ttraining's ndcg@3: 0.977354\ttraining's ndcg@4: 0.974954\ttraining's ndcg@5: 0.978257\tvalid_1's ndcg@1: 0.530485\tvalid_1's ndcg@2: 0.610033\tvalid_1's ndcg@3: 0.672935\tvalid_1's ndcg@4: 0.720949\tvalid_1's ndcg@5: 0.762071\n",
      "[173]\ttraining's ndcg@1: 0.990046\ttraining's ndcg@2: 0.983694\ttraining's ndcg@3: 0.977539\ttraining's ndcg@4: 0.975231\ttraining's ndcg@5: 0.978365\tvalid_1's ndcg@1: 0.525932\tvalid_1's ndcg@2: 0.608247\tvalid_1's ndcg@3: 0.67026\tvalid_1's ndcg@4: 0.719203\tvalid_1's ndcg@5: 0.759682\n",
      "[174]\ttraining's ndcg@1: 0.990757\ttraining's ndcg@2: 0.983893\ttraining's ndcg@3: 0.977906\ttraining's ndcg@4: 0.975678\ttraining's ndcg@5: 0.97872\tvalid_1's ndcg@1: 0.525932\tvalid_1's ndcg@2: 0.608322\tvalid_1's ndcg@3: 0.670093\tvalid_1's ndcg@4: 0.718687\tvalid_1's ndcg@5: 0.759674\n",
      "[175]\ttraining's ndcg@1: 0.991468\ttraining's ndcg@2: 0.984093\ttraining's ndcg@3: 0.978042\ttraining's ndcg@4: 0.975742\ttraining's ndcg@5: 0.979174\tvalid_1's ndcg@1: 0.525932\tvalid_1's ndcg@2: 0.609933\tvalid_1's ndcg@3: 0.670766\tvalid_1's ndcg@4: 0.720376\tvalid_1's ndcg@5: 0.760395\n",
      "[176]\ttraining's ndcg@1: 0.991468\ttraining's ndcg@2: 0.984391\ttraining's ndcg@3: 0.978315\ttraining's ndcg@4: 0.97597\ttraining's ndcg@5: 0.979322\tvalid_1's ndcg@1: 0.52487\tvalid_1's ndcg@2: 0.609635\tvalid_1's ndcg@3: 0.671046\tvalid_1's ndcg@4: 0.720637\tvalid_1's ndcg@5: 0.760255\n",
      "[177]\ttraining's ndcg@1: 0.991468\ttraining's ndcg@2: 0.984391\ttraining's ndcg@3: 0.978315\ttraining's ndcg@4: 0.975925\ttraining's ndcg@5: 0.979351\tvalid_1's ndcg@1: 0.529897\tvalid_1's ndcg@2: 0.607652\tvalid_1's ndcg@3: 0.670189\tvalid_1's ndcg@4: 0.721035\tvalid_1's ndcg@5: 0.760331\n",
      "[178]\ttraining's ndcg@1: 0.992178\ttraining's ndcg@2: 0.984591\ttraining's ndcg@3: 0.978436\ttraining's ndcg@4: 0.976367\ttraining's ndcg@5: 0.979563\tvalid_1's ndcg@1: 0.529191\tvalid_1's ndcg@2: 0.602109\tvalid_1's ndcg@3: 0.665216\tvalid_1's ndcg@4: 0.715903\tvalid_1's ndcg@5: 0.758746\n",
      "[179]\ttraining's ndcg@1: 0.992889\ttraining's ndcg@2: 0.985131\ttraining's ndcg@3: 0.979099\ttraining's ndcg@4: 0.976702\ttraining's ndcg@5: 0.979834\tvalid_1's ndcg@1: 0.524946\tvalid_1's ndcg@2: 0.604309\tvalid_1's ndcg@3: 0.665507\tvalid_1's ndcg@4: 0.713941\tvalid_1's ndcg@5: 0.759721\n",
      "[180]\ttraining's ndcg@1: 0.9936\ttraining's ndcg@2: 0.985331\ttraining's ndcg@3: 0.979451\ttraining's ndcg@4: 0.977254\ttraining's ndcg@5: 0.980107\tvalid_1's ndcg@1: 0.526713\tvalid_1's ndcg@2: 0.60344\tvalid_1's ndcg@3: 0.664964\tvalid_1's ndcg@4: 0.715741\tvalid_1's ndcg@5: 0.760002\n",
      "[181]\ttraining's ndcg@1: 0.994312\ttraining's ndcg@2: 0.985531\ttraining's ndcg@3: 0.979914\ttraining's ndcg@4: 0.977461\ttraining's ndcg@5: 0.980673\tvalid_1's ndcg@1: 0.526712\tvalid_1's ndcg@2: 0.603948\tvalid_1's ndcg@3: 0.66506\tvalid_1's ndcg@4: 0.71644\tvalid_1's ndcg@5: 0.760852\n",
      "[182]\ttraining's ndcg@1: 0.994312\ttraining's ndcg@2: 0.985531\ttraining's ndcg@3: 0.979914\ttraining's ndcg@4: 0.977518\ttraining's ndcg@5: 0.980741\tvalid_1's ndcg@1: 0.527067\tvalid_1's ndcg@2: 0.603314\tvalid_1's ndcg@3: 0.664081\tvalid_1's ndcg@4: 0.716929\tvalid_1's ndcg@5: 0.759553\n",
      "[183]\ttraining's ndcg@1: 0.995023\ttraining's ndcg@2: 0.986029\ttraining's ndcg@3: 0.980399\ttraining's ndcg@4: 0.978009\ttraining's ndcg@5: 0.981057\tvalid_1's ndcg@1: 0.524231\tvalid_1's ndcg@2: 0.600046\tvalid_1's ndcg@3: 0.661838\tvalid_1's ndcg@4: 0.715149\tvalid_1's ndcg@5: 0.758229\n",
      "[184]\ttraining's ndcg@1: 0.995023\ttraining's ndcg@2: 0.986029\ttraining's ndcg@3: 0.980523\ttraining's ndcg@4: 0.978018\ttraining's ndcg@5: 0.981265\tvalid_1's ndcg@1: 0.527773\tvalid_1's ndcg@2: 0.602652\tvalid_1's ndcg@3: 0.666679\tvalid_1's ndcg@4: 0.715152\tvalid_1's ndcg@5: 0.759695\n",
      "[185]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.986228\ttraining's ndcg@3: 0.980705\ttraining's ndcg@4: 0.978353\ttraining's ndcg@5: 0.981443\tvalid_1's ndcg@1: 0.535723\tvalid_1's ndcg@2: 0.608546\tvalid_1's ndcg@3: 0.668733\tvalid_1's ndcg@4: 0.717544\tvalid_1's ndcg@5: 0.762748\n",
      "[186]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.986228\ttraining's ndcg@3: 0.980891\ttraining's ndcg@4: 0.978316\ttraining's ndcg@5: 0.981468\tvalid_1's ndcg@1: 0.533081\tvalid_1's ndcg@2: 0.607805\tvalid_1's ndcg@3: 0.669011\tvalid_1's ndcg@4: 0.718607\tvalid_1's ndcg@5: 0.762254\n",
      "[187]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.986399\ttraining's ndcg@3: 0.981317\ttraining's ndcg@4: 0.978455\ttraining's ndcg@5: 0.98164\tvalid_1's ndcg@1: 0.527243\tvalid_1's ndcg@2: 0.604471\tvalid_1's ndcg@3: 0.665462\tvalid_1's ndcg@4: 0.716621\tvalid_1's ndcg@5: 0.760434\n",
      "[188]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.986569\ttraining's ndcg@3: 0.981403\ttraining's ndcg@4: 0.978659\ttraining's ndcg@5: 0.98178\tvalid_1's ndcg@1: 0.532727\tvalid_1's ndcg@2: 0.60601\tvalid_1's ndcg@3: 0.665668\tvalid_1's ndcg@4: 0.718046\tvalid_1's ndcg@5: 0.76373\n",
      "[189]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.98674\ttraining's ndcg@3: 0.981435\ttraining's ndcg@4: 0.978944\ttraining's ndcg@5: 0.981841\tvalid_1's ndcg@1: 0.530253\tvalid_1's ndcg@2: 0.607695\tvalid_1's ndcg@3: 0.667522\tvalid_1's ndcg@4: 0.717965\tvalid_1's ndcg@5: 0.762266\n",
      "[190]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.987082\ttraining's ndcg@3: 0.98167\ttraining's ndcg@4: 0.978998\ttraining's ndcg@5: 0.981982\tvalid_1's ndcg@1: 0.52177\tvalid_1's ndcg@2: 0.607436\tvalid_1's ndcg@3: 0.66394\tvalid_1's ndcg@4: 0.716575\tvalid_1's ndcg@5: 0.759163\n",
      "[191]\ttraining's ndcg@1: 0.995734\ttraining's ndcg@2: 0.987253\ttraining's ndcg@3: 0.982057\ttraining's ndcg@4: 0.979229\ttraining's ndcg@5: 0.981991\tvalid_1's ndcg@1: 0.525305\tvalid_1's ndcg@2: 0.60934\tvalid_1's ndcg@3: 0.665203\tvalid_1's ndcg@4: 0.716256\tvalid_1's ndcg@5: 0.760568\n",
      "[192]\ttraining's ndcg@1: 0.996445\ttraining's ndcg@2: 0.987452\ttraining's ndcg@3: 0.982209\ttraining's ndcg@4: 0.979494\ttraining's ndcg@5: 0.982257\tvalid_1's ndcg@1: 0.531667\tvalid_1's ndcg@2: 0.611719\tvalid_1's ndcg@3: 0.666017\tvalid_1's ndcg@4: 0.719348\tvalid_1's ndcg@5: 0.760855\n",
      "[193]\ttraining's ndcg@1: 0.996445\ttraining's ndcg@2: 0.987452\ttraining's ndcg@3: 0.982533\ttraining's ndcg@4: 0.979621\ttraining's ndcg@5: 0.982452\tvalid_1's ndcg@1: 0.525238\tvalid_1's ndcg@2: 0.610105\tvalid_1's ndcg@3: 0.665847\tvalid_1's ndcg@4: 0.718279\tvalid_1's ndcg@5: 0.758169\n",
      "[194]\ttraining's ndcg@1: 0.996445\ttraining's ndcg@2: 0.987794\ttraining's ndcg@3: 0.982613\ttraining's ndcg@4: 0.979852\ttraining's ndcg@5: 0.982515\tvalid_1's ndcg@1: 0.527359\tvalid_1's ndcg@2: 0.609889\tvalid_1's ndcg@3: 0.666113\tvalid_1's ndcg@4: 0.719503\tvalid_1's ndcg@5: 0.758359\n",
      "[195]\ttraining's ndcg@1: 0.996445\ttraining's ndcg@2: 0.987964\ttraining's ndcg@3: 0.982645\ttraining's ndcg@4: 0.97987\ttraining's ndcg@5: 0.982504\tvalid_1's ndcg@1: 0.530982\tvalid_1's ndcg@2: 0.610905\tvalid_1's ndcg@3: 0.66655\tvalid_1's ndcg@4: 0.72192\tvalid_1's ndcg@5: 0.759949\n",
      "[196]\ttraining's ndcg@1: 0.996445\ttraining's ndcg@2: 0.988135\ttraining's ndcg@3: 0.982678\ttraining's ndcg@4: 0.979888\ttraining's ndcg@5: 0.982511\tvalid_1's ndcg@1: 0.530982\tvalid_1's ndcg@2: 0.608522\tvalid_1's ndcg@3: 0.666098\tvalid_1's ndcg@4: 0.721432\tvalid_1's ndcg@5: 0.756622\n",
      "[197]\ttraining's ndcg@1: 0.996445\ttraining's ndcg@2: 0.988135\ttraining's ndcg@3: 0.98277\ttraining's ndcg@4: 0.980228\ttraining's ndcg@5: 0.982635\tvalid_1's ndcg@1: 0.53593\tvalid_1's ndcg@2: 0.613396\tvalid_1's ndcg@3: 0.668119\tvalid_1's ndcg@4: 0.723293\tvalid_1's ndcg@5: 0.757377\n",
      "[198]\ttraining's ndcg@1: 0.997156\ttraining's ndcg@2: 0.988335\ttraining's ndcg@3: 0.9832\ttraining's ndcg@4: 0.980502\ttraining's ndcg@5: 0.982791\tvalid_1's ndcg@1: 0.526387\tvalid_1's ndcg@2: 0.60808\tvalid_1's ndcg@3: 0.665434\tvalid_1's ndcg@4: 0.719009\tvalid_1's ndcg@5: 0.755731\n",
      "[199]\ttraining's ndcg@1: 0.997156\ttraining's ndcg@2: 0.988335\ttraining's ndcg@3: 0.9832\ttraining's ndcg@4: 0.980726\ttraining's ndcg@5: 0.98278\tvalid_1's ndcg@1: 0.531335\tvalid_1's ndcg@2: 0.609436\tvalid_1's ndcg@3: 0.666853\tvalid_1's ndcg@4: 0.719744\tvalid_1's ndcg@5: 0.758297\n",
      "[200]\ttraining's ndcg@1: 0.997156\ttraining's ndcg@2: 0.988548\ttraining's ndcg@3: 0.983302\ttraining's ndcg@4: 0.98085\ttraining's ndcg@5: 0.983034\tvalid_1's ndcg@1: 0.531335\tvalid_1's ndcg@2: 0.606205\tvalid_1's ndcg@3: 0.665378\tvalid_1's ndcg@4: 0.717509\tvalid_1's ndcg@5: 0.758192\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 5).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "# Split the data into input features (X) and target labels (y)\n",
    "X_train = train_data[columns_to_normalize]\n",
    "y_train = train_data['rank']\n",
    "\n",
    "X_test = test_data[columns_to_normalize]\n",
    "y_test = test_data['rank']\n",
    "\n",
    "train_groups = train_data.groupby('keyword').size().to_frame('size')['size'].to_numpy()\n",
    "test_groups = test_data.groupby('keyword').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "# Create DMatrix for training set\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtrain.set_group(train_groups)\n",
    "\n",
    "# Create DMatrix for test set\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "dtest.set_group(test_groups)\n",
    "\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': ['ndcg'],\n",
    "    'random_state': 21,\n",
    "    'lambdarank_num_pair_per_sample': 80,\n",
    "    'lambdarank_pair_method': 'mean',\n",
    "#     'max_depth': 6,\n",
    "    'reg_lambda': 1\n",
    "}\n",
    "# Train the model\n",
    "lgb_train = lgb.Dataset(X_train, y_train, group=train_groups)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, group=test_groups)\n",
    "ranker = lgb.train(params, lgb_train, num_boost_round=200, valid_sets=[lgb_train, lgb_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "68ad4c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9449905583622902, 0.9603936131623311, 1.0, 0.7124017207167848)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores3c =[]\n",
    "for query, group in test_data.groupby('keyword'):\n",
    "    if len(group['rank'].tolist()) != 1:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "        scores3c.append(nd(np.asarray([ranks]), np.asarray([ranker.predict(features).tolist()])))\n",
    "np.mean(scores3c), np.median(scores3c), max(scores3c), min(scores3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430176d4",
   "metadata": {},
   "source": [
    "### LambdaMart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481896b",
   "metadata": {},
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ca074571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:62.35318\tvalidation_0-ndcg:0.78264\tvalidation_1-logloss:67.52492\tvalidation_1-ndcg:0.72843\n",
      "[1]\tvalidation_0-logloss:77.40292\tvalidation_0-ndcg:0.85277\tvalidation_1-logloss:88.50595\tvalidation_1-ndcg:0.74418\n",
      "[2]\tvalidation_0-logloss:89.58074\tvalidation_0-ndcg:0.87910\tvalidation_1-logloss:99.45017\tvalidation_1-ndcg:0.74725\n",
      "[3]\tvalidation_0-logloss:95.71065\tvalidation_0-ndcg:0.89768\tvalidation_1-logloss:107.26710\tvalidation_1-ndcg:0.75971\n",
      "[4]\tvalidation_0-logloss:102.32850\tvalidation_0-ndcg:0.90708\tvalidation_1-logloss:109.62136\tvalidation_1-ndcg:0.76241\n",
      "[5]\tvalidation_0-logloss:105.07750\tvalidation_0-ndcg:0.91965\tvalidation_1-logloss:114.48542\tvalidation_1-ndcg:0.76986\n",
      "[6]\tvalidation_0-logloss:107.17785\tvalidation_0-ndcg:0.92569\tvalidation_1-logloss:114.37132\tvalidation_1-ndcg:0.78301\n",
      "[7]\tvalidation_0-logloss:108.43972\tvalidation_0-ndcg:0.93150\tvalidation_1-logloss:118.29738\tvalidation_1-ndcg:0.78659\n",
      "[8]\tvalidation_0-logloss:108.94164\tvalidation_0-ndcg:0.93625\tvalidation_1-logloss:117.26642\tvalidation_1-ndcg:0.78553\n",
      "[9]\tvalidation_0-logloss:109.92456\tvalidation_0-ndcg:0.94128\tvalidation_1-logloss:119.89747\tvalidation_1-ndcg:0.78323\n",
      "[10]\tvalidation_0-logloss:110.16459\tvalidation_0-ndcg:0.94399\tvalidation_1-logloss:119.76740\tvalidation_1-ndcg:0.78306\n",
      "[11]\tvalidation_0-logloss:111.45067\tvalidation_0-ndcg:0.94534\tvalidation_1-logloss:121.73436\tvalidation_1-ndcg:0.78115\n",
      "[12]\tvalidation_0-logloss:111.96648\tvalidation_0-ndcg:0.94826\tvalidation_1-logloss:120.98607\tvalidation_1-ndcg:0.78098\n",
      "[13]\tvalidation_0-logloss:111.80564\tvalidation_0-ndcg:0.95037\tvalidation_1-logloss:119.45633\tvalidation_1-ndcg:0.78332\n",
      "[14]\tvalidation_0-logloss:112.18919\tvalidation_0-ndcg:0.95188\tvalidation_1-logloss:121.48049\tvalidation_1-ndcg:0.78899\n",
      "[15]\tvalidation_0-logloss:113.13551\tvalidation_0-ndcg:0.95389\tvalidation_1-logloss:121.36400\tvalidation_1-ndcg:0.78833\n",
      "[16]\tvalidation_0-logloss:113.86101\tvalidation_0-ndcg:0.95668\tvalidation_1-logloss:122.16000\tvalidation_1-ndcg:0.78154\n",
      "[17]\tvalidation_0-logloss:114.43151\tvalidation_0-ndcg:0.95724\tvalidation_1-logloss:123.25297\tvalidation_1-ndcg:0.78182\n",
      "[18]\tvalidation_0-logloss:114.76514\tvalidation_0-ndcg:0.95804\tvalidation_1-logloss:122.44002\tvalidation_1-ndcg:0.78701\n",
      "[19]\tvalidation_0-logloss:114.91468\tvalidation_0-ndcg:0.96015\tvalidation_1-logloss:122.59803\tvalidation_1-ndcg:0.78755\n",
      "[20]\tvalidation_0-logloss:114.74568\tvalidation_0-ndcg:0.96122\tvalidation_1-logloss:123.23015\tvalidation_1-ndcg:0.79010\n",
      "[21]\tvalidation_0-logloss:115.24314\tvalidation_0-ndcg:0.96186\tvalidation_1-logloss:124.45102\tvalidation_1-ndcg:0.79068\n",
      "[22]\tvalidation_0-logloss:115.71005\tvalidation_0-ndcg:0.96276\tvalidation_1-logloss:123.40777\tvalidation_1-ndcg:0.79318\n",
      "[23]\tvalidation_0-logloss:115.48801\tvalidation_0-ndcg:0.96401\tvalidation_1-logloss:123.01084\tvalidation_1-ndcg:0.79294\n",
      "[24]\tvalidation_0-logloss:115.94013\tvalidation_0-ndcg:0.96499\tvalidation_1-logloss:123.29961\tvalidation_1-ndcg:0.79213\n",
      "[25]\tvalidation_0-logloss:116.01121\tvalidation_0-ndcg:0.96555\tvalidation_1-logloss:124.86322\tvalidation_1-ndcg:0.79447\n",
      "[26]\tvalidation_0-logloss:116.41344\tvalidation_0-ndcg:0.96618\tvalidation_1-logloss:124.66098\tvalidation_1-ndcg:0.78970\n",
      "[27]\tvalidation_0-logloss:116.54328\tvalidation_0-ndcg:0.96697\tvalidation_1-logloss:124.10357\tvalidation_1-ndcg:0.79418\n",
      "[28]\tvalidation_0-logloss:116.76597\tvalidation_0-ndcg:0.96784\tvalidation_1-logloss:124.54953\tvalidation_1-ndcg:0.79553\n",
      "[29]\tvalidation_0-logloss:116.42453\tvalidation_0-ndcg:0.96904\tvalidation_1-logloss:124.67759\tvalidation_1-ndcg:0.79496\n",
      "[30]\tvalidation_0-logloss:116.25933\tvalidation_0-ndcg:0.96918\tvalidation_1-logloss:124.99538\tvalidation_1-ndcg:0.79218\n",
      "[31]\tvalidation_0-logloss:116.50430\tvalidation_0-ndcg:0.96948\tvalidation_1-logloss:124.95753\tvalidation_1-ndcg:0.79393\n",
      "[32]\tvalidation_0-logloss:115.88337\tvalidation_0-ndcg:0.97003\tvalidation_1-logloss:124.68610\tvalidation_1-ndcg:0.79264\n",
      "[33]\tvalidation_0-logloss:115.84508\tvalidation_0-ndcg:0.97072\tvalidation_1-logloss:124.95469\tvalidation_1-ndcg:0.79003\n",
      "[34]\tvalidation_0-logloss:115.63388\tvalidation_0-ndcg:0.97114\tvalidation_1-logloss:125.77610\tvalidation_1-ndcg:0.79073\n",
      "[35]\tvalidation_0-logloss:115.52095\tvalidation_0-ndcg:0.97140\tvalidation_1-logloss:125.82466\tvalidation_1-ndcg:0.79269\n",
      "[36]\tvalidation_0-logloss:115.09918\tvalidation_0-ndcg:0.97232\tvalidation_1-logloss:125.66231\tvalidation_1-ndcg:0.79369\n",
      "[37]\tvalidation_0-logloss:114.35254\tvalidation_0-ndcg:0.97262\tvalidation_1-logloss:125.02171\tvalidation_1-ndcg:0.79212\n",
      "[38]\tvalidation_0-logloss:113.65302\tvalidation_0-ndcg:0.97298\tvalidation_1-logloss:125.83401\tvalidation_1-ndcg:0.79433\n",
      "[39]\tvalidation_0-logloss:113.16128\tvalidation_0-ndcg:0.97366\tvalidation_1-logloss:125.87571\tvalidation_1-ndcg:0.79452\n",
      "[40]\tvalidation_0-logloss:112.34884\tvalidation_0-ndcg:0.97384\tvalidation_1-logloss:126.01850\tvalidation_1-ndcg:0.79368\n",
      "[41]\tvalidation_0-logloss:111.95402\tvalidation_0-ndcg:0.97397\tvalidation_1-logloss:125.50906\tvalidation_1-ndcg:0.79560\n",
      "[42]\tvalidation_0-logloss:111.67948\tvalidation_0-ndcg:0.97420\tvalidation_1-logloss:126.19420\tvalidation_1-ndcg:0.79879\n",
      "[43]\tvalidation_0-logloss:111.24147\tvalidation_0-ndcg:0.97432\tvalidation_1-logloss:126.45004\tvalidation_1-ndcg:0.80105\n",
      "[44]\tvalidation_0-logloss:110.84615\tvalidation_0-ndcg:0.97463\tvalidation_1-logloss:126.47935\tvalidation_1-ndcg:0.80122\n",
      "[45]\tvalidation_0-logloss:110.35875\tvalidation_0-ndcg:0.97488\tvalidation_1-logloss:126.53983\tvalidation_1-ndcg:0.79753\n",
      "[46]\tvalidation_0-logloss:110.18084\tvalidation_0-ndcg:0.97515\tvalidation_1-logloss:127.06305\tvalidation_1-ndcg:0.80139\n",
      "[47]\tvalidation_0-logloss:109.76375\tvalidation_0-ndcg:0.97539\tvalidation_1-logloss:126.30568\tvalidation_1-ndcg:0.80074\n",
      "[48]\tvalidation_0-logloss:109.26890\tvalidation_0-ndcg:0.97573\tvalidation_1-logloss:127.04196\tvalidation_1-ndcg:0.80219\n",
      "[49]\tvalidation_0-logloss:108.50494\tvalidation_0-ndcg:0.97578\tvalidation_1-logloss:126.75717\tvalidation_1-ndcg:0.80160\n",
      "[50]\tvalidation_0-logloss:108.03088\tvalidation_0-ndcg:0.97582\tvalidation_1-logloss:126.19818\tvalidation_1-ndcg:0.79987\n",
      "[51]\tvalidation_0-logloss:107.52116\tvalidation_0-ndcg:0.97602\tvalidation_1-logloss:125.90374\tvalidation_1-ndcg:0.79864\n",
      "[52]\tvalidation_0-logloss:106.90472\tvalidation_0-ndcg:0.97599\tvalidation_1-logloss:126.79628\tvalidation_1-ndcg:0.80035\n",
      "[53]\tvalidation_0-logloss:106.48789\tvalidation_0-ndcg:0.97604\tvalidation_1-logloss:127.30984\tvalidation_1-ndcg:0.79983\n",
      "[54]\tvalidation_0-logloss:106.09980\tvalidation_0-ndcg:0.97614\tvalidation_1-logloss:127.74832\tvalidation_1-ndcg:0.79930\n",
      "[55]\tvalidation_0-logloss:105.64745\tvalidation_0-ndcg:0.97606\tvalidation_1-logloss:127.17657\tvalidation_1-ndcg:0.80061\n",
      "[56]\tvalidation_0-logloss:105.10290\tvalidation_0-ndcg:0.97613\tvalidation_1-logloss:127.37006\tvalidation_1-ndcg:0.80105\n",
      "[57]\tvalidation_0-logloss:104.99770\tvalidation_0-ndcg:0.97624\tvalidation_1-logloss:127.84601\tvalidation_1-ndcg:0.80073\n",
      "[58]\tvalidation_0-logloss:104.59972\tvalidation_0-ndcg:0.97640\tvalidation_1-logloss:127.78090\tvalidation_1-ndcg:0.80010\n",
      "[59]\tvalidation_0-logloss:104.31251\tvalidation_0-ndcg:0.97693\tvalidation_1-logloss:127.01414\tvalidation_1-ndcg:0.80211\n",
      "[60]\tvalidation_0-logloss:103.64906\tvalidation_0-ndcg:0.97716\tvalidation_1-logloss:127.24356\tvalidation_1-ndcg:0.80296\n",
      "[61]\tvalidation_0-logloss:103.59010\tvalidation_0-ndcg:0.97722\tvalidation_1-logloss:127.40373\tvalidation_1-ndcg:0.80083\n",
      "[62]\tvalidation_0-logloss:103.50457\tvalidation_0-ndcg:0.97743\tvalidation_1-logloss:126.59794\tvalidation_1-ndcg:0.80209\n",
      "[63]\tvalidation_0-logloss:103.06396\tvalidation_0-ndcg:0.97774\tvalidation_1-logloss:125.94738\tvalidation_1-ndcg:0.80423\n",
      "[64]\tvalidation_0-logloss:102.60565\tvalidation_0-ndcg:0.97779\tvalidation_1-logloss:125.67729\tvalidation_1-ndcg:0.80463\n",
      "[65]\tvalidation_0-logloss:102.33298\tvalidation_0-ndcg:0.97802\tvalidation_1-logloss:125.65247\tvalidation_1-ndcg:0.80492\n",
      "[66]\tvalidation_0-logloss:102.20432\tvalidation_0-ndcg:0.97822\tvalidation_1-logloss:125.77375\tvalidation_1-ndcg:0.80381\n",
      "[67]\tvalidation_0-logloss:101.99237\tvalidation_0-ndcg:0.97825\tvalidation_1-logloss:125.75629\tvalidation_1-ndcg:0.80365\n",
      "[68]\tvalidation_0-logloss:101.43231\tvalidation_0-ndcg:0.97841\tvalidation_1-logloss:126.12985\tvalidation_1-ndcg:0.80079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\tvalidation_0-logloss:100.96670\tvalidation_0-ndcg:0.97854\tvalidation_1-logloss:125.28407\tvalidation_1-ndcg:0.80223\n",
      "[70]\tvalidation_0-logloss:100.69875\tvalidation_0-ndcg:0.97871\tvalidation_1-logloss:125.31749\tvalidation_1-ndcg:0.80444\n",
      "[71]\tvalidation_0-logloss:100.90508\tvalidation_0-ndcg:0.97880\tvalidation_1-logloss:125.48418\tvalidation_1-ndcg:0.80204\n",
      "[72]\tvalidation_0-logloss:100.75086\tvalidation_0-ndcg:0.97887\tvalidation_1-logloss:125.09268\tvalidation_1-ndcg:0.80192\n",
      "[73]\tvalidation_0-logloss:100.40971\tvalidation_0-ndcg:0.97894\tvalidation_1-logloss:124.48037\tvalidation_1-ndcg:0.80134\n",
      "[74]\tvalidation_0-logloss:100.25984\tvalidation_0-ndcg:0.97907\tvalidation_1-logloss:124.88113\tvalidation_1-ndcg:0.80044\n",
      "[75]\tvalidation_0-logloss:100.15018\tvalidation_0-ndcg:0.97928\tvalidation_1-logloss:125.01572\tvalidation_1-ndcg:0.80065\n",
      "[76]\tvalidation_0-logloss:100.29378\tvalidation_0-ndcg:0.97924\tvalidation_1-logloss:124.75462\tvalidation_1-ndcg:0.80048\n",
      "[77]\tvalidation_0-logloss:100.11190\tvalidation_0-ndcg:0.97946\tvalidation_1-logloss:125.12078\tvalidation_1-ndcg:0.80035\n",
      "[78]\tvalidation_0-logloss:99.89192\tvalidation_0-ndcg:0.97965\tvalidation_1-logloss:124.91932\tvalidation_1-ndcg:0.80057\n",
      "[79]\tvalidation_0-logloss:99.71537\tvalidation_0-ndcg:0.97979\tvalidation_1-logloss:124.81459\tvalidation_1-ndcg:0.80047\n",
      "[80]\tvalidation_0-logloss:99.52536\tvalidation_0-ndcg:0.97986\tvalidation_1-logloss:124.74342\tvalidation_1-ndcg:0.80112\n",
      "[81]\tvalidation_0-logloss:99.26425\tvalidation_0-ndcg:0.98015\tvalidation_1-logloss:124.84598\tvalidation_1-ndcg:0.80139\n",
      "[82]\tvalidation_0-logloss:99.06334\tvalidation_0-ndcg:0.98027\tvalidation_1-logloss:124.98775\tvalidation_1-ndcg:0.80323\n",
      "[83]\tvalidation_0-logloss:98.66518\tvalidation_0-ndcg:0.98032\tvalidation_1-logloss:124.86243\tvalidation_1-ndcg:0.80304\n",
      "[84]\tvalidation_0-logloss:98.49694\tvalidation_0-ndcg:0.98041\tvalidation_1-logloss:124.37690\tvalidation_1-ndcg:0.80373\n",
      "[85]\tvalidation_0-logloss:98.18881\tvalidation_0-ndcg:0.98061\tvalidation_1-logloss:124.61846\tvalidation_1-ndcg:0.80450\n",
      "[86]\tvalidation_0-logloss:98.37438\tvalidation_0-ndcg:0.98083\tvalidation_1-logloss:125.44940\tvalidation_1-ndcg:0.80348\n",
      "[87]\tvalidation_0-logloss:98.34692\tvalidation_0-ndcg:0.98092\tvalidation_1-logloss:125.25466\tvalidation_1-ndcg:0.80325\n",
      "[88]\tvalidation_0-logloss:98.21909\tvalidation_0-ndcg:0.98123\tvalidation_1-logloss:125.35435\tvalidation_1-ndcg:0.80011\n",
      "[89]\tvalidation_0-logloss:98.16738\tvalidation_0-ndcg:0.98128\tvalidation_1-logloss:124.96806\tvalidation_1-ndcg:0.79984\n",
      "[90]\tvalidation_0-logloss:98.05248\tvalidation_0-ndcg:0.98148\tvalidation_1-logloss:125.47040\tvalidation_1-ndcg:0.80113\n",
      "[91]\tvalidation_0-logloss:97.81092\tvalidation_0-ndcg:0.98155\tvalidation_1-logloss:125.22607\tvalidation_1-ndcg:0.79965\n",
      "[92]\tvalidation_0-logloss:97.95770\tvalidation_0-ndcg:0.98192\tvalidation_1-logloss:124.85181\tvalidation_1-ndcg:0.80139\n",
      "[93]\tvalidation_0-logloss:97.60024\tvalidation_0-ndcg:0.98215\tvalidation_1-logloss:124.96346\tvalidation_1-ndcg:0.80002\n",
      "[94]\tvalidation_0-logloss:97.60938\tvalidation_0-ndcg:0.98222\tvalidation_1-logloss:125.46887\tvalidation_1-ndcg:0.80219\n",
      "[95]\tvalidation_0-logloss:97.50395\tvalidation_0-ndcg:0.98231\tvalidation_1-logloss:125.36296\tvalidation_1-ndcg:0.80384\n",
      "[96]\tvalidation_0-logloss:97.14247\tvalidation_0-ndcg:0.98228\tvalidation_1-logloss:125.55732\tvalidation_1-ndcg:0.80365\n",
      "[97]\tvalidation_0-logloss:97.08119\tvalidation_0-ndcg:0.98238\tvalidation_1-logloss:124.79354\tvalidation_1-ndcg:0.80340\n",
      "[98]\tvalidation_0-logloss:97.20313\tvalidation_0-ndcg:0.98239\tvalidation_1-logloss:124.72754\tvalidation_1-ndcg:0.80281\n",
      "[99]\tvalidation_0-logloss:97.14306\tvalidation_0-ndcg:0.98249\tvalidation_1-logloss:124.81860\tvalidation_1-ndcg:0.80225\n",
      "[100]\tvalidation_0-logloss:96.99041\tvalidation_0-ndcg:0.98253\tvalidation_1-logloss:124.96562\tvalidation_1-ndcg:0.80221\n",
      "[101]\tvalidation_0-logloss:96.84240\tvalidation_0-ndcg:0.98257\tvalidation_1-logloss:125.03550\tvalidation_1-ndcg:0.80158\n",
      "[102]\tvalidation_0-logloss:96.83445\tvalidation_0-ndcg:0.98261\tvalidation_1-logloss:125.42236\tvalidation_1-ndcg:0.80389\n",
      "[103]\tvalidation_0-logloss:96.61468\tvalidation_0-ndcg:0.98272\tvalidation_1-logloss:125.04382\tvalidation_1-ndcg:0.80327\n",
      "[104]\tvalidation_0-logloss:96.71385\tvalidation_0-ndcg:0.98273\tvalidation_1-logloss:125.02485\tvalidation_1-ndcg:0.80297\n",
      "[105]\tvalidation_0-logloss:96.63017\tvalidation_0-ndcg:0.98321\tvalidation_1-logloss:124.71108\tvalidation_1-ndcg:0.80030\n",
      "[106]\tvalidation_0-logloss:96.40347\tvalidation_0-ndcg:0.98326\tvalidation_1-logloss:125.35326\tvalidation_1-ndcg:0.79901\n",
      "[107]\tvalidation_0-logloss:96.22043\tvalidation_0-ndcg:0.98324\tvalidation_1-logloss:125.22000\tvalidation_1-ndcg:0.80066\n",
      "[108]\tvalidation_0-logloss:96.21209\tvalidation_0-ndcg:0.98333\tvalidation_1-logloss:125.15381\tvalidation_1-ndcg:0.79915\n",
      "[109]\tvalidation_0-logloss:96.01347\tvalidation_0-ndcg:0.98336\tvalidation_1-logloss:124.98892\tvalidation_1-ndcg:0.79975\n",
      "[110]\tvalidation_0-logloss:95.94581\tvalidation_0-ndcg:0.98355\tvalidation_1-logloss:124.66187\tvalidation_1-ndcg:0.80043\n",
      "[111]\tvalidation_0-logloss:96.13918\tvalidation_0-ndcg:0.98378\tvalidation_1-logloss:124.73163\tvalidation_1-ndcg:0.79934\n",
      "[112]\tvalidation_0-logloss:95.89866\tvalidation_0-ndcg:0.98376\tvalidation_1-logloss:124.96745\tvalidation_1-ndcg:0.79813\n",
      "[113]\tvalidation_0-logloss:95.89907\tvalidation_0-ndcg:0.98395\tvalidation_1-logloss:124.36201\tvalidation_1-ndcg:0.79796\n",
      "[114]\tvalidation_0-logloss:95.82897\tvalidation_0-ndcg:0.98430\tvalidation_1-logloss:124.52771\tvalidation_1-ndcg:0.79800\n",
      "[115]\tvalidation_0-logloss:95.79280\tvalidation_0-ndcg:0.98437\tvalidation_1-logloss:124.29397\tvalidation_1-ndcg:0.79797\n",
      "[116]\tvalidation_0-logloss:95.72494\tvalidation_0-ndcg:0.98439\tvalidation_1-logloss:124.55699\tvalidation_1-ndcg:0.79900\n",
      "[117]\tvalidation_0-logloss:95.70957\tvalidation_0-ndcg:0.98466\tvalidation_1-logloss:124.20397\tvalidation_1-ndcg:0.79922\n",
      "[118]\tvalidation_0-logloss:95.61851\tvalidation_0-ndcg:0.98498\tvalidation_1-logloss:124.42206\tvalidation_1-ndcg:0.80063\n",
      "[119]\tvalidation_0-logloss:95.48708\tvalidation_0-ndcg:0.98503\tvalidation_1-logloss:124.70372\tvalidation_1-ndcg:0.79989\n",
      "[120]\tvalidation_0-logloss:95.54949\tvalidation_0-ndcg:0.98506\tvalidation_1-logloss:124.60455\tvalidation_1-ndcg:0.80114\n",
      "[121]\tvalidation_0-logloss:95.53537\tvalidation_0-ndcg:0.98512\tvalidation_1-logloss:124.35373\tvalidation_1-ndcg:0.80206\n",
      "[122]\tvalidation_0-logloss:95.34664\tvalidation_0-ndcg:0.98513\tvalidation_1-logloss:124.78067\tvalidation_1-ndcg:0.80136\n",
      "[123]\tvalidation_0-logloss:95.25911\tvalidation_0-ndcg:0.98514\tvalidation_1-logloss:124.11265\tvalidation_1-ndcg:0.79944\n",
      "[124]\tvalidation_0-logloss:95.20757\tvalidation_0-ndcg:0.98519\tvalidation_1-logloss:124.44358\tvalidation_1-ndcg:0.80025\n",
      "[125]\tvalidation_0-logloss:95.30820\tvalidation_0-ndcg:0.98522\tvalidation_1-logloss:124.51375\tvalidation_1-ndcg:0.80021\n",
      "[126]\tvalidation_0-logloss:95.29367\tvalidation_0-ndcg:0.98526\tvalidation_1-logloss:124.74528\tvalidation_1-ndcg:0.80095\n",
      "[127]\tvalidation_0-logloss:95.40582\tvalidation_0-ndcg:0.98543\tvalidation_1-logloss:124.71565\tvalidation_1-ndcg:0.80039\n",
      "[128]\tvalidation_0-logloss:94.95246\tvalidation_0-ndcg:0.98562\tvalidation_1-logloss:124.67269\tvalidation_1-ndcg:0.79840\n",
      "[129]\tvalidation_0-logloss:94.80643\tvalidation_0-ndcg:0.98561\tvalidation_1-logloss:124.91010\tvalidation_1-ndcg:0.79766\n",
      "[130]\tvalidation_0-logloss:95.05700\tvalidation_0-ndcg:0.98566\tvalidation_1-logloss:125.64809\tvalidation_1-ndcg:0.79816\n",
      "[131]\tvalidation_0-logloss:95.17065\tvalidation_0-ndcg:0.98567\tvalidation_1-logloss:125.52589\tvalidation_1-ndcg:0.79893\n",
      "[132]\tvalidation_0-logloss:94.85931\tvalidation_0-ndcg:0.98568\tvalidation_1-logloss:125.17753\tvalidation_1-ndcg:0.79703\n",
      "[133]\tvalidation_0-logloss:94.74853\tvalidation_0-ndcg:0.98588\tvalidation_1-logloss:124.92414\tvalidation_1-ndcg:0.79877\n",
      "[134]\tvalidation_0-logloss:94.51786\tvalidation_0-ndcg:0.98586\tvalidation_1-logloss:124.89640\tvalidation_1-ndcg:0.79879\n",
      "[135]\tvalidation_0-logloss:94.77194\tvalidation_0-ndcg:0.98588\tvalidation_1-logloss:125.40591\tvalidation_1-ndcg:0.80043\n",
      "[136]\tvalidation_0-logloss:94.67990\tvalidation_0-ndcg:0.98590\tvalidation_1-logloss:124.94734\tvalidation_1-ndcg:0.80033\n",
      "[137]\tvalidation_0-logloss:94.62152\tvalidation_0-ndcg:0.98590\tvalidation_1-logloss:124.59098\tvalidation_1-ndcg:0.79999\n",
      "[138]\tvalidation_0-logloss:94.54695\tvalidation_0-ndcg:0.98591\tvalidation_1-logloss:124.48058\tvalidation_1-ndcg:0.80255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139]\tvalidation_0-logloss:94.74530\tvalidation_0-ndcg:0.98594\tvalidation_1-logloss:124.47181\tvalidation_1-ndcg:0.80108\n",
      "[140]\tvalidation_0-logloss:94.87491\tvalidation_0-ndcg:0.98596\tvalidation_1-logloss:124.63685\tvalidation_1-ndcg:0.80084\n",
      "[141]\tvalidation_0-logloss:94.66831\tvalidation_0-ndcg:0.98599\tvalidation_1-logloss:124.68066\tvalidation_1-ndcg:0.80071\n",
      "[142]\tvalidation_0-logloss:94.86355\tvalidation_0-ndcg:0.98600\tvalidation_1-logloss:124.35254\tvalidation_1-ndcg:0.80243\n",
      "[143]\tvalidation_0-logloss:94.55836\tvalidation_0-ndcg:0.98623\tvalidation_1-logloss:124.30186\tvalidation_1-ndcg:0.80151\n",
      "[144]\tvalidation_0-logloss:94.62505\tvalidation_0-ndcg:0.98622\tvalidation_1-logloss:124.31895\tvalidation_1-ndcg:0.80124\n",
      "[145]\tvalidation_0-logloss:94.65111\tvalidation_0-ndcg:0.98621\tvalidation_1-logloss:124.55183\tvalidation_1-ndcg:0.80091\n",
      "[146]\tvalidation_0-logloss:94.32419\tvalidation_0-ndcg:0.98623\tvalidation_1-logloss:124.84619\tvalidation_1-ndcg:0.80098\n",
      "[147]\tvalidation_0-logloss:94.40833\tvalidation_0-ndcg:0.98625\tvalidation_1-logloss:125.29387\tvalidation_1-ndcg:0.80079\n",
      "[148]\tvalidation_0-logloss:94.62750\tvalidation_0-ndcg:0.98629\tvalidation_1-logloss:125.11891\tvalidation_1-ndcg:0.80108\n",
      "[149]\tvalidation_0-logloss:94.33648\tvalidation_0-ndcg:0.98631\tvalidation_1-logloss:124.54318\tvalidation_1-ndcg:0.80045\n",
      "[150]\tvalidation_0-logloss:94.47548\tvalidation_0-ndcg:0.98637\tvalidation_1-logloss:124.57202\tvalidation_1-ndcg:0.80227\n",
      "[151]\tvalidation_0-logloss:94.36565\tvalidation_0-ndcg:0.98639\tvalidation_1-logloss:124.53510\tvalidation_1-ndcg:0.80244\n",
      "[152]\tvalidation_0-logloss:94.50896\tvalidation_0-ndcg:0.98639\tvalidation_1-logloss:124.63896\tvalidation_1-ndcg:0.80143\n",
      "[153]\tvalidation_0-logloss:94.47447\tvalidation_0-ndcg:0.98644\tvalidation_1-logloss:124.62313\tvalidation_1-ndcg:0.80160\n",
      "[154]\tvalidation_0-logloss:94.33549\tvalidation_0-ndcg:0.98648\tvalidation_1-logloss:125.19705\tvalidation_1-ndcg:0.80177\n",
      "[155]\tvalidation_0-logloss:94.28640\tvalidation_0-ndcg:0.98648\tvalidation_1-logloss:124.99461\tvalidation_1-ndcg:0.80120\n",
      "[156]\tvalidation_0-logloss:94.35054\tvalidation_0-ndcg:0.98649\tvalidation_1-logloss:125.00900\tvalidation_1-ndcg:0.80061\n",
      "[157]\tvalidation_0-logloss:94.19222\tvalidation_0-ndcg:0.98652\tvalidation_1-logloss:125.29614\tvalidation_1-ndcg:0.80085\n",
      "[158]\tvalidation_0-logloss:94.29015\tvalidation_0-ndcg:0.98653\tvalidation_1-logloss:125.71592\tvalidation_1-ndcg:0.80130\n",
      "[159]\tvalidation_0-logloss:94.40292\tvalidation_0-ndcg:0.98659\tvalidation_1-logloss:125.34064\tvalidation_1-ndcg:0.80148\n",
      "[160]\tvalidation_0-logloss:94.23572\tvalidation_0-ndcg:0.98661\tvalidation_1-logloss:124.74536\tvalidation_1-ndcg:0.80189\n",
      "[161]\tvalidation_0-logloss:94.22881\tvalidation_0-ndcg:0.98679\tvalidation_1-logloss:125.17816\tvalidation_1-ndcg:0.80247\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(alpha=0.006056282064610986, base_score=None, booster=&#x27;gbtree&#x27;,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;ndcg&#x27;],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy=&#x27;depth...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method=&#x27;topk&#x27;, learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(alpha=0.006056282064610986, base_score=None, booster=&#x27;gbtree&#x27;,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;ndcg&#x27;],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy=&#x27;depth...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method=&#x27;topk&#x27;, learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(alpha=0.006056282064610986, base_score=None, booster='gbtree',\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=['logloss', 'ndcg'],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy='depth...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method='topk', learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 7).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "# Split the data into input features (X) and target labels (y)\n",
    "X_train = train_data[columns_to_normalize]\n",
    "y_train = train_data['rank']\n",
    "\n",
    "X_test = test_data[columns_to_normalize]\n",
    "y_test = test_data['rank']\n",
    "\n",
    "train_groups = train_data.groupby('keyword').size().to_numpy()\n",
    "test_groups = test_data.groupby('keyword').size().to_numpy()\n",
    "\n",
    "ranker = xgb.XGBRanker(tree_method='hist',\n",
    "                        booster='gbtree',\n",
    "                        objective='rank:ndcg',\n",
    "                        eval_metric=['logloss', 'ndcg'],\n",
    "                        random_state=42,  \n",
    "                        learning_rate= 0.06338827428733157, \n",
    "                        n_estimators= 200, \n",
    "                        max_depth= 78, \n",
    "                        reg_lambda= 3.86173348994611e-07, \n",
    "                        lambdarank_num_pair_per_sample= 8, \n",
    "                        lambdarank_pair_method= 'topk', \n",
    "                        early_stopping_rounds= 97, \n",
    "                        alpha= 0.006056282064610986, \n",
    "                        gamma= 0.02093151208361956, \n",
    "                        grow_policy= 'depthwise', \n",
    "                        max_leaves= 151, \n",
    "                        min_child_weight= 9, \n",
    "                        colsample_bytree= 0.9120013237152653, \n",
    "                        colsample_bylevel= 0.9242331949841277, \n",
    "                        colsample_bynode= 0.5855540278431589, \n",
    "                        subsample= 0.8982514390329758)\n",
    "\n",
    "ranker.fit(X_train, y_train, group=train_groups, verbose=True, eval_set=[(X_train, y_train),(X_test, y_test)], eval_group=[train_groups, test_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "06929693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9405855388429982, 0.959490903156628, 1.0, 0.7439713644516851)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores4a =[]\n",
    "for query, group in test_data.groupby('keyword'):\n",
    "    if len(group['rank'].tolist()) != 1:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "        scores4a.append(nd(np.asarray([ranks]), np.asarray([ranker.predict(features).tolist()])))\n",
    "np.mean(scores4a), np.median(scores4a), max(scores4a), min(scores4a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba4e35",
   "metadata": {},
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b4a3dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:64.99075\tvalidation_0-ndcg:0.77313\tvalidation_1-logloss:66.89530\tvalidation_1-ndcg:0.75815\n",
      "[1]\tvalidation_0-logloss:81.20247\tvalidation_0-ndcg:0.85035\tvalidation_1-logloss:87.89483\tvalidation_1-ndcg:0.77615\n",
      "[2]\tvalidation_0-logloss:90.18863\tvalidation_0-ndcg:0.88011\tvalidation_1-logloss:95.19061\tvalidation_1-ndcg:0.76443\n",
      "[3]\tvalidation_0-logloss:97.86184\tvalidation_0-ndcg:0.89756\tvalidation_1-logloss:102.89025\tvalidation_1-ndcg:0.77833\n",
      "[4]\tvalidation_0-logloss:101.27685\tvalidation_0-ndcg:0.91257\tvalidation_1-logloss:105.88994\tvalidation_1-ndcg:0.77917\n",
      "[5]\tvalidation_0-logloss:103.24041\tvalidation_0-ndcg:0.92067\tvalidation_1-logloss:108.51664\tvalidation_1-ndcg:0.78126\n",
      "[6]\tvalidation_0-logloss:105.97224\tvalidation_0-ndcg:0.92715\tvalidation_1-logloss:111.11616\tvalidation_1-ndcg:0.78359\n",
      "[7]\tvalidation_0-logloss:107.05446\tvalidation_0-ndcg:0.93424\tvalidation_1-logloss:113.56579\tvalidation_1-ndcg:0.78887\n",
      "[8]\tvalidation_0-logloss:108.83269\tvalidation_0-ndcg:0.93841\tvalidation_1-logloss:116.91809\tvalidation_1-ndcg:0.79260\n",
      "[9]\tvalidation_0-logloss:110.21274\tvalidation_0-ndcg:0.94221\tvalidation_1-logloss:117.48614\tvalidation_1-ndcg:0.79460\n",
      "[10]\tvalidation_0-logloss:111.22174\tvalidation_0-ndcg:0.94559\tvalidation_1-logloss:116.60748\tvalidation_1-ndcg:0.79165\n",
      "[11]\tvalidation_0-logloss:112.58255\tvalidation_0-ndcg:0.94697\tvalidation_1-logloss:116.90516\tvalidation_1-ndcg:0.80013\n",
      "[12]\tvalidation_0-logloss:113.27018\tvalidation_0-ndcg:0.94785\tvalidation_1-logloss:119.35211\tvalidation_1-ndcg:0.79748\n",
      "[13]\tvalidation_0-logloss:113.98578\tvalidation_0-ndcg:0.95017\tvalidation_1-logloss:118.94110\tvalidation_1-ndcg:0.79541\n",
      "[14]\tvalidation_0-logloss:115.50409\tvalidation_0-ndcg:0.95400\tvalidation_1-logloss:120.28539\tvalidation_1-ndcg:0.80292\n",
      "[15]\tvalidation_0-logloss:115.67521\tvalidation_0-ndcg:0.95584\tvalidation_1-logloss:120.42027\tvalidation_1-ndcg:0.79941\n",
      "[16]\tvalidation_0-logloss:115.96142\tvalidation_0-ndcg:0.95738\tvalidation_1-logloss:120.09365\tvalidation_1-ndcg:0.79810\n",
      "[17]\tvalidation_0-logloss:116.16801\tvalidation_0-ndcg:0.95932\tvalidation_1-logloss:120.17403\tvalidation_1-ndcg:0.79992\n",
      "[18]\tvalidation_0-logloss:116.70904\tvalidation_0-ndcg:0.95952\tvalidation_1-logloss:120.92124\tvalidation_1-ndcg:0.80147\n",
      "[19]\tvalidation_0-logloss:116.61104\tvalidation_0-ndcg:0.96119\tvalidation_1-logloss:121.19624\tvalidation_1-ndcg:0.79782\n",
      "[20]\tvalidation_0-logloss:117.14385\tvalidation_0-ndcg:0.96158\tvalidation_1-logloss:121.59912\tvalidation_1-ndcg:0.79665\n",
      "[21]\tvalidation_0-logloss:117.08232\tvalidation_0-ndcg:0.96285\tvalidation_1-logloss:122.24725\tvalidation_1-ndcg:0.79337\n",
      "[22]\tvalidation_0-logloss:117.30531\tvalidation_0-ndcg:0.96445\tvalidation_1-logloss:123.57119\tvalidation_1-ndcg:0.79676\n",
      "[23]\tvalidation_0-logloss:117.75837\tvalidation_0-ndcg:0.96547\tvalidation_1-logloss:123.59701\tvalidation_1-ndcg:0.79391\n",
      "[24]\tvalidation_0-logloss:118.03662\tvalidation_0-ndcg:0.96640\tvalidation_1-logloss:124.20409\tvalidation_1-ndcg:0.79705\n",
      "[25]\tvalidation_0-logloss:117.86705\tvalidation_0-ndcg:0.96701\tvalidation_1-logloss:123.65572\tvalidation_1-ndcg:0.79772\n",
      "[26]\tvalidation_0-logloss:118.03457\tvalidation_0-ndcg:0.96797\tvalidation_1-logloss:123.81161\tvalidation_1-ndcg:0.79785\n",
      "[27]\tvalidation_0-logloss:117.77133\tvalidation_0-ndcg:0.96879\tvalidation_1-logloss:123.93099\tvalidation_1-ndcg:0.79648\n",
      "[28]\tvalidation_0-logloss:118.12341\tvalidation_0-ndcg:0.96893\tvalidation_1-logloss:124.83623\tvalidation_1-ndcg:0.79556\n",
      "[29]\tvalidation_0-logloss:117.90144\tvalidation_0-ndcg:0.96911\tvalidation_1-logloss:124.58230\tvalidation_1-ndcg:0.79879\n",
      "[30]\tvalidation_0-logloss:117.81575\tvalidation_0-ndcg:0.96949\tvalidation_1-logloss:124.95754\tvalidation_1-ndcg:0.79606\n",
      "[31]\tvalidation_0-logloss:117.79300\tvalidation_0-ndcg:0.97015\tvalidation_1-logloss:125.03691\tvalidation_1-ndcg:0.80002\n",
      "[32]\tvalidation_0-logloss:117.42760\tvalidation_0-ndcg:0.97021\tvalidation_1-logloss:125.29182\tvalidation_1-ndcg:0.80031\n",
      "[33]\tvalidation_0-logloss:117.68021\tvalidation_0-ndcg:0.97043\tvalidation_1-logloss:124.77841\tvalidation_1-ndcg:0.79886\n",
      "[34]\tvalidation_0-logloss:116.73227\tvalidation_0-ndcg:0.97117\tvalidation_1-logloss:125.20425\tvalidation_1-ndcg:0.79811\n",
      "[35]\tvalidation_0-logloss:116.51764\tvalidation_0-ndcg:0.97152\tvalidation_1-logloss:123.82075\tvalidation_1-ndcg:0.79964\n",
      "[36]\tvalidation_0-logloss:116.49689\tvalidation_0-ndcg:0.97190\tvalidation_1-logloss:124.23595\tvalidation_1-ndcg:0.80048\n",
      "[37]\tvalidation_0-logloss:115.90909\tvalidation_0-ndcg:0.97221\tvalidation_1-logloss:124.75399\tvalidation_1-ndcg:0.80169\n",
      "[38]\tvalidation_0-logloss:115.59158\tvalidation_0-ndcg:0.97240\tvalidation_1-logloss:124.12673\tvalidation_1-ndcg:0.79988\n",
      "[39]\tvalidation_0-logloss:114.80359\tvalidation_0-ndcg:0.97243\tvalidation_1-logloss:124.64190\tvalidation_1-ndcg:0.79970\n",
      "[40]\tvalidation_0-logloss:114.02835\tvalidation_0-ndcg:0.97288\tvalidation_1-logloss:125.03015\tvalidation_1-ndcg:0.80012\n",
      "[41]\tvalidation_0-logloss:113.92283\tvalidation_0-ndcg:0.97328\tvalidation_1-logloss:125.72643\tvalidation_1-ndcg:0.80101\n",
      "[42]\tvalidation_0-logloss:113.17607\tvalidation_0-ndcg:0.97347\tvalidation_1-logloss:124.97804\tvalidation_1-ndcg:0.80124\n",
      "[43]\tvalidation_0-logloss:112.99781\tvalidation_0-ndcg:0.97347\tvalidation_1-logloss:125.75523\tvalidation_1-ndcg:0.80115\n",
      "[44]\tvalidation_0-logloss:112.06815\tvalidation_0-ndcg:0.97365\tvalidation_1-logloss:125.67342\tvalidation_1-ndcg:0.80172\n",
      "[45]\tvalidation_0-logloss:111.91759\tvalidation_0-ndcg:0.97380\tvalidation_1-logloss:125.88742\tvalidation_1-ndcg:0.80155\n",
      "[46]\tvalidation_0-logloss:111.70229\tvalidation_0-ndcg:0.97381\tvalidation_1-logloss:125.93937\tvalidation_1-ndcg:0.79940\n",
      "[47]\tvalidation_0-logloss:111.04912\tvalidation_0-ndcg:0.97418\tvalidation_1-logloss:125.60065\tvalidation_1-ndcg:0.80125\n",
      "[48]\tvalidation_0-logloss:110.58197\tvalidation_0-ndcg:0.97437\tvalidation_1-logloss:126.03412\tvalidation_1-ndcg:0.80163\n",
      "[49]\tvalidation_0-logloss:109.42652\tvalidation_0-ndcg:0.97435\tvalidation_1-logloss:125.80928\tvalidation_1-ndcg:0.80056\n",
      "[50]\tvalidation_0-logloss:108.52691\tvalidation_0-ndcg:0.97486\tvalidation_1-logloss:125.52251\tvalidation_1-ndcg:0.80071\n",
      "[51]\tvalidation_0-logloss:108.71660\tvalidation_0-ndcg:0.97500\tvalidation_1-logloss:125.22182\tvalidation_1-ndcg:0.80035\n",
      "[52]\tvalidation_0-logloss:108.58761\tvalidation_0-ndcg:0.97507\tvalidation_1-logloss:126.14244\tvalidation_1-ndcg:0.80072\n",
      "[53]\tvalidation_0-logloss:108.02431\tvalidation_0-ndcg:0.97516\tvalidation_1-logloss:125.06519\tvalidation_1-ndcg:0.80051\n",
      "[54]\tvalidation_0-logloss:107.24765\tvalidation_0-ndcg:0.97540\tvalidation_1-logloss:125.36446\tvalidation_1-ndcg:0.80156\n",
      "[55]\tvalidation_0-logloss:106.86971\tvalidation_0-ndcg:0.97563\tvalidation_1-logloss:125.87434\tvalidation_1-ndcg:0.80099\n",
      "[56]\tvalidation_0-logloss:106.39227\tvalidation_0-ndcg:0.97572\tvalidation_1-logloss:125.77574\tvalidation_1-ndcg:0.80323\n",
      "[57]\tvalidation_0-logloss:106.20990\tvalidation_0-ndcg:0.97603\tvalidation_1-logloss:125.44608\tvalidation_1-ndcg:0.80458\n",
      "[58]\tvalidation_0-logloss:105.53315\tvalidation_0-ndcg:0.97590\tvalidation_1-logloss:124.73160\tvalidation_1-ndcg:0.80508\n",
      "[59]\tvalidation_0-logloss:104.93493\tvalidation_0-ndcg:0.97632\tvalidation_1-logloss:124.36090\tvalidation_1-ndcg:0.80470\n",
      "[60]\tvalidation_0-logloss:104.78780\tvalidation_0-ndcg:0.97647\tvalidation_1-logloss:124.29477\tvalidation_1-ndcg:0.80480\n",
      "[61]\tvalidation_0-logloss:104.17797\tvalidation_0-ndcg:0.97651\tvalidation_1-logloss:124.35574\tvalidation_1-ndcg:0.80457\n",
      "[62]\tvalidation_0-logloss:103.50210\tvalidation_0-ndcg:0.97676\tvalidation_1-logloss:124.35723\tvalidation_1-ndcg:0.80297\n",
      "[63]\tvalidation_0-logloss:102.86965\tvalidation_0-ndcg:0.97706\tvalidation_1-logloss:124.13300\tvalidation_1-ndcg:0.80390\n",
      "[64]\tvalidation_0-logloss:102.54256\tvalidation_0-ndcg:0.97706\tvalidation_1-logloss:124.77711\tvalidation_1-ndcg:0.80470\n",
      "[65]\tvalidation_0-logloss:102.40966\tvalidation_0-ndcg:0.97729\tvalidation_1-logloss:125.11648\tvalidation_1-ndcg:0.80477\n",
      "[66]\tvalidation_0-logloss:102.60829\tvalidation_0-ndcg:0.97734\tvalidation_1-logloss:125.55298\tvalidation_1-ndcg:0.80395\n",
      "[67]\tvalidation_0-logloss:101.91511\tvalidation_0-ndcg:0.97738\tvalidation_1-logloss:125.51840\tvalidation_1-ndcg:0.80514\n",
      "[68]\tvalidation_0-logloss:102.06716\tvalidation_0-ndcg:0.97749\tvalidation_1-logloss:125.01059\tvalidation_1-ndcg:0.80657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\tvalidation_0-logloss:102.06868\tvalidation_0-ndcg:0.97754\tvalidation_1-logloss:124.98138\tvalidation_1-ndcg:0.80574\n",
      "[70]\tvalidation_0-logloss:101.61750\tvalidation_0-ndcg:0.97765\tvalidation_1-logloss:124.10518\tvalidation_1-ndcg:0.80757\n",
      "[71]\tvalidation_0-logloss:101.32265\tvalidation_0-ndcg:0.97771\tvalidation_1-logloss:124.02455\tvalidation_1-ndcg:0.80734\n",
      "[72]\tvalidation_0-logloss:100.94815\tvalidation_0-ndcg:0.97773\tvalidation_1-logloss:123.59887\tvalidation_1-ndcg:0.80584\n",
      "[73]\tvalidation_0-logloss:100.82617\tvalidation_0-ndcg:0.97783\tvalidation_1-logloss:123.78909\tvalidation_1-ndcg:0.80627\n",
      "[74]\tvalidation_0-logloss:100.58246\tvalidation_0-ndcg:0.97794\tvalidation_1-logloss:123.13133\tvalidation_1-ndcg:0.80377\n",
      "[75]\tvalidation_0-logloss:100.53266\tvalidation_0-ndcg:0.97805\tvalidation_1-logloss:123.07682\tvalidation_1-ndcg:0.80352\n",
      "[76]\tvalidation_0-logloss:100.68023\tvalidation_0-ndcg:0.97839\tvalidation_1-logloss:122.74258\tvalidation_1-ndcg:0.80437\n",
      "[77]\tvalidation_0-logloss:100.59801\tvalidation_0-ndcg:0.97844\tvalidation_1-logloss:123.95619\tvalidation_1-ndcg:0.80443\n",
      "[78]\tvalidation_0-logloss:100.61163\tvalidation_0-ndcg:0.97859\tvalidation_1-logloss:123.98480\tvalidation_1-ndcg:0.80688\n",
      "[79]\tvalidation_0-logloss:99.89945\tvalidation_0-ndcg:0.97892\tvalidation_1-logloss:123.48970\tvalidation_1-ndcg:0.80607\n",
      "[80]\tvalidation_0-logloss:100.09978\tvalidation_0-ndcg:0.97887\tvalidation_1-logloss:123.91722\tvalidation_1-ndcg:0.80544\n",
      "[81]\tvalidation_0-logloss:99.58429\tvalidation_0-ndcg:0.97913\tvalidation_1-logloss:123.33025\tvalidation_1-ndcg:0.80668\n",
      "[82]\tvalidation_0-logloss:99.25270\tvalidation_0-ndcg:0.97940\tvalidation_1-logloss:123.85890\tvalidation_1-ndcg:0.80841\n",
      "[83]\tvalidation_0-logloss:99.63568\tvalidation_0-ndcg:0.97946\tvalidation_1-logloss:123.27665\tvalidation_1-ndcg:0.80878\n",
      "[84]\tvalidation_0-logloss:99.44868\tvalidation_0-ndcg:0.97952\tvalidation_1-logloss:123.49801\tvalidation_1-ndcg:0.80867\n",
      "[85]\tvalidation_0-logloss:99.38135\tvalidation_0-ndcg:0.97992\tvalidation_1-logloss:122.90876\tvalidation_1-ndcg:0.80872\n",
      "[86]\tvalidation_0-logloss:99.14923\tvalidation_0-ndcg:0.98013\tvalidation_1-logloss:122.74071\tvalidation_1-ndcg:0.80930\n",
      "[87]\tvalidation_0-logloss:99.47762\tvalidation_0-ndcg:0.98034\tvalidation_1-logloss:122.87045\tvalidation_1-ndcg:0.80768\n",
      "[88]\tvalidation_0-logloss:99.20777\tvalidation_0-ndcg:0.98042\tvalidation_1-logloss:123.90123\tvalidation_1-ndcg:0.80965\n",
      "[89]\tvalidation_0-logloss:98.89133\tvalidation_0-ndcg:0.98042\tvalidation_1-logloss:122.83461\tvalidation_1-ndcg:0.80816\n",
      "[90]\tvalidation_0-logloss:98.94364\tvalidation_0-ndcg:0.98052\tvalidation_1-logloss:123.20832\tvalidation_1-ndcg:0.80899\n",
      "[91]\tvalidation_0-logloss:98.96614\tvalidation_0-ndcg:0.98055\tvalidation_1-logloss:123.55131\tvalidation_1-ndcg:0.81057\n",
      "[92]\tvalidation_0-logloss:98.79315\tvalidation_0-ndcg:0.98054\tvalidation_1-logloss:123.85118\tvalidation_1-ndcg:0.80996\n",
      "[93]\tvalidation_0-logloss:98.40556\tvalidation_0-ndcg:0.98064\tvalidation_1-logloss:123.52767\tvalidation_1-ndcg:0.81045\n",
      "[94]\tvalidation_0-logloss:98.24028\tvalidation_0-ndcg:0.98066\tvalidation_1-logloss:123.37460\tvalidation_1-ndcg:0.81053\n",
      "[95]\tvalidation_0-logloss:98.19762\tvalidation_0-ndcg:0.98067\tvalidation_1-logloss:123.09943\tvalidation_1-ndcg:0.81126\n",
      "[96]\tvalidation_0-logloss:97.65285\tvalidation_0-ndcg:0.98086\tvalidation_1-logloss:123.41782\tvalidation_1-ndcg:0.81185\n",
      "[97]\tvalidation_0-logloss:97.66856\tvalidation_0-ndcg:0.98110\tvalidation_1-logloss:123.60055\tvalidation_1-ndcg:0.81203\n",
      "[98]\tvalidation_0-logloss:97.49807\tvalidation_0-ndcg:0.98109\tvalidation_1-logloss:123.91739\tvalidation_1-ndcg:0.81376\n",
      "[99]\tvalidation_0-logloss:97.24918\tvalidation_0-ndcg:0.98122\tvalidation_1-logloss:123.95946\tvalidation_1-ndcg:0.81331\n",
      "[100]\tvalidation_0-logloss:97.19418\tvalidation_0-ndcg:0.98142\tvalidation_1-logloss:123.80317\tvalidation_1-ndcg:0.81329\n",
      "[101]\tvalidation_0-logloss:97.08488\tvalidation_0-ndcg:0.98159\tvalidation_1-logloss:124.24269\tvalidation_1-ndcg:0.81265\n",
      "[102]\tvalidation_0-logloss:97.16058\tvalidation_0-ndcg:0.98165\tvalidation_1-logloss:123.88957\tvalidation_1-ndcg:0.81262\n",
      "[103]\tvalidation_0-logloss:97.24465\tvalidation_0-ndcg:0.98167\tvalidation_1-logloss:123.52765\tvalidation_1-ndcg:0.81307\n",
      "[104]\tvalidation_0-logloss:97.37859\tvalidation_0-ndcg:0.98171\tvalidation_1-logloss:124.02129\tvalidation_1-ndcg:0.81265\n",
      "[105]\tvalidation_0-logloss:97.16934\tvalidation_0-ndcg:0.98172\tvalidation_1-logloss:124.20036\tvalidation_1-ndcg:0.81276\n",
      "[106]\tvalidation_0-logloss:97.07378\tvalidation_0-ndcg:0.98174\tvalidation_1-logloss:123.73224\tvalidation_1-ndcg:0.81344\n",
      "[107]\tvalidation_0-logloss:96.77584\tvalidation_0-ndcg:0.98179\tvalidation_1-logloss:123.80001\tvalidation_1-ndcg:0.81322\n",
      "[108]\tvalidation_0-logloss:97.17028\tvalidation_0-ndcg:0.98184\tvalidation_1-logloss:123.70184\tvalidation_1-ndcg:0.81498\n",
      "[109]\tvalidation_0-logloss:97.05152\tvalidation_0-ndcg:0.98184\tvalidation_1-logloss:123.32348\tvalidation_1-ndcg:0.81469\n",
      "[110]\tvalidation_0-logloss:97.04840\tvalidation_0-ndcg:0.98186\tvalidation_1-logloss:123.70750\tvalidation_1-ndcg:0.81402\n",
      "[111]\tvalidation_0-logloss:97.08374\tvalidation_0-ndcg:0.98187\tvalidation_1-logloss:123.76513\tvalidation_1-ndcg:0.81268\n",
      "[112]\tvalidation_0-logloss:97.11906\tvalidation_0-ndcg:0.98188\tvalidation_1-logloss:123.70192\tvalidation_1-ndcg:0.81289\n",
      "[113]\tvalidation_0-logloss:97.00896\tvalidation_0-ndcg:0.98196\tvalidation_1-logloss:123.63570\tvalidation_1-ndcg:0.81243\n",
      "[114]\tvalidation_0-logloss:96.73483\tvalidation_0-ndcg:0.98197\tvalidation_1-logloss:123.70298\tvalidation_1-ndcg:0.81274\n",
      "[115]\tvalidation_0-logloss:96.67141\tvalidation_0-ndcg:0.98200\tvalidation_1-logloss:123.96575\tvalidation_1-ndcg:0.81370\n",
      "[116]\tvalidation_0-logloss:96.74769\tvalidation_0-ndcg:0.98207\tvalidation_1-logloss:124.10770\tvalidation_1-ndcg:0.81239\n",
      "[117]\tvalidation_0-logloss:96.43295\tvalidation_0-ndcg:0.98210\tvalidation_1-logloss:123.80055\tvalidation_1-ndcg:0.81277\n",
      "[118]\tvalidation_0-logloss:96.23670\tvalidation_0-ndcg:0.98218\tvalidation_1-logloss:123.60961\tvalidation_1-ndcg:0.81232\n",
      "[119]\tvalidation_0-logloss:96.18087\tvalidation_0-ndcg:0.98223\tvalidation_1-logloss:123.43958\tvalidation_1-ndcg:0.81364\n",
      "[120]\tvalidation_0-logloss:96.39083\tvalidation_0-ndcg:0.98226\tvalidation_1-logloss:123.53979\tvalidation_1-ndcg:0.81548\n",
      "[121]\tvalidation_0-logloss:96.42459\tvalidation_0-ndcg:0.98229\tvalidation_1-logloss:123.07108\tvalidation_1-ndcg:0.81505\n",
      "[122]\tvalidation_0-logloss:96.33495\tvalidation_0-ndcg:0.98255\tvalidation_1-logloss:123.01028\tvalidation_1-ndcg:0.81661\n",
      "[123]\tvalidation_0-logloss:96.16650\tvalidation_0-ndcg:0.98275\tvalidation_1-logloss:123.28862\tvalidation_1-ndcg:0.81605\n",
      "[124]\tvalidation_0-logloss:96.34465\tvalidation_0-ndcg:0.98274\tvalidation_1-logloss:123.60160\tvalidation_1-ndcg:0.81640\n",
      "[125]\tvalidation_0-logloss:96.21338\tvalidation_0-ndcg:0.98274\tvalidation_1-logloss:124.26825\tvalidation_1-ndcg:0.81611\n",
      "[126]\tvalidation_0-logloss:96.29259\tvalidation_0-ndcg:0.98277\tvalidation_1-logloss:123.69876\tvalidation_1-ndcg:0.81496\n",
      "[127]\tvalidation_0-logloss:96.07349\tvalidation_0-ndcg:0.98294\tvalidation_1-logloss:123.68417\tvalidation_1-ndcg:0.81474\n",
      "[128]\tvalidation_0-logloss:96.09437\tvalidation_0-ndcg:0.98293\tvalidation_1-logloss:123.50074\tvalidation_1-ndcg:0.81410\n",
      "[129]\tvalidation_0-logloss:96.22466\tvalidation_0-ndcg:0.98305\tvalidation_1-logloss:124.03676\tvalidation_1-ndcg:0.81440\n",
      "[130]\tvalidation_0-logloss:96.17404\tvalidation_0-ndcg:0.98309\tvalidation_1-logloss:123.42568\tvalidation_1-ndcg:0.81405\n",
      "[131]\tvalidation_0-logloss:96.05759\tvalidation_0-ndcg:0.98309\tvalidation_1-logloss:123.48267\tvalidation_1-ndcg:0.81134\n",
      "[132]\tvalidation_0-logloss:95.95186\tvalidation_0-ndcg:0.98307\tvalidation_1-logloss:123.75175\tvalidation_1-ndcg:0.81111\n",
      "[133]\tvalidation_0-logloss:95.69962\tvalidation_0-ndcg:0.98309\tvalidation_1-logloss:123.20925\tvalidation_1-ndcg:0.81432\n",
      "[134]\tvalidation_0-logloss:95.46350\tvalidation_0-ndcg:0.98316\tvalidation_1-logloss:122.68000\tvalidation_1-ndcg:0.81215\n",
      "[135]\tvalidation_0-logloss:95.39723\tvalidation_0-ndcg:0.98318\tvalidation_1-logloss:122.93682\tvalidation_1-ndcg:0.80985\n",
      "[136]\tvalidation_0-logloss:95.44677\tvalidation_0-ndcg:0.98319\tvalidation_1-logloss:122.63772\tvalidation_1-ndcg:0.81085\n",
      "[137]\tvalidation_0-logloss:95.24380\tvalidation_0-ndcg:0.98325\tvalidation_1-logloss:122.60246\tvalidation_1-ndcg:0.81057\n",
      "[138]\tvalidation_0-logloss:95.48360\tvalidation_0-ndcg:0.98327\tvalidation_1-logloss:122.63640\tvalidation_1-ndcg:0.80965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139]\tvalidation_0-logloss:95.47574\tvalidation_0-ndcg:0.98335\tvalidation_1-logloss:122.34908\tvalidation_1-ndcg:0.81013\n",
      "[140]\tvalidation_0-logloss:95.47109\tvalidation_0-ndcg:0.98335\tvalidation_1-logloss:122.22948\tvalidation_1-ndcg:0.81071\n",
      "[141]\tvalidation_0-logloss:95.59912\tvalidation_0-ndcg:0.98338\tvalidation_1-logloss:122.28399\tvalidation_1-ndcg:0.81089\n",
      "[142]\tvalidation_0-logloss:95.48844\tvalidation_0-ndcg:0.98338\tvalidation_1-logloss:122.55051\tvalidation_1-ndcg:0.80902\n",
      "[143]\tvalidation_0-logloss:95.51320\tvalidation_0-ndcg:0.98343\tvalidation_1-logloss:122.85082\tvalidation_1-ndcg:0.81180\n",
      "[144]\tvalidation_0-logloss:95.22216\tvalidation_0-ndcg:0.98344\tvalidation_1-logloss:122.29906\tvalidation_1-ndcg:0.81136\n",
      "[145]\tvalidation_0-logloss:95.17658\tvalidation_0-ndcg:0.98347\tvalidation_1-logloss:122.65294\tvalidation_1-ndcg:0.81313\n",
      "[146]\tvalidation_0-logloss:95.16815\tvalidation_0-ndcg:0.98348\tvalidation_1-logloss:123.12373\tvalidation_1-ndcg:0.81442\n",
      "[147]\tvalidation_0-logloss:95.11607\tvalidation_0-ndcg:0.98349\tvalidation_1-logloss:123.04010\tvalidation_1-ndcg:0.81220\n",
      "[148]\tvalidation_0-logloss:95.13466\tvalidation_0-ndcg:0.98353\tvalidation_1-logloss:122.82639\tvalidation_1-ndcg:0.81021\n",
      "[149]\tvalidation_0-logloss:95.15301\tvalidation_0-ndcg:0.98355\tvalidation_1-logloss:122.74080\tvalidation_1-ndcg:0.81316\n",
      "[150]\tvalidation_0-logloss:95.15662\tvalidation_0-ndcg:0.98356\tvalidation_1-logloss:121.84779\tvalidation_1-ndcg:0.81325\n",
      "[151]\tvalidation_0-logloss:94.84917\tvalidation_0-ndcg:0.98380\tvalidation_1-logloss:122.16171\tvalidation_1-ndcg:0.81297\n",
      "[152]\tvalidation_0-logloss:94.90471\tvalidation_0-ndcg:0.98384\tvalidation_1-logloss:121.94879\tvalidation_1-ndcg:0.81473\n",
      "[153]\tvalidation_0-logloss:94.73474\tvalidation_0-ndcg:0.98385\tvalidation_1-logloss:121.90690\tvalidation_1-ndcg:0.81523\n",
      "[154]\tvalidation_0-logloss:94.54564\tvalidation_0-ndcg:0.98392\tvalidation_1-logloss:121.88277\tvalidation_1-ndcg:0.81514\n",
      "[155]\tvalidation_0-logloss:94.55218\tvalidation_0-ndcg:0.98409\tvalidation_1-logloss:121.73947\tvalidation_1-ndcg:0.81533\n",
      "[156]\tvalidation_0-logloss:94.62855\tvalidation_0-ndcg:0.98409\tvalidation_1-logloss:122.22680\tvalidation_1-ndcg:0.81424\n",
      "[157]\tvalidation_0-logloss:94.32968\tvalidation_0-ndcg:0.98412\tvalidation_1-logloss:121.36235\tvalidation_1-ndcg:0.81391\n",
      "[158]\tvalidation_0-logloss:94.62220\tvalidation_0-ndcg:0.98434\tvalidation_1-logloss:122.06620\tvalidation_1-ndcg:0.81481\n",
      "[159]\tvalidation_0-logloss:94.61450\tvalidation_0-ndcg:0.98443\tvalidation_1-logloss:122.82714\tvalidation_1-ndcg:0.81551\n",
      "[160]\tvalidation_0-logloss:94.44144\tvalidation_0-ndcg:0.98447\tvalidation_1-logloss:122.32973\tvalidation_1-ndcg:0.81643\n",
      "[161]\tvalidation_0-logloss:94.35283\tvalidation_0-ndcg:0.98448\tvalidation_1-logloss:122.61740\tvalidation_1-ndcg:0.81522\n",
      "[162]\tvalidation_0-logloss:94.38362\tvalidation_0-ndcg:0.98458\tvalidation_1-logloss:122.28122\tvalidation_1-ndcg:0.81290\n",
      "[163]\tvalidation_0-logloss:94.21435\tvalidation_0-ndcg:0.98459\tvalidation_1-logloss:122.47661\tvalidation_1-ndcg:0.81428\n",
      "[164]\tvalidation_0-logloss:94.24507\tvalidation_0-ndcg:0.98474\tvalidation_1-logloss:122.64492\tvalidation_1-ndcg:0.81518\n",
      "[165]\tvalidation_0-logloss:94.24503\tvalidation_0-ndcg:0.98477\tvalidation_1-logloss:122.65384\tvalidation_1-ndcg:0.81407\n",
      "[166]\tvalidation_0-logloss:94.26027\tvalidation_0-ndcg:0.98478\tvalidation_1-logloss:122.44244\tvalidation_1-ndcg:0.81386\n",
      "[167]\tvalidation_0-logloss:94.13475\tvalidation_0-ndcg:0.98484\tvalidation_1-logloss:122.31803\tvalidation_1-ndcg:0.81425\n",
      "[168]\tvalidation_0-logloss:94.02228\tvalidation_0-ndcg:0.98485\tvalidation_1-logloss:122.47523\tvalidation_1-ndcg:0.81274\n",
      "[169]\tvalidation_0-logloss:94.11595\tvalidation_0-ndcg:0.98514\tvalidation_1-logloss:122.26459\tvalidation_1-ndcg:0.81360\n",
      "[170]\tvalidation_0-logloss:94.03593\tvalidation_0-ndcg:0.98513\tvalidation_1-logloss:122.19034\tvalidation_1-ndcg:0.81257\n",
      "[171]\tvalidation_0-logloss:94.10619\tvalidation_0-ndcg:0.98513\tvalidation_1-logloss:122.59300\tvalidation_1-ndcg:0.81336\n",
      "[172]\tvalidation_0-logloss:94.10111\tvalidation_0-ndcg:0.98520\tvalidation_1-logloss:122.74241\tvalidation_1-ndcg:0.81322\n",
      "[173]\tvalidation_0-logloss:94.10502\tvalidation_0-ndcg:0.98529\tvalidation_1-logloss:122.33313\tvalidation_1-ndcg:0.81401\n",
      "[174]\tvalidation_0-logloss:94.15451\tvalidation_0-ndcg:0.98531\tvalidation_1-logloss:122.43531\tvalidation_1-ndcg:0.81435\n",
      "[175]\tvalidation_0-logloss:94.10005\tvalidation_0-ndcg:0.98531\tvalidation_1-logloss:122.56054\tvalidation_1-ndcg:0.81502\n",
      "[176]\tvalidation_0-logloss:93.95034\tvalidation_0-ndcg:0.98551\tvalidation_1-logloss:122.21788\tvalidation_1-ndcg:0.81313\n",
      "[177]\tvalidation_0-logloss:93.75851\tvalidation_0-ndcg:0.98550\tvalidation_1-logloss:122.40808\tvalidation_1-ndcg:0.81379\n",
      "[178]\tvalidation_0-logloss:93.65028\tvalidation_0-ndcg:0.98551\tvalidation_1-logloss:122.29426\tvalidation_1-ndcg:0.81397\n",
      "[179]\tvalidation_0-logloss:93.57044\tvalidation_0-ndcg:0.98553\tvalidation_1-logloss:122.46383\tvalidation_1-ndcg:0.81373\n",
      "[180]\tvalidation_0-logloss:93.69921\tvalidation_0-ndcg:0.98557\tvalidation_1-logloss:122.58602\tvalidation_1-ndcg:0.81287\n",
      "[181]\tvalidation_0-logloss:93.87547\tvalidation_0-ndcg:0.98560\tvalidation_1-logloss:122.33409\tvalidation_1-ndcg:0.81383\n",
      "[182]\tvalidation_0-logloss:93.67447\tvalidation_0-ndcg:0.98577\tvalidation_1-logloss:122.19605\tvalidation_1-ndcg:0.81406\n",
      "[183]\tvalidation_0-logloss:93.58038\tvalidation_0-ndcg:0.98586\tvalidation_1-logloss:122.70781\tvalidation_1-ndcg:0.81345\n",
      "[184]\tvalidation_0-logloss:93.53602\tvalidation_0-ndcg:0.98587\tvalidation_1-logloss:122.51832\tvalidation_1-ndcg:0.81481\n",
      "[185]\tvalidation_0-logloss:93.45618\tvalidation_0-ndcg:0.98607\tvalidation_1-logloss:122.22900\tvalidation_1-ndcg:0.81315\n",
      "[186]\tvalidation_0-logloss:93.54831\tvalidation_0-ndcg:0.98608\tvalidation_1-logloss:122.29370\tvalidation_1-ndcg:0.81279\n",
      "[187]\tvalidation_0-logloss:93.63464\tvalidation_0-ndcg:0.98612\tvalidation_1-logloss:122.40865\tvalidation_1-ndcg:0.81209\n",
      "[188]\tvalidation_0-logloss:93.71412\tvalidation_0-ndcg:0.98615\tvalidation_1-logloss:122.22486\tvalidation_1-ndcg:0.81251\n",
      "[189]\tvalidation_0-logloss:93.79693\tvalidation_0-ndcg:0.98619\tvalidation_1-logloss:122.36202\tvalidation_1-ndcg:0.81390\n",
      "[190]\tvalidation_0-logloss:93.76013\tvalidation_0-ndcg:0.98618\tvalidation_1-logloss:122.15230\tvalidation_1-ndcg:0.81417\n",
      "[191]\tvalidation_0-logloss:93.93680\tvalidation_0-ndcg:0.98621\tvalidation_1-logloss:122.40306\tvalidation_1-ndcg:0.81312\n",
      "[192]\tvalidation_0-logloss:93.92299\tvalidation_0-ndcg:0.98627\tvalidation_1-logloss:122.48935\tvalidation_1-ndcg:0.81228\n",
      "[193]\tvalidation_0-logloss:93.74198\tvalidation_0-ndcg:0.98627\tvalidation_1-logloss:122.30873\tvalidation_1-ndcg:0.81314\n",
      "[194]\tvalidation_0-logloss:93.68051\tvalidation_0-ndcg:0.98628\tvalidation_1-logloss:122.52424\tvalidation_1-ndcg:0.81443\n",
      "[195]\tvalidation_0-logloss:93.89835\tvalidation_0-ndcg:0.98628\tvalidation_1-logloss:123.05526\tvalidation_1-ndcg:0.81205\n",
      "[196]\tvalidation_0-logloss:93.93959\tvalidation_0-ndcg:0.98637\tvalidation_1-logloss:122.78374\tvalidation_1-ndcg:0.81178\n",
      "[197]\tvalidation_0-logloss:93.68489\tvalidation_0-ndcg:0.98639\tvalidation_1-logloss:123.09555\tvalidation_1-ndcg:0.81105\n",
      "[198]\tvalidation_0-logloss:93.45410\tvalidation_0-ndcg:0.98645\tvalidation_1-logloss:122.74024\tvalidation_1-ndcg:0.81285\n",
      "[199]\tvalidation_0-logloss:93.34790\tvalidation_0-ndcg:0.98648\tvalidation_1-logloss:123.14251\tvalidation_1-ndcg:0.81326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(alpha=0.006056282064610986, base_score=None, booster=None,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;ndcg&#x27;],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy=&#x27;depthwise...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method=&#x27;topk&#x27;, learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(alpha=0.006056282064610986, base_score=None, booster=None,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;ndcg&#x27;],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy=&#x27;depthwise...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method=&#x27;topk&#x27;, learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(alpha=0.006056282064610986, base_score=None, booster=None,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=['logloss', 'ndcg'],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy='depthwise...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method='topk', learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 1).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "# Split the data into input features (X) and target labels (y)\n",
    "X_train = train_data[columns_to_normalize]\n",
    "y_train = train_data['rank']\n",
    "\n",
    "X_test = test_data[columns_to_normalize]\n",
    "y_test = test_data['rank']\n",
    "\n",
    "train_groups = train_data.groupby('keyword').size().to_numpy()\n",
    "test_groups = test_data.groupby('keyword').size().to_numpy()\n",
    "\n",
    "ranker = xgb.XGBRanker(tree_method='hist',\n",
    "#                         booster='gbtree',\n",
    "                        objective='rank:ndcg',\n",
    "                        eval_metric=['logloss', 'ndcg'],\n",
    "                        random_state=123,  \n",
    "                        learning_rate= 0.06338827428733157, \n",
    "                        n_estimators= 200, \n",
    "                        max_depth= 78, \n",
    "                        reg_lambda= 3.86173348994611e-07, \n",
    "                        lambdarank_num_pair_per_sample= 8, \n",
    "                        lambdarank_pair_method= 'topk', \n",
    "                        early_stopping_rounds= 97, \n",
    "                        alpha= 0.006056282064610986, \n",
    "                        gamma= 0.02093151208361956, \n",
    "                        grow_policy= 'depthwise', \n",
    "                        max_leaves= 151, \n",
    "                        min_child_weight= 9, \n",
    "                        colsample_bytree= 0.9120013237152653, \n",
    "                        colsample_bylevel= 0.9242331949841277, \n",
    "                        colsample_bynode= 0.5855540278431589, \n",
    "                        subsample= 0.8982514390329758)\n",
    "\n",
    "ranker.fit(X_train, y_train, group=train_groups, verbose=True, eval_set=[(X_train, y_train),(X_test, y_test)], eval_group=[train_groups, test_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ca50f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9477629701814294, 0.9607014659327789, 1.0, 0.7537428918493829)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores4b =[]\n",
    "for query, group in test_data.groupby('keyword'):\n",
    "    if len(group['rank'].tolist()) != 1:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "        scores4b.append(nd(np.asarray([ranks]), np.asarray([ranker.predict(features).tolist()])))\n",
    "np.mean(scores4b), np.median(scores4b), max(scores4b), min(scores4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02aad23",
   "metadata": {},
   "source": [
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b1a5762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:58.67070\tvalidation_0-ndcg:0.77358\tvalidation_1-logloss:59.02332\tvalidation_1-ndcg:0.72943\n",
      "[1]\tvalidation_0-logloss:81.24446\tvalidation_0-ndcg:0.84817\tvalidation_1-logloss:83.43624\tvalidation_1-ndcg:0.74475\n",
      "[2]\tvalidation_0-logloss:91.09856\tvalidation_0-ndcg:0.88379\tvalidation_1-logloss:95.91733\tvalidation_1-ndcg:0.76473\n",
      "[3]\tvalidation_0-logloss:97.58592\tvalidation_0-ndcg:0.90369\tvalidation_1-logloss:103.16549\tvalidation_1-ndcg:0.75881\n",
      "[4]\tvalidation_0-logloss:101.46456\tvalidation_0-ndcg:0.91379\tvalidation_1-logloss:106.13143\tvalidation_1-ndcg:0.76189\n",
      "[5]\tvalidation_0-logloss:103.58037\tvalidation_0-ndcg:0.92338\tvalidation_1-logloss:107.23295\tvalidation_1-ndcg:0.76546\n",
      "[6]\tvalidation_0-logloss:106.32198\tvalidation_0-ndcg:0.92893\tvalidation_1-logloss:107.34784\tvalidation_1-ndcg:0.77908\n",
      "[7]\tvalidation_0-logloss:107.60691\tvalidation_0-ndcg:0.93487\tvalidation_1-logloss:109.69489\tvalidation_1-ndcg:0.77324\n",
      "[8]\tvalidation_0-logloss:109.24807\tvalidation_0-ndcg:0.93864\tvalidation_1-logloss:111.93578\tvalidation_1-ndcg:0.78135\n",
      "[9]\tvalidation_0-logloss:110.77006\tvalidation_0-ndcg:0.94224\tvalidation_1-logloss:110.95476\tvalidation_1-ndcg:0.77478\n",
      "[10]\tvalidation_0-logloss:112.23001\tvalidation_0-ndcg:0.94539\tvalidation_1-logloss:112.57743\tvalidation_1-ndcg:0.77813\n",
      "[11]\tvalidation_0-logloss:113.04749\tvalidation_0-ndcg:0.94870\tvalidation_1-logloss:114.92807\tvalidation_1-ndcg:0.78085\n",
      "[12]\tvalidation_0-logloss:112.92681\tvalidation_0-ndcg:0.95063\tvalidation_1-logloss:114.56284\tvalidation_1-ndcg:0.77976\n",
      "[13]\tvalidation_0-logloss:112.69978\tvalidation_0-ndcg:0.95206\tvalidation_1-logloss:117.16813\tvalidation_1-ndcg:0.77943\n",
      "[14]\tvalidation_0-logloss:113.67329\tvalidation_0-ndcg:0.95403\tvalidation_1-logloss:116.27752\tvalidation_1-ndcg:0.78034\n",
      "[15]\tvalidation_0-logloss:113.95730\tvalidation_0-ndcg:0.95699\tvalidation_1-logloss:115.98782\tvalidation_1-ndcg:0.78575\n",
      "[16]\tvalidation_0-logloss:114.44194\tvalidation_0-ndcg:0.95775\tvalidation_1-logloss:117.04942\tvalidation_1-ndcg:0.78571\n",
      "[17]\tvalidation_0-logloss:114.51916\tvalidation_0-ndcg:0.95900\tvalidation_1-logloss:117.29102\tvalidation_1-ndcg:0.78885\n",
      "[18]\tvalidation_0-logloss:115.80094\tvalidation_0-ndcg:0.95983\tvalidation_1-logloss:117.88973\tvalidation_1-ndcg:0.78727\n",
      "[19]\tvalidation_0-logloss:116.15434\tvalidation_0-ndcg:0.96129\tvalidation_1-logloss:119.17577\tvalidation_1-ndcg:0.79097\n",
      "[20]\tvalidation_0-logloss:116.36012\tvalidation_0-ndcg:0.96159\tvalidation_1-logloss:118.84086\tvalidation_1-ndcg:0.78915\n",
      "[21]\tvalidation_0-logloss:116.66732\tvalidation_0-ndcg:0.96276\tvalidation_1-logloss:118.55375\tvalidation_1-ndcg:0.79224\n",
      "[22]\tvalidation_0-logloss:116.32003\tvalidation_0-ndcg:0.96400\tvalidation_1-logloss:118.11552\tvalidation_1-ndcg:0.79520\n",
      "[23]\tvalidation_0-logloss:116.66992\tvalidation_0-ndcg:0.96437\tvalidation_1-logloss:117.86118\tvalidation_1-ndcg:0.79393\n",
      "[24]\tvalidation_0-logloss:117.21709\tvalidation_0-ndcg:0.96531\tvalidation_1-logloss:119.49820\tvalidation_1-ndcg:0.79707\n",
      "[25]\tvalidation_0-logloss:116.75751\tvalidation_0-ndcg:0.96602\tvalidation_1-logloss:119.76130\tvalidation_1-ndcg:0.79824\n",
      "[26]\tvalidation_0-logloss:116.69547\tvalidation_0-ndcg:0.96623\tvalidation_1-logloss:118.62299\tvalidation_1-ndcg:0.79418\n",
      "[27]\tvalidation_0-logloss:116.98336\tvalidation_0-ndcg:0.96668\tvalidation_1-logloss:118.55184\tvalidation_1-ndcg:0.79072\n",
      "[28]\tvalidation_0-logloss:116.92799\tvalidation_0-ndcg:0.96755\tvalidation_1-logloss:118.91820\tvalidation_1-ndcg:0.78797\n",
      "[29]\tvalidation_0-logloss:116.74303\tvalidation_0-ndcg:0.96770\tvalidation_1-logloss:119.21398\tvalidation_1-ndcg:0.78860\n",
      "[30]\tvalidation_0-logloss:116.26874\tvalidation_0-ndcg:0.96825\tvalidation_1-logloss:119.27649\tvalidation_1-ndcg:0.78732\n",
      "[31]\tvalidation_0-logloss:116.31177\tvalidation_0-ndcg:0.96868\tvalidation_1-logloss:119.87979\tvalidation_1-ndcg:0.78880\n",
      "[32]\tvalidation_0-logloss:115.83947\tvalidation_0-ndcg:0.96895\tvalidation_1-logloss:120.50746\tvalidation_1-ndcg:0.78931\n",
      "[33]\tvalidation_0-logloss:115.22091\tvalidation_0-ndcg:0.96962\tvalidation_1-logloss:119.75549\tvalidation_1-ndcg:0.78882\n",
      "[34]\tvalidation_0-logloss:115.12953\tvalidation_0-ndcg:0.96969\tvalidation_1-logloss:119.80088\tvalidation_1-ndcg:0.79040\n",
      "[35]\tvalidation_0-logloss:114.86025\tvalidation_0-ndcg:0.97006\tvalidation_1-logloss:120.15587\tvalidation_1-ndcg:0.78818\n",
      "[36]\tvalidation_0-logloss:114.21755\tvalidation_0-ndcg:0.97057\tvalidation_1-logloss:119.24834\tvalidation_1-ndcg:0.78574\n",
      "[37]\tvalidation_0-logloss:114.11077\tvalidation_0-ndcg:0.97073\tvalidation_1-logloss:119.40526\tvalidation_1-ndcg:0.79298\n",
      "[38]\tvalidation_0-logloss:113.88875\tvalidation_0-ndcg:0.97103\tvalidation_1-logloss:118.90622\tvalidation_1-ndcg:0.79314\n",
      "[39]\tvalidation_0-logloss:113.22411\tvalidation_0-ndcg:0.97181\tvalidation_1-logloss:118.46401\tvalidation_1-ndcg:0.79219\n",
      "[40]\tvalidation_0-logloss:112.66725\tvalidation_0-ndcg:0.97203\tvalidation_1-logloss:118.77570\tvalidation_1-ndcg:0.79210\n",
      "[41]\tvalidation_0-logloss:112.03537\tvalidation_0-ndcg:0.97208\tvalidation_1-logloss:119.79629\tvalidation_1-ndcg:0.79309\n",
      "[42]\tvalidation_0-logloss:111.93221\tvalidation_0-ndcg:0.97204\tvalidation_1-logloss:119.52130\tvalidation_1-ndcg:0.79028\n",
      "[43]\tvalidation_0-logloss:111.00764\tvalidation_0-ndcg:0.97251\tvalidation_1-logloss:118.84326\tvalidation_1-ndcg:0.78895\n",
      "[44]\tvalidation_0-logloss:110.89844\tvalidation_0-ndcg:0.97295\tvalidation_1-logloss:117.70463\tvalidation_1-ndcg:0.78816\n",
      "[45]\tvalidation_0-logloss:109.90417\tvalidation_0-ndcg:0.97319\tvalidation_1-logloss:118.90912\tvalidation_1-ndcg:0.79174\n",
      "[46]\tvalidation_0-logloss:109.68545\tvalidation_0-ndcg:0.97353\tvalidation_1-logloss:118.82686\tvalidation_1-ndcg:0.79044\n",
      "[47]\tvalidation_0-logloss:109.28750\tvalidation_0-ndcg:0.97355\tvalidation_1-logloss:119.01927\tvalidation_1-ndcg:0.79097\n",
      "[48]\tvalidation_0-logloss:109.29606\tvalidation_0-ndcg:0.97399\tvalidation_1-logloss:119.20354\tvalidation_1-ndcg:0.79341\n",
      "[49]\tvalidation_0-logloss:108.94751\tvalidation_0-ndcg:0.97403\tvalidation_1-logloss:117.82758\tvalidation_1-ndcg:0.79387\n",
      "[50]\tvalidation_0-logloss:108.27911\tvalidation_0-ndcg:0.97414\tvalidation_1-logloss:118.48605\tvalidation_1-ndcg:0.79478\n",
      "[51]\tvalidation_0-logloss:108.16711\tvalidation_0-ndcg:0.97419\tvalidation_1-logloss:118.99999\tvalidation_1-ndcg:0.79488\n",
      "[52]\tvalidation_0-logloss:107.33383\tvalidation_0-ndcg:0.97448\tvalidation_1-logloss:118.87892\tvalidation_1-ndcg:0.79383\n",
      "[53]\tvalidation_0-logloss:107.40434\tvalidation_0-ndcg:0.97460\tvalidation_1-logloss:118.38066\tvalidation_1-ndcg:0.79527\n",
      "[54]\tvalidation_0-logloss:106.88293\tvalidation_0-ndcg:0.97487\tvalidation_1-logloss:118.90932\tvalidation_1-ndcg:0.79428\n",
      "[55]\tvalidation_0-logloss:106.47374\tvalidation_0-ndcg:0.97527\tvalidation_1-logloss:119.36618\tvalidation_1-ndcg:0.79511\n",
      "[56]\tvalidation_0-logloss:106.47475\tvalidation_0-ndcg:0.97581\tvalidation_1-logloss:118.76050\tvalidation_1-ndcg:0.79749\n",
      "[57]\tvalidation_0-logloss:105.40859\tvalidation_0-ndcg:0.97611\tvalidation_1-logloss:118.68710\tvalidation_1-ndcg:0.79790\n",
      "[58]\tvalidation_0-logloss:105.65921\tvalidation_0-ndcg:0.97646\tvalidation_1-logloss:118.96883\tvalidation_1-ndcg:0.79714\n",
      "[59]\tvalidation_0-logloss:104.80352\tvalidation_0-ndcg:0.97667\tvalidation_1-logloss:118.42593\tvalidation_1-ndcg:0.79626\n",
      "[60]\tvalidation_0-logloss:104.34066\tvalidation_0-ndcg:0.97674\tvalidation_1-logloss:118.56118\tvalidation_1-ndcg:0.79592\n",
      "[61]\tvalidation_0-logloss:103.89556\tvalidation_0-ndcg:0.97691\tvalidation_1-logloss:118.25558\tvalidation_1-ndcg:0.79726\n",
      "[62]\tvalidation_0-logloss:103.94751\tvalidation_0-ndcg:0.97713\tvalidation_1-logloss:118.12361\tvalidation_1-ndcg:0.79875\n",
      "[63]\tvalidation_0-logloss:103.60785\tvalidation_0-ndcg:0.97729\tvalidation_1-logloss:118.24013\tvalidation_1-ndcg:0.79831\n",
      "[64]\tvalidation_0-logloss:103.27873\tvalidation_0-ndcg:0.97729\tvalidation_1-logloss:118.09171\tvalidation_1-ndcg:0.79839\n",
      "[65]\tvalidation_0-logloss:103.12026\tvalidation_0-ndcg:0.97752\tvalidation_1-logloss:117.56538\tvalidation_1-ndcg:0.79742\n",
      "[66]\tvalidation_0-logloss:103.05945\tvalidation_0-ndcg:0.97777\tvalidation_1-logloss:118.03213\tvalidation_1-ndcg:0.79848\n",
      "[67]\tvalidation_0-logloss:102.51113\tvalidation_0-ndcg:0.97783\tvalidation_1-logloss:117.94388\tvalidation_1-ndcg:0.79862\n",
      "[68]\tvalidation_0-logloss:101.92049\tvalidation_0-ndcg:0.97807\tvalidation_1-logloss:117.93520\tvalidation_1-ndcg:0.80021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\tvalidation_0-logloss:101.83652\tvalidation_0-ndcg:0.97842\tvalidation_1-logloss:117.63709\tvalidation_1-ndcg:0.79983\n",
      "[70]\tvalidation_0-logloss:102.11748\tvalidation_0-ndcg:0.97849\tvalidation_1-logloss:117.59416\tvalidation_1-ndcg:0.79968\n",
      "[71]\tvalidation_0-logloss:101.51101\tvalidation_0-ndcg:0.97898\tvalidation_1-logloss:117.04044\tvalidation_1-ndcg:0.79830\n",
      "[72]\tvalidation_0-logloss:101.64158\tvalidation_0-ndcg:0.97911\tvalidation_1-logloss:117.62437\tvalidation_1-ndcg:0.79989\n",
      "[73]\tvalidation_0-logloss:101.27343\tvalidation_0-ndcg:0.97934\tvalidation_1-logloss:116.10533\tvalidation_1-ndcg:0.80007\n",
      "[74]\tvalidation_0-logloss:100.92330\tvalidation_0-ndcg:0.97944\tvalidation_1-logloss:116.76638\tvalidation_1-ndcg:0.80066\n",
      "[75]\tvalidation_0-logloss:100.68503\tvalidation_0-ndcg:0.97971\tvalidation_1-logloss:116.59778\tvalidation_1-ndcg:0.79826\n",
      "[76]\tvalidation_0-logloss:100.18651\tvalidation_0-ndcg:0.98007\tvalidation_1-logloss:116.79849\tvalidation_1-ndcg:0.79966\n",
      "[77]\tvalidation_0-logloss:100.26090\tvalidation_0-ndcg:0.97996\tvalidation_1-logloss:116.59695\tvalidation_1-ndcg:0.79931\n",
      "[78]\tvalidation_0-logloss:99.86263\tvalidation_0-ndcg:0.98009\tvalidation_1-logloss:116.61654\tvalidation_1-ndcg:0.79679\n",
      "[79]\tvalidation_0-logloss:99.99274\tvalidation_0-ndcg:0.98015\tvalidation_1-logloss:116.78798\tvalidation_1-ndcg:0.79632\n",
      "[80]\tvalidation_0-logloss:99.68156\tvalidation_0-ndcg:0.98022\tvalidation_1-logloss:116.17617\tvalidation_1-ndcg:0.79717\n",
      "[81]\tvalidation_0-logloss:99.68155\tvalidation_0-ndcg:0.98027\tvalidation_1-logloss:116.14852\tvalidation_1-ndcg:0.79902\n",
      "[82]\tvalidation_0-logloss:99.17259\tvalidation_0-ndcg:0.98029\tvalidation_1-logloss:116.38051\tvalidation_1-ndcg:0.79853\n",
      "[83]\tvalidation_0-logloss:99.26532\tvalidation_0-ndcg:0.98052\tvalidation_1-logloss:116.29560\tvalidation_1-ndcg:0.79887\n",
      "[84]\tvalidation_0-logloss:98.92905\tvalidation_0-ndcg:0.98054\tvalidation_1-logloss:116.65668\tvalidation_1-ndcg:0.79947\n",
      "[85]\tvalidation_0-logloss:98.89211\tvalidation_0-ndcg:0.98061\tvalidation_1-logloss:116.31105\tvalidation_1-ndcg:0.79960\n",
      "[86]\tvalidation_0-logloss:99.01788\tvalidation_0-ndcg:0.98066\tvalidation_1-logloss:116.02509\tvalidation_1-ndcg:0.79977\n",
      "[87]\tvalidation_0-logloss:99.01205\tvalidation_0-ndcg:0.98067\tvalidation_1-logloss:116.39647\tvalidation_1-ndcg:0.80020\n",
      "[88]\tvalidation_0-logloss:98.69552\tvalidation_0-ndcg:0.98076\tvalidation_1-logloss:116.10451\tvalidation_1-ndcg:0.80312\n",
      "[89]\tvalidation_0-logloss:98.64201\tvalidation_0-ndcg:0.98081\tvalidation_1-logloss:116.42807\tvalidation_1-ndcg:0.80234\n",
      "[90]\tvalidation_0-logloss:98.55983\tvalidation_0-ndcg:0.98087\tvalidation_1-logloss:115.77686\tvalidation_1-ndcg:0.80055\n",
      "[91]\tvalidation_0-logloss:98.38927\tvalidation_0-ndcg:0.98110\tvalidation_1-logloss:115.66477\tvalidation_1-ndcg:0.80212\n",
      "[92]\tvalidation_0-logloss:98.60065\tvalidation_0-ndcg:0.98109\tvalidation_1-logloss:115.50619\tvalidation_1-ndcg:0.80058\n",
      "[93]\tvalidation_0-logloss:98.68315\tvalidation_0-ndcg:0.98111\tvalidation_1-logloss:115.33130\tvalidation_1-ndcg:0.79837\n",
      "[94]\tvalidation_0-logloss:98.28825\tvalidation_0-ndcg:0.98113\tvalidation_1-logloss:114.71075\tvalidation_1-ndcg:0.80109\n",
      "[95]\tvalidation_0-logloss:97.98475\tvalidation_0-ndcg:0.98116\tvalidation_1-logloss:114.77863\tvalidation_1-ndcg:0.80339\n",
      "[96]\tvalidation_0-logloss:97.71359\tvalidation_0-ndcg:0.98132\tvalidation_1-logloss:114.74552\tvalidation_1-ndcg:0.80309\n",
      "[97]\tvalidation_0-logloss:97.54344\tvalidation_0-ndcg:0.98141\tvalidation_1-logloss:115.05566\tvalidation_1-ndcg:0.80466\n",
      "[98]\tvalidation_0-logloss:97.16940\tvalidation_0-ndcg:0.98147\tvalidation_1-logloss:114.95663\tvalidation_1-ndcg:0.80353\n",
      "[99]\tvalidation_0-logloss:97.46477\tvalidation_0-ndcg:0.98145\tvalidation_1-logloss:114.68334\tvalidation_1-ndcg:0.80331\n",
      "[100]\tvalidation_0-logloss:96.96321\tvalidation_0-ndcg:0.98150\tvalidation_1-logloss:114.79402\tvalidation_1-ndcg:0.80058\n",
      "[101]\tvalidation_0-logloss:97.09915\tvalidation_0-ndcg:0.98150\tvalidation_1-logloss:114.96515\tvalidation_1-ndcg:0.80215\n",
      "[102]\tvalidation_0-logloss:97.10003\tvalidation_0-ndcg:0.98155\tvalidation_1-logloss:114.82413\tvalidation_1-ndcg:0.80265\n",
      "[103]\tvalidation_0-logloss:97.12508\tvalidation_0-ndcg:0.98157\tvalidation_1-logloss:114.47745\tvalidation_1-ndcg:0.80183\n",
      "[104]\tvalidation_0-logloss:97.02732\tvalidation_0-ndcg:0.98161\tvalidation_1-logloss:114.53234\tvalidation_1-ndcg:0.80193\n",
      "[105]\tvalidation_0-logloss:96.99503\tvalidation_0-ndcg:0.98168\tvalidation_1-logloss:114.11358\tvalidation_1-ndcg:0.80259\n",
      "[106]\tvalidation_0-logloss:96.84090\tvalidation_0-ndcg:0.98170\tvalidation_1-logloss:114.22543\tvalidation_1-ndcg:0.80025\n",
      "[107]\tvalidation_0-logloss:96.72981\tvalidation_0-ndcg:0.98191\tvalidation_1-logloss:114.51670\tvalidation_1-ndcg:0.80018\n",
      "[108]\tvalidation_0-logloss:96.63350\tvalidation_0-ndcg:0.98208\tvalidation_1-logloss:114.71724\tvalidation_1-ndcg:0.80038\n",
      "[109]\tvalidation_0-logloss:96.77431\tvalidation_0-ndcg:0.98215\tvalidation_1-logloss:114.56915\tvalidation_1-ndcg:0.79924\n",
      "[110]\tvalidation_0-logloss:96.49396\tvalidation_0-ndcg:0.98214\tvalidation_1-logloss:114.15034\tvalidation_1-ndcg:0.80010\n",
      "[111]\tvalidation_0-logloss:96.58148\tvalidation_0-ndcg:0.98228\tvalidation_1-logloss:114.81095\tvalidation_1-ndcg:0.80042\n",
      "[112]\tvalidation_0-logloss:96.40092\tvalidation_0-ndcg:0.98229\tvalidation_1-logloss:114.11136\tvalidation_1-ndcg:0.80046\n",
      "[113]\tvalidation_0-logloss:96.51895\tvalidation_0-ndcg:0.98233\tvalidation_1-logloss:113.53678\tvalidation_1-ndcg:0.80045\n",
      "[114]\tvalidation_0-logloss:96.80985\tvalidation_0-ndcg:0.98235\tvalidation_1-logloss:113.68065\tvalidation_1-ndcg:0.80091\n",
      "[115]\tvalidation_0-logloss:96.24195\tvalidation_0-ndcg:0.98236\tvalidation_1-logloss:113.61060\tvalidation_1-ndcg:0.80061\n",
      "[116]\tvalidation_0-logloss:96.35822\tvalidation_0-ndcg:0.98240\tvalidation_1-logloss:114.10346\tvalidation_1-ndcg:0.80181\n",
      "[117]\tvalidation_0-logloss:95.92774\tvalidation_0-ndcg:0.98238\tvalidation_1-logloss:114.11565\tvalidation_1-ndcg:0.80230\n",
      "[118]\tvalidation_0-logloss:95.66322\tvalidation_0-ndcg:0.98237\tvalidation_1-logloss:113.91233\tvalidation_1-ndcg:0.80107\n",
      "[119]\tvalidation_0-logloss:95.34955\tvalidation_0-ndcg:0.98250\tvalidation_1-logloss:113.77219\tvalidation_1-ndcg:0.80076\n",
      "[120]\tvalidation_0-logloss:95.43046\tvalidation_0-ndcg:0.98251\tvalidation_1-logloss:113.38464\tvalidation_1-ndcg:0.80009\n",
      "[121]\tvalidation_0-logloss:95.32291\tvalidation_0-ndcg:0.98256\tvalidation_1-logloss:113.69078\tvalidation_1-ndcg:0.80062\n",
      "[122]\tvalidation_0-logloss:95.34960\tvalidation_0-ndcg:0.98276\tvalidation_1-logloss:113.88719\tvalidation_1-ndcg:0.80169\n",
      "[123]\tvalidation_0-logloss:95.28585\tvalidation_0-ndcg:0.98298\tvalidation_1-logloss:114.29471\tvalidation_1-ndcg:0.80249\n",
      "[124]\tvalidation_0-logloss:95.22537\tvalidation_0-ndcg:0.98300\tvalidation_1-logloss:113.47620\tvalidation_1-ndcg:0.80075\n",
      "[125]\tvalidation_0-logloss:95.21100\tvalidation_0-ndcg:0.98301\tvalidation_1-logloss:113.91982\tvalidation_1-ndcg:0.80128\n",
      "[126]\tvalidation_0-logloss:94.81953\tvalidation_0-ndcg:0.98308\tvalidation_1-logloss:113.70744\tvalidation_1-ndcg:0.80146\n",
      "[127]\tvalidation_0-logloss:95.01057\tvalidation_0-ndcg:0.98318\tvalidation_1-logloss:113.91255\tvalidation_1-ndcg:0.80168\n",
      "[128]\tvalidation_0-logloss:95.07164\tvalidation_0-ndcg:0.98322\tvalidation_1-logloss:113.90413\tvalidation_1-ndcg:0.80081\n",
      "[129]\tvalidation_0-logloss:94.81446\tvalidation_0-ndcg:0.98329\tvalidation_1-logloss:113.39448\tvalidation_1-ndcg:0.80239\n",
      "[130]\tvalidation_0-logloss:94.82889\tvalidation_0-ndcg:0.98332\tvalidation_1-logloss:113.84329\tvalidation_1-ndcg:0.80316\n",
      "[131]\tvalidation_0-logloss:94.72755\tvalidation_0-ndcg:0.98333\tvalidation_1-logloss:113.68652\tvalidation_1-ndcg:0.80248\n",
      "[132]\tvalidation_0-logloss:94.68771\tvalidation_0-ndcg:0.98328\tvalidation_1-logloss:113.55720\tvalidation_1-ndcg:0.80316\n",
      "[133]\tvalidation_0-logloss:94.66052\tvalidation_0-ndcg:0.98343\tvalidation_1-logloss:113.82201\tvalidation_1-ndcg:0.80354\n",
      "[134]\tvalidation_0-logloss:94.67763\tvalidation_0-ndcg:0.98343\tvalidation_1-logloss:114.01656\tvalidation_1-ndcg:0.80232\n",
      "[135]\tvalidation_0-logloss:94.70497\tvalidation_0-ndcg:0.98342\tvalidation_1-logloss:114.03932\tvalidation_1-ndcg:0.80252\n",
      "[136]\tvalidation_0-logloss:94.77666\tvalidation_0-ndcg:0.98344\tvalidation_1-logloss:113.60047\tvalidation_1-ndcg:0.80175\n",
      "[137]\tvalidation_0-logloss:94.61251\tvalidation_0-ndcg:0.98345\tvalidation_1-logloss:113.56836\tvalidation_1-ndcg:0.80221\n",
      "[138]\tvalidation_0-logloss:94.33484\tvalidation_0-ndcg:0.98352\tvalidation_1-logloss:113.52262\tvalidation_1-ndcg:0.80329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139]\tvalidation_0-logloss:94.36110\tvalidation_0-ndcg:0.98387\tvalidation_1-logloss:113.25862\tvalidation_1-ndcg:0.80387\n",
      "[140]\tvalidation_0-logloss:94.25701\tvalidation_0-ndcg:0.98391\tvalidation_1-logloss:113.47694\tvalidation_1-ndcg:0.80408\n",
      "[141]\tvalidation_0-logloss:94.58227\tvalidation_0-ndcg:0.98401\tvalidation_1-logloss:113.57762\tvalidation_1-ndcg:0.80475\n",
      "[142]\tvalidation_0-logloss:94.44452\tvalidation_0-ndcg:0.98413\tvalidation_1-logloss:113.13209\tvalidation_1-ndcg:0.80468\n",
      "[143]\tvalidation_0-logloss:94.46691\tvalidation_0-ndcg:0.98413\tvalidation_1-logloss:113.30945\tvalidation_1-ndcg:0.80522\n",
      "[144]\tvalidation_0-logloss:94.33868\tvalidation_0-ndcg:0.98431\tvalidation_1-logloss:113.32330\tvalidation_1-ndcg:0.80584\n",
      "[145]\tvalidation_0-logloss:94.54112\tvalidation_0-ndcg:0.98433\tvalidation_1-logloss:113.51314\tvalidation_1-ndcg:0.80678\n",
      "[146]\tvalidation_0-logloss:94.39679\tvalidation_0-ndcg:0.98435\tvalidation_1-logloss:112.90781\tvalidation_1-ndcg:0.80687\n",
      "[147]\tvalidation_0-logloss:94.38872\tvalidation_0-ndcg:0.98445\tvalidation_1-logloss:113.12300\tvalidation_1-ndcg:0.80498\n",
      "[148]\tvalidation_0-logloss:94.46465\tvalidation_0-ndcg:0.98452\tvalidation_1-logloss:113.20814\tvalidation_1-ndcg:0.80527\n",
      "[149]\tvalidation_0-logloss:94.25675\tvalidation_0-ndcg:0.98465\tvalidation_1-logloss:113.21945\tvalidation_1-ndcg:0.80547\n",
      "[150]\tvalidation_0-logloss:94.31728\tvalidation_0-ndcg:0.98467\tvalidation_1-logloss:113.65707\tvalidation_1-ndcg:0.80452\n",
      "[151]\tvalidation_0-logloss:94.15694\tvalidation_0-ndcg:0.98467\tvalidation_1-logloss:113.70018\tvalidation_1-ndcg:0.80476\n",
      "[152]\tvalidation_0-logloss:94.13834\tvalidation_0-ndcg:0.98485\tvalidation_1-logloss:113.87200\tvalidation_1-ndcg:0.80519\n",
      "[153]\tvalidation_0-logloss:94.16567\tvalidation_0-ndcg:0.98485\tvalidation_1-logloss:113.55784\tvalidation_1-ndcg:0.80596\n",
      "[154]\tvalidation_0-logloss:94.05367\tvalidation_0-ndcg:0.98489\tvalidation_1-logloss:113.83361\tvalidation_1-ndcg:0.80545\n",
      "[155]\tvalidation_0-logloss:93.82692\tvalidation_0-ndcg:0.98492\tvalidation_1-logloss:113.12710\tvalidation_1-ndcg:0.80564\n",
      "[156]\tvalidation_0-logloss:93.76693\tvalidation_0-ndcg:0.98493\tvalidation_1-logloss:114.62899\tvalidation_1-ndcg:0.80602\n",
      "[157]\tvalidation_0-logloss:93.88605\tvalidation_0-ndcg:0.98498\tvalidation_1-logloss:114.77378\tvalidation_1-ndcg:0.80650\n",
      "[158]\tvalidation_0-logloss:93.64053\tvalidation_0-ndcg:0.98499\tvalidation_1-logloss:114.53132\tvalidation_1-ndcg:0.80596\n",
      "[159]\tvalidation_0-logloss:93.51017\tvalidation_0-ndcg:0.98510\tvalidation_1-logloss:114.33626\tvalidation_1-ndcg:0.80395\n",
      "[160]\tvalidation_0-logloss:93.45795\tvalidation_0-ndcg:0.98528\tvalidation_1-logloss:114.32262\tvalidation_1-ndcg:0.80458\n",
      "[161]\tvalidation_0-logloss:93.49725\tvalidation_0-ndcg:0.98531\tvalidation_1-logloss:114.07669\tvalidation_1-ndcg:0.80408\n",
      "[162]\tvalidation_0-logloss:93.49466\tvalidation_0-ndcg:0.98533\tvalidation_1-logloss:114.25823\tvalidation_1-ndcg:0.80487\n",
      "[163]\tvalidation_0-logloss:93.56799\tvalidation_0-ndcg:0.98534\tvalidation_1-logloss:114.11352\tvalidation_1-ndcg:0.80513\n",
      "[164]\tvalidation_0-logloss:93.56248\tvalidation_0-ndcg:0.98547\tvalidation_1-logloss:114.37779\tvalidation_1-ndcg:0.80637\n",
      "[165]\tvalidation_0-logloss:93.87061\tvalidation_0-ndcg:0.98550\tvalidation_1-logloss:113.84741\tvalidation_1-ndcg:0.80726\n",
      "[166]\tvalidation_0-logloss:93.62728\tvalidation_0-ndcg:0.98554\tvalidation_1-logloss:114.48576\tvalidation_1-ndcg:0.80574\n",
      "[167]\tvalidation_0-logloss:93.69808\tvalidation_0-ndcg:0.98572\tvalidation_1-logloss:114.35953\tvalidation_1-ndcg:0.80544\n",
      "[168]\tvalidation_0-logloss:93.79420\tvalidation_0-ndcg:0.98577\tvalidation_1-logloss:114.45297\tvalidation_1-ndcg:0.80494\n",
      "[169]\tvalidation_0-logloss:93.64854\tvalidation_0-ndcg:0.98579\tvalidation_1-logloss:114.14254\tvalidation_1-ndcg:0.80502\n",
      "[170]\tvalidation_0-logloss:93.60580\tvalidation_0-ndcg:0.98582\tvalidation_1-logloss:114.03467\tvalidation_1-ndcg:0.80485\n",
      "[171]\tvalidation_0-logloss:93.51010\tvalidation_0-ndcg:0.98584\tvalidation_1-logloss:114.10511\tvalidation_1-ndcg:0.80528\n",
      "[172]\tvalidation_0-logloss:93.40495\tvalidation_0-ndcg:0.98587\tvalidation_1-logloss:113.66323\tvalidation_1-ndcg:0.80509\n",
      "[173]\tvalidation_0-logloss:93.37772\tvalidation_0-ndcg:0.98593\tvalidation_1-logloss:113.87883\tvalidation_1-ndcg:0.80556\n",
      "[174]\tvalidation_0-logloss:93.34275\tvalidation_0-ndcg:0.98595\tvalidation_1-logloss:113.83470\tvalidation_1-ndcg:0.80600\n",
      "[175]\tvalidation_0-logloss:93.28267\tvalidation_0-ndcg:0.98600\tvalidation_1-logloss:113.43674\tvalidation_1-ndcg:0.80654\n",
      "[176]\tvalidation_0-logloss:93.04731\tvalidation_0-ndcg:0.98610\tvalidation_1-logloss:113.24366\tvalidation_1-ndcg:0.80691\n",
      "[177]\tvalidation_0-logloss:93.47829\tvalidation_0-ndcg:0.98616\tvalidation_1-logloss:113.60167\tvalidation_1-ndcg:0.80705\n",
      "[178]\tvalidation_0-logloss:93.63431\tvalidation_0-ndcg:0.98618\tvalidation_1-logloss:113.31364\tvalidation_1-ndcg:0.80734\n",
      "[179]\tvalidation_0-logloss:93.37345\tvalidation_0-ndcg:0.98617\tvalidation_1-logloss:112.94482\tvalidation_1-ndcg:0.80701\n",
      "[180]\tvalidation_0-logloss:93.39546\tvalidation_0-ndcg:0.98620\tvalidation_1-logloss:112.93773\tvalidation_1-ndcg:0.80620\n",
      "[181]\tvalidation_0-logloss:93.31362\tvalidation_0-ndcg:0.98626\tvalidation_1-logloss:113.11061\tvalidation_1-ndcg:0.80660\n",
      "[182]\tvalidation_0-logloss:93.19818\tvalidation_0-ndcg:0.98627\tvalidation_1-logloss:112.75193\tvalidation_1-ndcg:0.80667\n",
      "[183]\tvalidation_0-logloss:93.28923\tvalidation_0-ndcg:0.98631\tvalidation_1-logloss:112.97141\tvalidation_1-ndcg:0.80696\n",
      "[184]\tvalidation_0-logloss:93.29804\tvalidation_0-ndcg:0.98630\tvalidation_1-logloss:112.83781\tvalidation_1-ndcg:0.80728\n",
      "[185]\tvalidation_0-logloss:93.29825\tvalidation_0-ndcg:0.98630\tvalidation_1-logloss:112.97810\tvalidation_1-ndcg:0.80675\n",
      "[186]\tvalidation_0-logloss:93.52260\tvalidation_0-ndcg:0.98629\tvalidation_1-logloss:112.63167\tvalidation_1-ndcg:0.80672\n",
      "[187]\tvalidation_0-logloss:93.34219\tvalidation_0-ndcg:0.98647\tvalidation_1-logloss:113.08638\tvalidation_1-ndcg:0.80756\n",
      "[188]\tvalidation_0-logloss:93.35201\tvalidation_0-ndcg:0.98649\tvalidation_1-logloss:112.55518\tvalidation_1-ndcg:0.80789\n",
      "[189]\tvalidation_0-logloss:93.51502\tvalidation_0-ndcg:0.98669\tvalidation_1-logloss:112.57085\tvalidation_1-ndcg:0.80821\n",
      "[190]\tvalidation_0-logloss:93.36805\tvalidation_0-ndcg:0.98669\tvalidation_1-logloss:113.07042\tvalidation_1-ndcg:0.80680\n",
      "[191]\tvalidation_0-logloss:93.43143\tvalidation_0-ndcg:0.98675\tvalidation_1-logloss:112.93699\tvalidation_1-ndcg:0.80790\n",
      "[192]\tvalidation_0-logloss:93.31160\tvalidation_0-ndcg:0.98677\tvalidation_1-logloss:112.59207\tvalidation_1-ndcg:0.80765\n",
      "[193]\tvalidation_0-logloss:93.35989\tvalidation_0-ndcg:0.98680\tvalidation_1-logloss:112.52495\tvalidation_1-ndcg:0.80882\n",
      "[194]\tvalidation_0-logloss:93.39447\tvalidation_0-ndcg:0.98680\tvalidation_1-logloss:112.49640\tvalidation_1-ndcg:0.80689\n",
      "[195]\tvalidation_0-logloss:93.42444\tvalidation_0-ndcg:0.98688\tvalidation_1-logloss:112.50668\tvalidation_1-ndcg:0.80722\n",
      "[196]\tvalidation_0-logloss:93.21945\tvalidation_0-ndcg:0.98694\tvalidation_1-logloss:112.28503\tvalidation_1-ndcg:0.80821\n",
      "[197]\tvalidation_0-logloss:93.23087\tvalidation_0-ndcg:0.98695\tvalidation_1-logloss:112.14002\tvalidation_1-ndcg:0.80688\n",
      "[198]\tvalidation_0-logloss:93.32763\tvalidation_0-ndcg:0.98706\tvalidation_1-logloss:112.12838\tvalidation_1-ndcg:0.80776\n",
      "[199]\tvalidation_0-logloss:93.18736\tvalidation_0-ndcg:0.98725\tvalidation_1-logloss:112.58197\tvalidation_1-ndcg:0.80713\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(alpha=0.006056282064610986, base_score=None, booster=None,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;ndcg&#x27;],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy=&#x27;depthwise...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method=&#x27;topk&#x27;, learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(alpha=0.006056282064610986, base_score=None, booster=None,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=[&#x27;logloss&#x27;, &#x27;ndcg&#x27;],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy=&#x27;depthwise...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method=&#x27;topk&#x27;, learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(alpha=0.006056282064610986, base_score=None, booster=None,\n",
       "          callbacks=None, colsample_bylevel=0.9242331949841277,\n",
       "          colsample_bynode=0.5855540278431589,\n",
       "          colsample_bytree=0.9120013237152653, early_stopping_rounds=97,\n",
       "          enable_categorical=False, eval_metric=['logloss', 'ndcg'],\n",
       "          feature_types=None, gamma=0.02093151208361956, gpu_id=None,\n",
       "          grow_policy='depthwise...portance_type=None,\n",
       "          interaction_constraints=None, lambdarank_num_pair_per_sample=8,\n",
       "          lambdarank_pair_method='topk', learning_rate=0.06338827428733157,\n",
       "          max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "          max_delta_step=None, max_depth=78, max_leaves=151, min_child_weight=9,\n",
       "          missing=nan, monotone_constraints=None, multi_strategy=None,\n",
       "          n_estimators=200, ...)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 5).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds]\n",
    "\n",
    "# Split the data into input features (X) and target labels (y)\n",
    "X_train = train_data[columns_to_normalize]\n",
    "y_train = train_data['rank']\n",
    "\n",
    "X_test = test_data[columns_to_normalize]\n",
    "y_test = test_data['rank']\n",
    "\n",
    "train_groups = train_data.groupby('keyword').size().to_numpy()\n",
    "test_groups = test_data.groupby('keyword').size().to_numpy()\n",
    "\n",
    "ranker = xgb.XGBRanker(tree_method='hist',\n",
    "#                         booster='gbtree',\n",
    "                        objective='rank:ndcg',\n",
    "                        eval_metric=['logloss', 'ndcg'],\n",
    "#                         random_state=421,  \n",
    "                        learning_rate= 0.06338827428733157, \n",
    "                        n_estimators= 200, \n",
    "                        max_depth= 78, \n",
    "                        reg_lambda= 3.86173348994611e-07, \n",
    "                        lambdarank_num_pair_per_sample= 8, \n",
    "                        lambdarank_pair_method= 'topk', \n",
    "                        early_stopping_rounds= 97, \n",
    "                        alpha= 0.006056282064610986, \n",
    "                        gamma= 0.02093151208361956, \n",
    "                        grow_policy= 'depthwise', \n",
    "                        max_leaves= 151, \n",
    "                        min_child_weight= 9, \n",
    "                        colsample_bytree= 0.9120013237152653, \n",
    "                        colsample_bylevel= 0.9242331949841277, \n",
    "                        colsample_bynode= 0.5855540278431589, \n",
    "                        subsample= 0.8982514390329758)\n",
    "\n",
    "ranker.fit(X_train, y_train, group=train_groups, verbose=True, eval_set=[(X_train, y_train),(X_test, y_test)], eval_group=[train_groups, test_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4ee506ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.947568897174001, 0.9600058265608311, 1.0, 0.7587486838975916)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores4c =[]\n",
    "for query, group in test_data.groupby('keyword'):\n",
    "    if len(group['rank'].tolist()) != 1:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "        scores4c.append(nd(np.asarray([ranks]), np.asarray([ranker.predict(features).tolist()])))\n",
    "np.mean(scores4c), np.median(scores4c), max(scores4c), min(scores4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b66eca",
   "metadata": {},
   "source": [
    "## Listwise Learning-To-Rank Model\n",
    "- Loss function = LambdaLoss (Listwise Loss)\n",
    "    - Use Neural Net model\n",
    "- Evaluation Metric: NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "19d26906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        target = self.targets[index]\n",
    "        return feature, target\n",
    "    \n",
    "class ListNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ListNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear: # by checking the type we can init different layers in different ways\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "def listnet_loss(y_pred, y_true):\n",
    "\n",
    "    preds_smax = F.softmax(y_pred, dim=1)\n",
    "    true_smax = F.softmax(y_true, dim=1)\n",
    "\n",
    "    preds_log = torch.log(preds_smax)\n",
    "\n",
    "    return torch.mean(-torch.sum(true_smax * preds_log, dim=1))\n",
    "    \n",
    "def data(data, columns_to_normalize):\n",
    "    grouped_data = data.groupby('keyword')\n",
    "    query_features = []  # List to store query features\n",
    "    query_urls = []  # List to store URLs for each query\n",
    "    query_ranks = []  # List to store ranks for each URL\n",
    "\n",
    "    for query, group in grouped_data:\n",
    "        urls = group['LP_link'].tolist()  # Get URLs for the query\n",
    "        ranks = group['rank'].tolist()  # Get ranks for the URLs\n",
    "        features = group[columns_to_normalize].values.tolist()  # Get features for the URLs\n",
    "\n",
    "        query_urls.append(urls)\n",
    "        query_ranks.append(ranks)\n",
    "        query_features.append(features)\n",
    "\n",
    "    # Determine the maximum number of URLs per query\n",
    "    max_urls_per_query = max([len(urls) for urls in query_urls])\n",
    "\n",
    "    # Pad the sequences in query_features\n",
    "    padded_query_features = []\n",
    "    for features in query_features:\n",
    "        num_features = len(features[0])  # Assuming all URLs have the same number of features\n",
    "        padded_features = features + [[0] * num_features] * (max_urls_per_query - len(features))\n",
    "        padded_query_features.append(padded_features)\n",
    "\n",
    "    # Convert the padded query_features to a NumPy array\n",
    "    query_features_array = np.array(padded_query_features, dtype=np.float32)\n",
    "\n",
    "    # Determine the maximum number of ranks per query\n",
    "    max_ranks_per_query = max([len(ranks) for ranks in query_ranks])\n",
    "\n",
    "    # Pad the sequences in query_ranks\n",
    "    padded_query_ranks = []\n",
    "    for ranks in query_ranks:\n",
    "        padded_ranks = ranks + [max_ranks_per_query+1] * (max_ranks_per_query - len(ranks))\n",
    "        padded_query_ranks.append(padded_ranks)\n",
    "        \n",
    "    # Convert the padded query_ranks to a NumPy array\n",
    "    query_ranks_array = np.array(padded_query_ranks, dtype=np.int64)\n",
    "    \n",
    "    \n",
    "    input_data = torch.tensor(query_features_array, dtype=torch.float32)\n",
    "    target_data = torch.tensor(query_ranks_array, dtype=torch.long)\n",
    "    \n",
    "    return input_data, target_data\n",
    "\n",
    "def train_ltr(net, train_iter, test_iter, loss, num_epochs, optimizer):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "        \n",
    "    loss_ = []\n",
    "    train_ndcg = []\n",
    "    test_ndcg = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        total_ndcg = []\n",
    "        total_test_ndcg = []\n",
    "        test_batches = 0\n",
    "        for i, v in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(i)\n",
    "            v = v.float()\n",
    "            loss = listnet_loss(output.view(-1,10), v)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            for t3, t4 in zip(output, v):\n",
    "                ndcg_score = nd(np.asarray([t4.flatten().tolist()]), np.asarray([t3.flatten().tolist()]))\n",
    "                total_ndcg.append(ndcg_score)\n",
    "\n",
    "        for a, b in test_iter:\n",
    "            test_output = model(a)\n",
    "            test_batches += 1\n",
    "            \n",
    "            for a1, b1 in zip(test_output, b):\n",
    "                test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "                total_test_ndcg.append(test_ndcg_score)\n",
    "                \n",
    "        average_loss = total_loss/num_batches\n",
    "        average_ndcg = np.mean(total_ndcg)\n",
    "        average_test_ndcg = np.mean(total_test_ndcg)\n",
    "        \n",
    "        loss_.append(average_loss)\n",
    "        train_ndcg.append(average_ndcg)\n",
    "        test_ndcg.append(average_test_ndcg)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Train-NDCG: {average_ndcg}, Test-NDCG: {average_test_ndcg}\")\n",
    "    return loss_, train_ndcg, test_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb290ae",
   "metadata": {},
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e5cbd807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.1300756497816606, Train-NDCG: 0.9237353941188438, Test-NDCG: 0.9378382064456627\n",
      "Epoch 2/100, Loss: 1.7634781734509901, Train-NDCG: 0.9450238080097436, Test-NDCG: 0.9409136096442511\n",
      "Epoch 3/100, Loss: 1.6957042271440679, Train-NDCG: 0.9517926090780279, Test-NDCG: 0.9411541024375356\n",
      "Epoch 4/100, Loss: 1.657327421686866, Train-NDCG: 0.9556829513035631, Test-NDCG: 0.9439149443036821\n",
      "Epoch 5/100, Loss: 1.6277257231148807, Train-NDCG: 0.9583287214550047, Test-NDCG: 0.9427360336123526\n",
      "Epoch 6/100, Loss: 1.576784534887834, Train-NDCG: 0.9615012576450358, Test-NDCG: 0.9448325393109854\n",
      "Epoch 7/100, Loss: 1.5492378581653943, Train-NDCG: 0.963590631047635, Test-NDCG: 0.94532809018402\n",
      "Epoch 8/100, Loss: 1.5397458455779336, Train-NDCG: 0.9644179069280479, Test-NDCG: 0.9425382085462608\n",
      "Epoch 9/100, Loss: 1.5129285942424426, Train-NDCG: 0.9675927967163088, Test-NDCG: 0.9460176439356927\n",
      "Epoch 10/100, Loss: 1.5058312145146457, Train-NDCG: 0.9671500933724618, Test-NDCG: 0.9440613219870443\n",
      "Epoch 11/100, Loss: 1.50262882492759, Train-NDCG: 0.9663898284915995, Test-NDCG: 0.9415990969848351\n",
      "Epoch 12/100, Loss: 1.4902728579261086, Train-NDCG: 0.9680631992636446, Test-NDCG: 0.9472578330480576\n",
      "Epoch 13/100, Loss: 1.4646620587869124, Train-NDCG: 0.968885458937237, Test-NDCG: 0.9466202911518299\n",
      "Epoch 14/100, Loss: 1.4672492498701268, Train-NDCG: 0.9691667079440265, Test-NDCG: 0.9441045279486019\n",
      "Epoch 15/100, Loss: 1.4679177836938337, Train-NDCG: 0.9692138034331085, Test-NDCG: 0.9422332435761299\n",
      "Epoch 16/100, Loss: 1.44114610823718, Train-NDCG: 0.9716872123151389, Test-NDCG: 0.9464136454461677\n",
      "Epoch 17/100, Loss: 1.4182809699665417, Train-NDCG: 0.9728680900257373, Test-NDCG: 0.9470291000793006\n",
      "Epoch 18/100, Loss: 1.4286725521087646, Train-NDCG: 0.9718684194647651, Test-NDCG: 0.948072953850175\n",
      "Epoch 19/100, Loss: 1.4485468566417694, Train-NDCG: 0.9708121787921733, Test-NDCG: 0.946411715134971\n",
      "Epoch 20/100, Loss: 1.4196618063883348, Train-NDCG: 0.9713124173191396, Test-NDCG: 0.9464549030031917\n",
      "Epoch 21/100, Loss: 1.405236311934211, Train-NDCG: 0.9742834504028242, Test-NDCG: 0.9485538584710456\n",
      "Epoch 22/100, Loss: 1.4016575325619092, Train-NDCG: 0.9739632755727698, Test-NDCG: 0.947140296699241\n",
      "Epoch 23/100, Loss: 1.411188621412624, Train-NDCG: 0.9725009734398832, Test-NDCG: 0.9446189101746418\n",
      "Epoch 24/100, Loss: 1.3861522864211688, Train-NDCG: 0.9748618482570474, Test-NDCG: 0.9474885505009918\n",
      "Epoch 25/100, Loss: 1.3797340040857142, Train-NDCG: 0.9745430729089685, Test-NDCG: 0.9481209108347691\n",
      "Epoch 26/100, Loss: 1.3702126091176814, Train-NDCG: 0.9760959310564847, Test-NDCG: 0.9477184946507382\n",
      "Epoch 27/100, Loss: 1.3701976050030102, Train-NDCG: 0.9753297308716525, Test-NDCG: 0.9448360612949305\n",
      "Epoch 28/100, Loss: 1.363907353444533, Train-NDCG: 0.9761530345111353, Test-NDCG: 0.9461726539436973\n",
      "Epoch 29/100, Loss: 1.3644081056118011, Train-NDCG: 0.9758161863506271, Test-NDCG: 0.9440001842642967\n",
      "Epoch 30/100, Loss: 1.3659556291320107, Train-NDCG: 0.9765997351234332, Test-NDCG: 0.9431253865523453\n",
      "Epoch 31/100, Loss: 1.3600341840223833, Train-NDCG: 0.9761664150540493, Test-NDCG: 0.9445336901957481\n",
      "Epoch 32/100, Loss: 1.3480057662183589, Train-NDCG: 0.9771605675144474, Test-NDCG: 0.943456779861803\n",
      "Epoch 33/100, Loss: 1.3497644744136117, Train-NDCG: 0.9769866522321707, Test-NDCG: 0.9460080052997613\n",
      "Epoch 34/100, Loss: 1.3451999832283368, Train-NDCG: 0.9770360537307016, Test-NDCG: 0.9442088713476037\n",
      "Epoch 35/100, Loss: 1.3435340442440726, Train-NDCG: 0.9772318835421646, Test-NDCG: 0.9458726555191026\n",
      "Epoch 36/100, Loss: 1.3283178806304932, Train-NDCG: 0.9782627449481562, Test-NDCG: 0.9475462881924218\n",
      "Epoch 37/100, Loss: 1.337280346588655, Train-NDCG: 0.9771157223373156, Test-NDCG: 0.948961078410091\n",
      "Epoch 38/100, Loss: 1.3302717344327406, Train-NDCG: 0.9790302403400211, Test-NDCG: 0.9468653689043507\n",
      "Epoch 39/100, Loss: 1.3240310034968636, Train-NDCG: 0.9786721930834319, Test-NDCG: 0.9439330770839467\n",
      "Epoch 40/100, Loss: 1.3336450593038038, Train-NDCG: 0.9779562631873286, Test-NDCG: 0.9495502583528963\n",
      "Epoch 41/100, Loss: 1.3251211372288791, Train-NDCG: 0.977979340418095, Test-NDCG: 0.9470223034890921\n",
      "Epoch 42/100, Loss: 1.3333429829640822, Train-NDCG: 0.9783196117376771, Test-NDCG: 0.9395809797610513\n",
      "Epoch 43/100, Loss: 1.3373275805603375, Train-NDCG: 0.9775075131018311, Test-NDCG: 0.9475423984211874\n",
      "Epoch 44/100, Loss: 1.326253812421452, Train-NDCG: 0.9780953633277716, Test-NDCG: 0.9472306105928763\n",
      "Epoch 45/100, Loss: 1.3180319423025304, Train-NDCG: 0.9791626783187316, Test-NDCG: 0.9453633354800294\n",
      "Epoch 46/100, Loss: 1.3274537189440294, Train-NDCG: 0.978685138378358, Test-NDCG: 0.9454265037086229\n",
      "Epoch 47/100, Loss: 1.3206031105735085, Train-NDCG: 0.9787648974270056, Test-NDCG: 0.943378538066096\n",
      "Epoch 48/100, Loss: 1.3116563314741307, Train-NDCG: 0.9794705214655873, Test-NDCG: 0.9447217387644186\n",
      "Epoch 49/100, Loss: 1.2987862039696088, Train-NDCG: 0.979824663500774, Test-NDCG: 0.9446653748083139\n",
      "Epoch 50/100, Loss: 1.2933728694915771, Train-NDCG: 0.9809033507576214, Test-NDCG: 0.9447086562728222\n",
      "Epoch 51/100, Loss: 1.2959039563482457, Train-NDCG: 0.9803381102953795, Test-NDCG: 0.9448447398424519\n",
      "Epoch 52/100, Loss: 1.3117224817926234, Train-NDCG: 0.9793142613049817, Test-NDCG: 0.9453522846794292\n",
      "Epoch 53/100, Loss: 1.296130359172821, Train-NDCG: 0.9798206002359051, Test-NDCG: 0.9443653069384429\n",
      "Epoch 54/100, Loss: 1.2963788075880571, Train-NDCG: 0.9801026960418026, Test-NDCG: 0.9474021494594803\n",
      "Epoch 55/100, Loss: 1.2953483110124415, Train-NDCG: 0.9805060790059211, Test-NDCG: 0.9474363411522478\n",
      "Epoch 56/100, Loss: 1.291361470114101, Train-NDCG: 0.980426357658165, Test-NDCG: 0.9468673946977616\n",
      "Epoch 57/100, Loss: 1.2943478389219805, Train-NDCG: 0.9814107942314586, Test-NDCG: 0.9414467178377166\n",
      "Epoch 58/100, Loss: 1.297701521353288, Train-NDCG: 0.9801352335132699, Test-NDCG: 0.9462686931322231\n",
      "Epoch 59/100, Loss: 1.3004134649580175, Train-NDCG: 0.9801437179090476, Test-NDCG: 0.9465425673587642\n",
      "Epoch 60/100, Loss: 1.2858587882735513, Train-NDCG: 0.9812061009467665, Test-NDCG: 0.946192237461804\n",
      "Epoch 61/100, Loss: 1.2888518571853638, Train-NDCG: 0.9807380003457017, Test-NDCG: 0.9493796557035875\n",
      "Epoch 62/100, Loss: 1.3038056140596217, Train-NDCG: 0.9797249483219711, Test-NDCG: 0.9467261763673254\n",
      "Epoch 63/100, Loss: 1.2924529151483015, Train-NDCG: 0.9801360297609414, Test-NDCG: 0.947133411985386\n",
      "Epoch 64/100, Loss: 1.2800658453594556, Train-NDCG: 0.9813778361519905, Test-NDCG: 0.9447880702965629\n",
      "Epoch 65/100, Loss: 1.2867788699540226, Train-NDCG: 0.9809294691819879, Test-NDCG: 0.9480454819281027\n",
      "Epoch 66/100, Loss: 1.2797695398330688, Train-NDCG: 0.9811598145359536, Test-NDCG: 0.9458299553743607\n",
      "Epoch 67/100, Loss: 1.2782545279372821, Train-NDCG: 0.9814225974555749, Test-NDCG: 0.9505681932770282\n",
      "Epoch 68/100, Loss: 1.280109551819888, Train-NDCG: 0.9812024111584621, Test-NDCG: 0.9462816919090035\n",
      "Epoch 69/100, Loss: 1.2752432227134705, Train-NDCG: 0.9815531798532592, Test-NDCG: 0.9485601329058945\n",
      "Epoch 70/100, Loss: 1.2795467322522944, Train-NDCG: 0.9817101828231976, Test-NDCG: 0.9444998258718068\n",
      "Epoch 71/100, Loss: 1.2736990262161603, Train-NDCG: 0.9814962493206972, Test-NDCG: 0.9497225191676649\n",
      "Epoch 72/100, Loss: 1.2729537026448683, Train-NDCG: 0.9813800376700775, Test-NDCG: 0.9483946991102622\n",
      "Epoch 73/100, Loss: 1.2635961445895108, Train-NDCG: 0.9825451121901362, Test-NDCG: 0.9456677039976376\n",
      "Epoch 74/100, Loss: 1.2681558999148281, Train-NDCG: 0.9821793538217509, Test-NDCG: 0.94599810610425\n",
      "Epoch 75/100, Loss: 1.269612201235511, Train-NDCG: 0.9822654545435608, Test-NDCG: 0.9472721425316957\n",
      "Epoch 76/100, Loss: 1.261290810324929, Train-NDCG: 0.9824898661227682, Test-NDCG: 0.9457628773149805\n",
      "Epoch 77/100, Loss: 1.266659671610052, Train-NDCG: 0.9825027505044907, Test-NDCG: 0.9439136950567715\n",
      "Epoch 78/100, Loss: 1.2843244157054208, Train-NDCG: 0.980701031176798, Test-NDCG: 0.9459240099111972\n",
      "Epoch 79/100, Loss: 1.2729166881604628, Train-NDCG: 0.982412414577669, Test-NDCG: 0.9470403506885294\n",
      "Epoch 80/100, Loss: 1.2628268003463745, Train-NDCG: 0.9814963595732273, Test-NDCG: 0.9478088773380189\n",
      "Epoch 81/100, Loss: 1.2679620818658308, Train-NDCG: 0.9825205110947074, Test-NDCG: 0.9471290953548757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 1.2607452029531652, Train-NDCG: 0.9827913413045635, Test-NDCG: 0.9509846825533573\n",
      "Epoch 83/100, Loss: 1.2584334178404375, Train-NDCG: 0.9825619885716069, Test-NDCG: 0.9461619843890664\n",
      "Epoch 84/100, Loss: 1.2513272735205563, Train-NDCG: 0.9836719140885125, Test-NDCG: 0.9482249059011683\n",
      "Epoch 85/100, Loss: 1.2610108716921373, Train-NDCG: 0.9827357748601429, Test-NDCG: 0.9452699317076984\n",
      "Epoch 86/100, Loss: 1.248857858506116, Train-NDCG: 0.9832784921786565, Test-NDCG: 0.9440596824694825\n",
      "Epoch 87/100, Loss: 1.2565192553130062, Train-NDCG: 0.9827596977816846, Test-NDCG: 0.9486307603705315\n",
      "Epoch 88/100, Loss: 1.2502553137865933, Train-NDCG: 0.983746744477865, Test-NDCG: 0.9438923996746865\n",
      "Epoch 89/100, Loss: 1.2570297528396954, Train-NDCG: 0.9830567351806873, Test-NDCG: 0.9456669302879939\n",
      "Epoch 90/100, Loss: 1.2549231567166068, Train-NDCG: 0.9828934476755999, Test-NDCG: 0.9466232238898447\n",
      "Epoch 91/100, Loss: 1.2541100057688626, Train-NDCG: 0.9830666315038652, Test-NDCG: 0.9484940370267337\n",
      "Epoch 92/100, Loss: 1.2451308586380698, Train-NDCG: 0.9835175305175106, Test-NDCG: 0.9494412830794321\n",
      "Epoch 93/100, Loss: 1.2535736018961126, Train-NDCG: 0.983016164678231, Test-NDCG: 0.9468912567850981\n",
      "Epoch 94/100, Loss: 1.2436121729287235, Train-NDCG: 0.9836180603585081, Test-NDCG: 0.9461054339281788\n",
      "Epoch 95/100, Loss: 1.2560596276413312, Train-NDCG: 0.9829281491493663, Test-NDCG: 0.9479186537366956\n",
      "Epoch 96/100, Loss: 1.2539236653934827, Train-NDCG: 0.9824924804297975, Test-NDCG: 0.946486745536374\n",
      "Epoch 97/100, Loss: 1.241879877719012, Train-NDCG: 0.9832997892393638, Test-NDCG: 0.9483844166638236\n",
      "Epoch 98/100, Loss: 1.2534279389814897, Train-NDCG: 0.9832302241457288, Test-NDCG: 0.9463288128882659\n",
      "Epoch 99/100, Loss: 1.243695397268642, Train-NDCG: 0.9837046985794312, Test-NDCG: 0.9454037080398413\n",
      "Epoch 100/100, Loss: 1.2409003404053776, Train-NDCG: 0.9836886803521487, Test-NDCG: 0.9451064900722133\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 7).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds] \n",
    "\n",
    "train_input, train_target = data(train_data, columns_to_normalize)\n",
    "test_input, test_target = data(test_data, columns_to_normalize)\n",
    "\n",
    "train_dataset = MyDataset(train_input, train_target)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(test_input, test_target)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the ListNet model\n",
    "input_dim = train_input.size()[2]\n",
    "hidden_dim = 512  # Define the desired number of hidden units\n",
    "output_dim = 1  # Assuming you are predicting a single value per query\n",
    "\n",
    "# Initialize the ListNet model\n",
    "model = ListNet(input_dim, hidden_dim, output_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = LambdaNDCGLoss2()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "start_time = time.time()\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader, test_dataloader, l, 100, optimizer)\n",
    "end_time = time.time()\n",
    "train_elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ad56b7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9451064900722133, 0.960133689011746, 1.0000000000000002, 0.7239038339500155)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores5a = []\n",
    "for a, b in test_dataloader:\n",
    "    test_output = model(a)\n",
    "            \n",
    "    for a1, b1 in zip(test_output, b):\n",
    "        test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "        scores5a.append(test_ndcg_score)\n",
    "np.mean(scores5a), np.median(scores5a), max(scores5a), min(scores5a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc9795",
   "metadata": {},
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e576e29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.021537160331553, Train-NDCG: 0.9285131024416796, Test-NDCG: 0.9386286789255203\n",
      "Epoch 2/100, Loss: 1.7470252188769253, Train-NDCG: 0.946223149072674, Test-NDCG: 0.9381771852338132\n",
      "Epoch 3/100, Loss: 1.704436719417572, Train-NDCG: 0.9495814287486614, Test-NDCG: 0.9468449847469074\n",
      "Epoch 4/100, Loss: 1.636535714973103, Train-NDCG: 0.9543881885571046, Test-NDCG: 0.9453400060996356\n",
      "Epoch 5/100, Loss: 1.601716472343965, Train-NDCG: 0.9564007404071805, Test-NDCG: 0.945947049329009\n",
      "Epoch 6/100, Loss: 1.6143968592990527, Train-NDCG: 0.9585266055856142, Test-NDCG: 0.9478721924993266\n",
      "Epoch 7/100, Loss: 1.574225271289999, Train-NDCG: 0.9593552096183419, Test-NDCG: 0.9476669789173421\n",
      "Epoch 8/100, Loss: 1.557261269200932, Train-NDCG: 0.9619486480285889, Test-NDCG: 0.9505282796485532\n",
      "Epoch 9/100, Loss: 1.5468098602511666, Train-NDCG: 0.9643401032727393, Test-NDCG: 0.9466037926647565\n",
      "Epoch 10/100, Loss: 1.5183034566315738, Train-NDCG: 0.964677783746234, Test-NDCG: 0.950606008041639\n",
      "Epoch 11/100, Loss: 1.5231583497741006, Train-NDCG: 0.9641729750724667, Test-NDCG: 0.947853393914841\n",
      "Epoch 12/100, Loss: 1.4808140207420697, Train-NDCG: 0.9663990104370549, Test-NDCG: 0.9502967665360404\n",
      "Epoch 13/100, Loss: 1.4800404554063624, Train-NDCG: 0.9674116282122189, Test-NDCG: 0.9508114628971932\n",
      "Epoch 14/100, Loss: 1.4709795009006152, Train-NDCG: 0.9686567304498035, Test-NDCG: 0.9498898552754713\n",
      "Epoch 15/100, Loss: 1.4643239947882565, Train-NDCG: 0.9692979989891138, Test-NDCG: 0.9489306916862243\n",
      "Epoch 16/100, Loss: 1.4397468350150369, Train-NDCG: 0.9702435418294582, Test-NDCG: 0.9507214876467813\n",
      "Epoch 17/100, Loss: 1.4341279430822893, Train-NDCG: 0.9707086414793235, Test-NDCG: 0.9517077946994735\n",
      "Epoch 18/100, Loss: 1.4399109103462913, Train-NDCG: 0.9701072658992832, Test-NDCG: 0.950467241258715\n",
      "Epoch 19/100, Loss: 1.4222527119246395, Train-NDCG: 0.972012038767111, Test-NDCG: 0.9523732915947419\n",
      "Epoch 20/100, Loss: 1.4145249290899797, Train-NDCG: 0.9717615749184126, Test-NDCG: 0.9514377900629555\n",
      "Epoch 21/100, Loss: 1.4123765501109036, Train-NDCG: 0.9719903285362098, Test-NDCG: 0.9456677013878515\n",
      "Epoch 22/100, Loss: 1.4188463579524646, Train-NDCG: 0.9728532625918681, Test-NDCG: 0.9500242804193751\n",
      "Epoch 23/100, Loss: 1.401083000681617, Train-NDCG: 0.9727820575544652, Test-NDCG: 0.9489064052320307\n",
      "Epoch 24/100, Loss: 1.3869306959889152, Train-NDCG: 0.973361211603585, Test-NDCG: 0.948730053045015\n",
      "Epoch 25/100, Loss: 1.3746683624657718, Train-NDCG: 0.9739622075939618, Test-NDCG: 0.9506854073384248\n",
      "Epoch 26/100, Loss: 1.375646634535356, Train-NDCG: 0.9738356512165919, Test-NDCG: 0.9525239337673757\n",
      "Epoch 27/100, Loss: 1.3681242303414778, Train-NDCG: 0.9755686117952607, Test-NDCG: 0.9501734171732914\n",
      "Epoch 28/100, Loss: 1.3635150465098294, Train-NDCG: 0.975842581667933, Test-NDCG: 0.9504364005842009\n",
      "Epoch 29/100, Loss: 1.3674919361417943, Train-NDCG: 0.975517396207966, Test-NDCG: 0.9494983550221859\n",
      "Epoch 30/100, Loss: 1.3647435063665563, Train-NDCG: 0.9752916638735614, Test-NDCG: 0.952645705935729\n",
      "Epoch 31/100, Loss: 1.3672153028574856, Train-NDCG: 0.9751883286467908, Test-NDCG: 0.9502401359462076\n",
      "Epoch 32/100, Loss: 1.3500106795267626, Train-NDCG: 0.9762312470138768, Test-NDCG: 0.9515225480467885\n",
      "Epoch 33/100, Loss: 1.344572590156035, Train-NDCG: 0.9762815658381669, Test-NDCG: 0.9512481036448579\n",
      "Epoch 34/100, Loss: 1.3400110412727704, Train-NDCG: 0.9768572883122378, Test-NDCG: 0.9516483267912127\n",
      "Epoch 35/100, Loss: 1.3316487019712275, Train-NDCG: 0.9772824560743782, Test-NDCG: 0.9500871747041688\n",
      "Epoch 36/100, Loss: 1.3341426388783888, Train-NDCG: 0.9773603828461878, Test-NDCG: 0.9527414872834208\n",
      "Epoch 37/100, Loss: 1.3350509919903495, Train-NDCG: 0.9766404913478482, Test-NDCG: 0.9515649797004091\n",
      "Epoch 38/100, Loss: 1.3270420025695453, Train-NDCG: 0.9778784056534136, Test-NDCG: 0.9537590315262365\n",
      "Epoch 39/100, Loss: 1.3289774466644635, Train-NDCG: 0.9775547174493873, Test-NDCG: 0.9506923049886707\n",
      "Epoch 40/100, Loss: 1.3283069377595729, Train-NDCG: 0.977093544522523, Test-NDCG: 0.9555172007068621\n",
      "Epoch 41/100, Loss: 1.3128848699006168, Train-NDCG: 0.9784192514184944, Test-NDCG: 0.9494352165323544\n",
      "Epoch 42/100, Loss: 1.3245379464192824, Train-NDCG: 0.9776958324868782, Test-NDCG: 0.9517196613705255\n",
      "Epoch 43/100, Loss: 1.3290798474441876, Train-NDCG: 0.9770105608287875, Test-NDCG: 0.9523084619417725\n",
      "Epoch 44/100, Loss: 1.312549127773805, Train-NDCG: 0.9779317683892829, Test-NDCG: 0.9532716717960026\n",
      "Epoch 45/100, Loss: 1.310451938347383, Train-NDCG: 0.9783872724813688, Test-NDCG: 0.951192847177064\n",
      "Epoch 46/100, Loss: 1.313675357536836, Train-NDCG: 0.9786364603425707, Test-NDCG: 0.9543957607938124\n",
      "Epoch 47/100, Loss: 1.318235765803944, Train-NDCG: 0.977687975572758, Test-NDCG: 0.9526953894672622\n",
      "Epoch 48/100, Loss: 1.3167072101072832, Train-NDCG: 0.9773563243283793, Test-NDCG: 0.9526307702697607\n",
      "Epoch 49/100, Loss: 1.2979476614431902, Train-NDCG: 0.9795295611760507, Test-NDCG: 0.9536897329560221\n",
      "Epoch 50/100, Loss: 1.3078200004317544, Train-NDCG: 0.9782334694045205, Test-NDCG: 0.9501071193389093\n",
      "Epoch 51/100, Loss: 1.2992598739537327, Train-NDCG: 0.9791700676187308, Test-NDCG: 0.9541751094567641\n",
      "Epoch 52/100, Loss: 1.3015791963447223, Train-NDCG: 0.97895675716808, Test-NDCG: 0.9525392531760623\n",
      "Epoch 53/100, Loss: 1.2933736606077715, Train-NDCG: 0.9786206440364903, Test-NDCG: 0.9537044644523943\n",
      "Epoch 54/100, Loss: 1.2878847826610913, Train-NDCG: 0.9805213439060686, Test-NDCG: 0.9520164126351276\n",
      "Epoch 55/100, Loss: 1.28720422224565, Train-NDCG: 0.9801278761799512, Test-NDCG: 0.952815892708047\n",
      "Epoch 56/100, Loss: 1.2861130969090895, Train-NDCG: 0.9798417387841712, Test-NDCG: 0.953189166605866\n",
      "Epoch 57/100, Loss: 1.2881790697574615, Train-NDCG: 0.9798499029840454, Test-NDCG: 0.9529768231741509\n",
      "Epoch 58/100, Loss: 1.2853093851696362, Train-NDCG: 0.9802889584433767, Test-NDCG: 0.9522784599605958\n",
      "Epoch 59/100, Loss: 1.272230866280469, Train-NDCG: 0.9802129933174636, Test-NDCG: 0.9524455607783778\n",
      "Epoch 60/100, Loss: 1.2901839126240124, Train-NDCG: 0.9791827307351272, Test-NDCG: 0.9510647807155002\n",
      "Epoch 61/100, Loss: 1.290122313932939, Train-NDCG: 0.9797638319523536, Test-NDCG: 0.9489551372113403\n",
      "Epoch 62/100, Loss: 1.2895899496295236, Train-NDCG: 0.9795278023405132, Test-NDCG: 0.9540562999812878\n",
      "Epoch 63/100, Loss: 1.289209777658636, Train-NDCG: 0.979743183655862, Test-NDCG: 0.9541042993784544\n",
      "Epoch 64/100, Loss: 1.2795588021928614, Train-NDCG: 0.9798449979155496, Test-NDCG: 0.9519533835015153\n",
      "Epoch 65/100, Loss: 1.2732966190034694, Train-NDCG: 0.9804720312458124, Test-NDCG: 0.955095520585397\n",
      "Epoch 66/100, Loss: 1.276258092034947, Train-NDCG: 0.980908547371935, Test-NDCG: 0.9503189678403913\n",
      "Epoch 67/100, Loss: 1.2728897576982325, Train-NDCG: 0.9809213275144532, Test-NDCG: 0.9524556731383641\n",
      "Epoch 68/100, Loss: 1.26625716957179, Train-NDCG: 0.9814434064453063, Test-NDCG: 0.9510189708710186\n",
      "Epoch 69/100, Loss: 1.2753525945273312, Train-NDCG: 0.9807274172865444, Test-NDCG: 0.9546739318119455\n",
      "Epoch 70/100, Loss: 1.2675741748376326, Train-NDCG: 0.9813747272811145, Test-NDCG: 0.9516778064477508\n",
      "Epoch 71/100, Loss: 1.2786222642118281, Train-NDCG: 0.9798554805904446, Test-NDCG: 0.9523799833298228\n",
      "Epoch 72/100, Loss: 1.271858361634341, Train-NDCG: 0.9810640499966923, Test-NDCG: 0.9543796494548132\n",
      "Epoch 73/100, Loss: 1.2569921070879155, Train-NDCG: 0.9822221082897907, Test-NDCG: 0.9499194968084422\n",
      "Epoch 74/100, Loss: 1.2716364589604465, Train-NDCG: 0.9810284841071564, Test-NDCG: 0.9536372981909476\n",
      "Epoch 75/100, Loss: 1.2564528557387264, Train-NDCG: 0.9814851171901097, Test-NDCG: 0.9539320157313884\n",
      "Epoch 76/100, Loss: 1.2616928409446369, Train-NDCG: 0.9815138426977036, Test-NDCG: 0.9523168670714502\n",
      "Epoch 77/100, Loss: 1.2653703391551971, Train-NDCG: 0.9817153759687294, Test-NDCG: 0.9531260467924435\n",
      "Epoch 78/100, Loss: 1.255465485832908, Train-NDCG: 0.9822407715920666, Test-NDCG: 0.9519982473492562\n",
      "Epoch 79/100, Loss: 1.2747848684137517, Train-NDCG: 0.9810837998549871, Test-NDCG: 0.9500866712620855\n",
      "Epoch 80/100, Loss: 1.261391363360665, Train-NDCG: 0.9815458117209265, Test-NDCG: 0.9531657478144965\n",
      "Epoch 81/100, Loss: 1.2768082320690155, Train-NDCG: 0.980945088376551, Test-NDCG: 0.9508320202172617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 1.2591534880074589, Train-NDCG: 0.9819586080224755, Test-NDCG: 0.9530957816397918\n",
      "Epoch 83/100, Loss: 1.2479777119376443, Train-NDCG: 0.9829004468497498, Test-NDCG: 0.9510976201535916\n",
      "Epoch 84/100, Loss: 1.2577812373638153, Train-NDCG: 0.9819857640011691, Test-NDCG: 0.9496604796823865\n",
      "Epoch 85/100, Loss: 1.2729468616572293, Train-NDCG: 0.9805890540872692, Test-NDCG: 0.9518639944286129\n",
      "Epoch 86/100, Loss: 1.2454408651048487, Train-NDCG: 0.9822870258951859, Test-NDCG: 0.9548090587132687\n",
      "Epoch 87/100, Loss: 1.2404258820143612, Train-NDCG: 0.9837286834379458, Test-NDCG: 0.9516722295703484\n",
      "Epoch 88/100, Loss: 1.2434480000625958, Train-NDCG: 0.9826858649633128, Test-NDCG: 0.9525269484086819\n",
      "Epoch 89/100, Loss: 1.2422347935763272, Train-NDCG: 0.982749651914249, Test-NDCG: 0.9524145298982191\n",
      "Epoch 90/100, Loss: 1.2309760993177241, Train-NDCG: 0.9836995751694854, Test-NDCG: 0.9531522599173158\n",
      "Epoch 91/100, Loss: 1.2455662976611743, Train-NDCG: 0.9825105791615026, Test-NDCG: 0.9496733566376577\n",
      "Epoch 92/100, Loss: 1.2584933801130815, Train-NDCG: 0.9806325235985643, Test-NDCG: 0.9504619533741078\n",
      "Epoch 93/100, Loss: 1.25964708219875, Train-NDCG: 0.9815781080086119, Test-NDCG: 0.9545416131955002\n",
      "Epoch 94/100, Loss: 1.2550108216025613, Train-NDCG: 0.9815688762656236, Test-NDCG: 0.9502758393942375\n",
      "Epoch 95/100, Loss: 1.2480345828966661, Train-NDCG: 0.9825458294162132, Test-NDCG: 0.9526081405675123\n",
      "Epoch 96/100, Loss: 1.2411319017410278, Train-NDCG: 0.983155266228627, Test-NDCG: 0.949559567799893\n",
      "Epoch 97/100, Loss: 1.2484540885145015, Train-NDCG: 0.982681374229637, Test-NDCG: 0.9522046377704041\n",
      "Epoch 98/100, Loss: 1.2381269173188643, Train-NDCG: 0.9830461229173305, Test-NDCG: 0.9536167431935335\n",
      "Epoch 99/100, Loss: 1.2387965402819894, Train-NDCG: 0.9836254905726732, Test-NDCG: 0.9526945538209641\n",
      "Epoch 100/100, Loss: 1.2293798327445984, Train-NDCG: 0.9835933619214849, Test-NDCG: 0.9535947229567664\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 1).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds] \n",
    "\n",
    "train_input, train_target = data(train_data, columns_to_normalize)\n",
    "test_input, test_target = data(test_data, columns_to_normalize)\n",
    "\n",
    "train_dataset = MyDataset(train_input, train_target)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(test_input, test_target)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the ListNet model\n",
    "input_dim = train_input.size()[2]\n",
    "hidden_dim = 512  # Define the desired number of hidden units\n",
    "output_dim = 1  # Assuming you are predicting a single value per query\n",
    "\n",
    "# Initialize the ListNet model\n",
    "model = ListNet(input_dim, hidden_dim, output_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = LambdaNDCGLoss2()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "start_time = time.time()\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader, test_dataloader, l, 100, optimizer)\n",
    "end_time = time.time()\n",
    "train_elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7d7e6955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9535947229567664,\n",
       " 0.9658403978326453,\n",
       " 1.0000000000000002,\n",
       " 0.7772159205974075)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores5b = []\n",
    "for a, b in test_dataloader:\n",
    "    test_output = model(a)\n",
    "            \n",
    "    for a1, b1 in zip(test_output, b):\n",
    "        test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "        scores5b.append(test_ndcg_score)\n",
    "np.mean(scores5b), np.median(scores5b), max(scores5b), min(scores5b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8a780",
   "metadata": {},
   "source": [
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "21ef95be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.085164725780487, Train-NDCG: 0.9277933408890054, Test-NDCG: 0.9346512157479082\n",
      "Epoch 2/100, Loss: 1.740451157093048, Train-NDCG: 0.9471998883195208, Test-NDCG: 0.939091229685024\n",
      "Epoch 3/100, Loss: 1.6494442495432766, Train-NDCG: 0.9530045221475615, Test-NDCG: 0.9419089236966269\n",
      "Epoch 4/100, Loss: 1.6214351816610857, Train-NDCG: 0.9563177265311912, Test-NDCG: 0.9436014235566949\n",
      "Epoch 5/100, Loss: 1.5843379768458279, Train-NDCG: 0.9592281470837226, Test-NDCG: 0.9452767484880683\n",
      "Epoch 6/100, Loss: 1.5675807080485604, Train-NDCG: 0.9603399816905984, Test-NDCG: 0.9427795561260282\n",
      "Epoch 7/100, Loss: 1.5712521997365085, Train-NDCG: 0.9601164771972018, Test-NDCG: 0.9465833590466102\n",
      "Epoch 8/100, Loss: 1.525160093199123, Train-NDCG: 0.9623877704660221, Test-NDCG: 0.9458804818643824\n",
      "Epoch 9/100, Loss: 1.4991894283077933, Train-NDCG: 0.9651645348609186, Test-NDCG: 0.94729809882308\n",
      "Epoch 10/100, Loss: 1.4860282350670209, Train-NDCG: 0.9675878806596856, Test-NDCG: 0.9474449644033516\n",
      "Epoch 11/100, Loss: 1.497316387566653, Train-NDCG: 0.9662779880236574, Test-NDCG: 0.9489513198305558\n",
      "Epoch 12/100, Loss: 1.4709792679006404, Train-NDCG: 0.967197210127279, Test-NDCG: 0.9457496040084791\n",
      "Epoch 13/100, Loss: 1.4566460224715145, Train-NDCG: 0.9684399140673718, Test-NDCG: 0.951331615724609\n",
      "Epoch 14/100, Loss: 1.4557549005204982, Train-NDCG: 0.968273016414631, Test-NDCG: 0.9494091350646793\n",
      "Epoch 15/100, Loss: 1.4419254470955243, Train-NDCG: 0.9705927545582455, Test-NDCG: 0.9456199423239712\n",
      "Epoch 16/100, Loss: 1.4328061938285828, Train-NDCG: 0.9704745445571263, Test-NDCG: 0.9506114594192423\n",
      "Epoch 17/100, Loss: 1.4125622023235669, Train-NDCG: 0.9711708285006118, Test-NDCG: 0.9481888994325437\n",
      "Epoch 18/100, Loss: 1.4103140207854183, Train-NDCG: 0.9716127534135285, Test-NDCG: 0.949997469552107\n",
      "Epoch 19/100, Loss: 1.422910674051805, Train-NDCG: 0.9705370142813259, Test-NDCG: 0.9480350421643547\n",
      "Epoch 20/100, Loss: 1.4077861634167759, Train-NDCG: 0.9720056421956239, Test-NDCG: 0.9480670616964515\n",
      "Epoch 21/100, Loss: 1.4098255065354435, Train-NDCG: 0.9701196055633994, Test-NDCG: 0.9508206316949154\n",
      "Epoch 22/100, Loss: 1.3846211623061786, Train-NDCG: 0.9734239770068328, Test-NDCG: 0.9467201175868653\n",
      "Epoch 23/100, Loss: 1.3922329301183873, Train-NDCG: 0.9730902007184852, Test-NDCG: 0.9464848782395198\n",
      "Epoch 24/100, Loss: 1.3822237334468148, Train-NDCG: 0.9737110037686435, Test-NDCG: 0.9497930604726853\n",
      "Epoch 25/100, Loss: 1.3673479069362988, Train-NDCG: 0.9743262397949508, Test-NDCG: 0.9489983587422116\n",
      "Epoch 26/100, Loss: 1.3761286654255607, Train-NDCG: 0.9735993379288466, Test-NDCG: 0.9482910810565808\n",
      "Epoch 27/100, Loss: 1.35880293358456, Train-NDCG: 0.9753097221154742, Test-NDCG: 0.9492675369351599\n",
      "Epoch 28/100, Loss: 1.351511299610138, Train-NDCG: 0.9755529623275123, Test-NDCG: 0.9501823450740798\n",
      "Epoch 29/100, Loss: 1.3585320982066067, Train-NDCG: 0.9749675533727842, Test-NDCG: 0.9487405199462094\n",
      "Epoch 30/100, Loss: 1.353396938605742, Train-NDCG: 0.9756344639555485, Test-NDCG: 0.9498672969202352\n",
      "Epoch 31/100, Loss: 1.3370965068990535, Train-NDCG: 0.9765129691887604, Test-NDCG: 0.9515121824142755\n",
      "Epoch 32/100, Loss: 1.3639160990715027, Train-NDCG: 0.9735172485229717, Test-NDCG: 0.9483245168097629\n",
      "Epoch 33/100, Loss: 1.3645374558188699, Train-NDCG: 0.9742969237430771, Test-NDCG: 0.9505292321151564\n",
      "Epoch 34/100, Loss: 1.3389306203885512, Train-NDCG: 0.9761769455105881, Test-NDCG: 0.9498040828172437\n",
      "Epoch 35/100, Loss: 1.3368971943855286, Train-NDCG: 0.9760680792521618, Test-NDCG: 0.9499544393753065\n",
      "Epoch 36/100, Loss: 1.3265386034141888, Train-NDCG: 0.977246365645394, Test-NDCG: 0.9509532268691607\n",
      "Epoch 37/100, Loss: 1.337375513531945, Train-NDCG: 0.9764848152915054, Test-NDCG: 0.9474728761905562\n",
      "Epoch 38/100, Loss: 1.3081325075843118, Train-NDCG: 0.9774269783418125, Test-NDCG: 0.9528260267047619\n",
      "Epoch 39/100, Loss: 1.3248967094854875, Train-NDCG: 0.9761233278280715, Test-NDCG: 0.9492943145286042\n",
      "Epoch 40/100, Loss: 1.3294980444691398, Train-NDCG: 0.9762854192274876, Test-NDCG: 0.9490310341689825\n",
      "Epoch 41/100, Loss: 1.3181520971384915, Train-NDCG: 0.977496598631467, Test-NDCG: 0.9493229967124053\n",
      "Epoch 42/100, Loss: 1.3153764524243095, Train-NDCG: 0.977488619672476, Test-NDCG: 0.9474623453447633\n",
      "Epoch 43/100, Loss: 1.3151254708116704, Train-NDCG: 0.9781409739389076, Test-NDCG: 0.9516233407677986\n",
      "Epoch 44/100, Loss: 1.3125143918124111, Train-NDCG: 0.9776841789362308, Test-NDCG: 0.950012863537827\n",
      "Epoch 45/100, Loss: 1.3178820230744102, Train-NDCG: 0.9775499466669314, Test-NDCG: 0.9479190988948912\n",
      "Epoch 46/100, Loss: 1.3085617531429639, Train-NDCG: 0.97775410859174, Test-NDCG: 0.945454737776948\n",
      "Epoch 47/100, Loss: 1.2994891107082367, Train-NDCG: 0.9788706401702506, Test-NDCG: 0.9485514666884374\n",
      "Epoch 48/100, Loss: 1.2976973327723416, Train-NDCG: 0.9785952075387527, Test-NDCG: 0.9505422001183399\n",
      "Epoch 49/100, Loss: 1.3000834611329166, Train-NDCG: 0.9782618204775457, Test-NDCG: 0.9483638999402692\n",
      "Epoch 50/100, Loss: 1.2925853349945762, Train-NDCG: 0.9789137072924887, Test-NDCG: 0.9496249867662512\n",
      "Epoch 51/100, Loss: 1.3173998648470098, Train-NDCG: 0.9773247569244465, Test-NDCG: 0.948786776382491\n",
      "Epoch 52/100, Loss: 1.2966751158237457, Train-NDCG: 0.9785645702193219, Test-NDCG: 0.947461332607117\n",
      "Epoch 53/100, Loss: 1.2983064218000933, Train-NDCG: 0.9792180766538288, Test-NDCG: 0.9487362902197072\n",
      "Epoch 54/100, Loss: 1.2824049835855311, Train-NDCG: 0.9797516008060576, Test-NDCG: 0.9527962971404675\n",
      "Epoch 55/100, Loss: 1.2817913808605887, Train-NDCG: 0.9798271549445855, Test-NDCG: 0.9504869396439228\n",
      "Epoch 56/100, Loss: 1.292775267904455, Train-NDCG: 0.9798498040537124, Test-NDCG: 0.9484427151846319\n",
      "Epoch 57/100, Loss: 1.2968499741771005, Train-NDCG: 0.979222975546231, Test-NDCG: 0.9505598212440919\n",
      "Epoch 58/100, Loss: 1.2968103343790227, Train-NDCG: 0.9787459114335003, Test-NDCG: 0.9497056441214644\n",
      "Epoch 59/100, Loss: 1.2894020216031508, Train-NDCG: 0.9793374657334628, Test-NDCG: 0.9475409349755897\n",
      "Epoch 60/100, Loss: 1.296730493957346, Train-NDCG: 0.9797291985928441, Test-NDCG: 0.949930276959578\n",
      "Epoch 61/100, Loss: 1.2829274480993098, Train-NDCG: 0.9797327374990864, Test-NDCG: 0.9515987422213411\n",
      "Epoch 62/100, Loss: 1.2728592482480137, Train-NDCG: 0.9806661321344321, Test-NDCG: 0.9488608322065747\n",
      "Epoch 63/100, Loss: 1.2712594866752625, Train-NDCG: 0.9800674929443735, Test-NDCG: 0.9521236500460734\n",
      "Epoch 64/100, Loss: 1.2719974328171124, Train-NDCG: 0.9804428495733368, Test-NDCG: 0.95083881050989\n",
      "Epoch 65/100, Loss: 1.2816916568712755, Train-NDCG: 0.9801943141481886, Test-NDCG: 0.9474142404531789\n",
      "Epoch 66/100, Loss: 1.2820454862984745, Train-NDCG: 0.9808564829920368, Test-NDCG: 0.9493140600669666\n",
      "Epoch 67/100, Loss: 1.2717406289143995, Train-NDCG: 0.9803586541285316, Test-NDCG: 0.9513660959407304\n",
      "Epoch 68/100, Loss: 1.2639154412529685, Train-NDCG: 0.9810179145136135, Test-NDCG: 0.9493057762590338\n",
      "Epoch 69/100, Loss: 1.2634131854230708, Train-NDCG: 0.9819507641043697, Test-NDCG: 0.9473167327872511\n",
      "Epoch 70/100, Loss: 1.2507387535138563, Train-NDCG: 0.9819437924578335, Test-NDCG: 0.9509986339612804\n",
      "Epoch 71/100, Loss: 1.265862456776879, Train-NDCG: 0.98126509913144, Test-NDCG: 0.9490642373202689\n",
      "Epoch 72/100, Loss: 1.2499839609319514, Train-NDCG: 0.9821501320043371, Test-NDCG: 0.949984953415931\n",
      "Epoch 73/100, Loss: 1.259873547337272, Train-NDCG: 0.9813751916265353, Test-NDCG: 0.9502326024545255\n",
      "Epoch 74/100, Loss: 1.2645949206568978, Train-NDCG: 0.9809361768986676, Test-NDCG: 0.952705456273309\n",
      "Epoch 75/100, Loss: 1.2638983536850323, Train-NDCG: 0.9815324036517903, Test-NDCG: 0.9512603794736083\n",
      "Epoch 76/100, Loss: 1.2582859315655448, Train-NDCG: 0.9816079888572808, Test-NDCG: 0.9498429533474414\n",
      "Epoch 77/100, Loss: 1.2504097602584145, Train-NDCG: 0.9820317337060177, Test-NDCG: 0.9499199303906602\n",
      "Epoch 78/100, Loss: 1.249835044145584, Train-NDCG: 0.9820343126272839, Test-NDCG: 0.9495533237044395\n",
      "Epoch 79/100, Loss: 1.2525401630184867, Train-NDCG: 0.9816969342188506, Test-NDCG: 0.9461623388603949\n",
      "Epoch 80/100, Loss: 1.249832421541214, Train-NDCG: 0.9812555657491007, Test-NDCG: 0.9515212130447553\n",
      "Epoch 81/100, Loss: 1.2511087412183934, Train-NDCG: 0.981657311983908, Test-NDCG: 0.9497122476750735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 1.25040802359581, Train-NDCG: 0.9814335472760923, Test-NDCG: 0.9519243445848546\n",
      "Epoch 83/100, Loss: 1.2575935829769482, Train-NDCG: 0.9817835170123966, Test-NDCG: 0.9509923873424232\n",
      "Epoch 84/100, Loss: 1.2700668898495762, Train-NDCG: 0.9807809851066474, Test-NDCG: 0.9511970714696653\n",
      "Epoch 85/100, Loss: 1.251300036907196, Train-NDCG: 0.9816635510007728, Test-NDCG: 0.9499667102104217\n",
      "Epoch 86/100, Loss: 1.2477519810199738, Train-NDCG: 0.9822163338365755, Test-NDCG: 0.9522729876688039\n",
      "Epoch 87/100, Loss: 1.24115681106394, Train-NDCG: 0.9826957360374228, Test-NDCG: 0.9496351167776982\n",
      "Epoch 88/100, Loss: 1.2504365091974086, Train-NDCG: 0.9820065753249204, Test-NDCG: 0.9499337054772858\n",
      "Epoch 89/100, Loss: 1.2602181841026654, Train-NDCG: 0.9819605035247201, Test-NDCG: 0.9534840055348093\n",
      "Epoch 90/100, Loss: 1.2377256398851222, Train-NDCG: 0.982892367847064, Test-NDCG: 0.9499236826850389\n",
      "Epoch 91/100, Loss: 1.2480185546658256, Train-NDCG: 0.981988963118363, Test-NDCG: 0.9504218561316021\n",
      "Epoch 92/100, Loss: 1.247216593135487, Train-NDCG: 0.983046179392258, Test-NDCG: 0.950443487585528\n",
      "Epoch 93/100, Loss: 1.2453265596519818, Train-NDCG: 0.9820155968446087, Test-NDCG: 0.948605503812905\n",
      "Epoch 94/100, Loss: 1.2405378385023638, Train-NDCG: 0.9825860522723218, Test-NDCG: 0.9525202171428143\n",
      "Epoch 95/100, Loss: 1.2342269176786596, Train-NDCG: 0.9832023499323488, Test-NDCG: 0.9497693514407646\n",
      "Epoch 96/100, Loss: 1.2350637425075879, Train-NDCG: 0.9832158549190058, Test-NDCG: 0.9500589944242152\n",
      "Epoch 97/100, Loss: 1.2351005185734143, Train-NDCG: 0.9828689163429949, Test-NDCG: 0.9500529073508801\n",
      "Epoch 98/100, Loss: 1.234527823599902, Train-NDCG: 0.9828237653807634, Test-NDCG: 0.9498772301889478\n",
      "Epoch 99/100, Loss: 1.240875013849952, Train-NDCG: 0.9831215101595614, Test-NDCG: 0.9473006567414697\n",
      "Epoch 100/100, Loss: 1.2412353266369214, Train-NDCG: 0.9821789619693093, Test-NDCG: 0.9501324346384403\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state = 5).split(pointwise_data, groups=pointwise_data['keyword'])\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "train_data= pointwise_data.iloc[X_train_inds]\n",
    "test_data= pointwise_data.iloc[X_test_inds] \n",
    "\n",
    "train_input, train_target = data(train_data, columns_to_normalize)\n",
    "test_input, test_target = data(test_data, columns_to_normalize)\n",
    "\n",
    "train_dataset = MyDataset(train_input, train_target)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(test_input, test_target)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the ListNet model\n",
    "input_dim = train_input.size()[2]\n",
    "hidden_dim = 512  # Define the desired number of hidden units\n",
    "output_dim = 1  # Assuming you are predicting a single value per query\n",
    "\n",
    "# Initialize the ListNet model\n",
    "model = ListNet(input_dim, hidden_dim, output_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "#Loss function\n",
    "l = LambdaNDCGLoss2()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#Train model\n",
    "start_time = time.time()\n",
    "loss, train_ndcg, test_ndcg = train_ltr(model, train_dataloader, test_dataloader, l, 100, optimizer)\n",
    "end_time = time.time()\n",
    "train_elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f6312375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9501324346384403, 0.9609885287749416, 1.0, 0.7646234647833257)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores5c = []\n",
    "for a, b in test_dataloader:\n",
    "    test_output = model(a)\n",
    "            \n",
    "    for a1, b1 in zip(test_output, b):\n",
    "        test_ndcg_score = nd(np.asarray([b1.flatten().tolist()]), np.asarray([a1.flatten().tolist()]))\n",
    "        scores5c.append(test_ndcg_score)\n",
    "np.mean(scores5c), np.median(scores5c), max(scores5c), min(scores5c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607a4a5",
   "metadata": {},
   "source": [
    "## Model Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5eda1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "seed1=[scores1b, score2b, scores3b, scores4b, scores5b]\n",
    "seed5=[scores1c, score2c, scores3c, scores4c, scores5c]\n",
    "seed7=[scores1a, score2a, scores3a, scores4a, scores5a]\n",
    "ticks = ['Pointwise', 'RankNet', 'LambdaRank', 'LambdaMART', 'ListNet']\n",
    "\n",
    "mean = {\"color\":\"white\", 'linewidth':1.5}\n",
    "\n",
    "seed1_plot = plt.boxplot(seed1, positions=np.array(np.arange(len(seed1)))*2.0-0.35, widths=0.3, showfliers=False, showmeans = True, notch=True, patch_artist=True, meanline=True, meanprops=mean)\n",
    "seed5_plot = plt.boxplot(seed5, positions=np.array(np.arange(len(seed5)))*2.0, widths=0.3, showfliers=False, showmeans = True, notch=True, patch_artist=True, meanline=True, meanprops=mean)\n",
    "seed7_plot = plt.boxplot(seed7, positions=np.array(np.arange(len(seed7)))*2.0+0.35, widths=0.3, showfliers=False, showmeans = True, notch=True, patch_artist=True, meanline=True, meanprops=mean)\n",
    "\n",
    "for box in seed1_plot['boxes']:\n",
    "    box.set(hatch = '\\\\')\n",
    "for box in seed5_plot['boxes']:\n",
    "    box.set(hatch = 'o')\n",
    "for box in seed7_plot['boxes']:\n",
    "    box.set(hatch = '*')\n",
    "    \n",
    "label1 = mpatches.Patch(hatch='\\\\',label='randomseed=1')\n",
    "label2= mpatches.Patch(hatch='o',label='randomseed=5')\n",
    "label3 = mpatches.Patch(hatch='*',label='randomseed=7')\n",
    "\n",
    "def define_box_properties(plot_name, color_code, label):\n",
    "    for k, v in plot_name.items():\n",
    "        plt.setp(plot_name.get(k), color=color_code)\n",
    "         \n",
    "    # use plot function to draw a small line to name the legend.\n",
    "    plt.plot([], c=color_code, label=label)\n",
    "    plt.legend()\n",
    "\n",
    "# define_box_properties(seed1_plot, '#D7191C', 'Seed1')\n",
    "# define_box_properties(seed5_plot, '#2C7BB6', 'Seed5')\n",
    "# define_box_properties(seed7_plot, '#2E7FF5', 'Seed7')\n",
    "\n",
    "plt.xticks(np.arange(0, len(ticks) * 2, 2), ticks)\n",
    "plt.ylabel('NDCG')\n",
    "plt.grid()\n",
    "legend = plt.legend(handles = [label1, label2, label3], handlelength=4, labelspacing=1.5)\n",
    "for patch in legend.get_patches():\n",
    "    patch.set_height(15)\n",
    "    patch.set_y(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f274b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
