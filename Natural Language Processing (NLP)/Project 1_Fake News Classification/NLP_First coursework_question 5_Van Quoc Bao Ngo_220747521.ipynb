{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab619297",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d2077489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407addc",
   "metadata": {},
   "source": [
    "## First, reuse all the created function in question 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d0966c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path, encoding='utf-8') as f: #add 'encoding = utf-8' to original load_data function to avoid loading error\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text)),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "939785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    \"\"\"Converts the multiple classes into two,\n",
    "    making it a binary distinction between fake news and real.\"\"\"\n",
    "    #return label\n",
    "    # Converting the multiclass labels to binary label\n",
    "    labels_map = {\n",
    "        'true': 'REAL',\n",
    "        'mostly-true': 'REAL',\n",
    "        'half-true': 'REAL',\n",
    "        'false': 'FAKE',\n",
    "        'barely-true': 'FAKE',\n",
    "        'pants-fire': 'FAKE'\n",
    "    }\n",
    "    return labels_map[label]\n",
    "\n",
    "\n",
    "def parse_data_line(data_line):\n",
    "    # Should return a tuple of the label as just FAKE or REAL and the statement\n",
    "    # e.g. (label, statement)\n",
    "    \n",
    "    # the function input (data_line) should be a list, then subset the list with index 1 to get the 'label' data\n",
    "    label = convert_label(data_line[1])\n",
    "    # the same approach can be applied to extract 'text' data with index 2\n",
    "    text = data_line[2]\n",
    "    return (label, text) #return the tuple (label, text), which has the same order as in the 'load_data' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2043a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    feature_dict = {} # Created an empty dictionary (local feature dict) to store the vocab\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    for i in tokens: # Loop the input tokens. \n",
    "        #For each token, look up in the global dictionary to see if it exists\n",
    "        if i not in global_feature_dict: # If no, then assign value 1 for that token and add it to the global dictionary\n",
    "            global_feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the global dictionary\n",
    "            global_feature_dict[i] = float(global_feature_dict[i] + 1)\n",
    "        #For each token, look up in the feature dictionary to see if it exists (same approach as above)\n",
    "        if i not in feature_dict:  # If no, then assign value 1 for that token and add it to the feature dictionary\n",
    "            feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the feature dictionary\n",
    "            feature_dict[i] = float(feature_dict[i] + 1)\n",
    "    return feature_dict # Return the feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f209487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "def train_classifier(data):\n",
    "#     print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7de7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validate ==> remove shuffle to make the result consistent for comparison\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from random import shuffle # Import shuffle method\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "    #shuffle(dataset) # Shuffle the input dataset\n",
    "    results = [] # Create an empty list to store the result after running the function\n",
    "    fold_size = int(len(dataset)/folds) + 1 # Set number of folds to run\n",
    "    accuracy_rate = [] # Create list to store accuracy\n",
    "    for i in range(0,len(dataset),int(fold_size)):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        #print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "        # FILL IN THE METHOD HERE\n",
    "        # Set up validation dataset in cross-validation with its size being equal to 1 fold_size\n",
    "        validation = dataset[i : i+fold_size]\n",
    "        # Set up trainset in cross-validation by excluding the validation set from the input dataset, and take the rest\n",
    "        train = dataset[:i] + dataset[i+fold_size:]\n",
    "        # Train the classifier with the pipeline function created with the model initialiser (LinearSVC in this case)\n",
    "        classifier = train_classifier(train)\n",
    "        # Extract the ground-truth labels from the validation dataset\n",
    "        yval_true = [t[1] for t in validation]\n",
    "        #Â Implement the trained model into validation set and get predicted labels\n",
    "        yval_pred = predict_labels([x[0] for x in validation], classifier)\n",
    "        # Calculate precision, recall and fscore from the prediction, compared to ground truth\n",
    "        final_scores = precision_recall_fscore_support(yval_true, yval_pred, average='weighted', zero_division=0) # evaluate\n",
    "        # Calculate model accuracy\n",
    "        accuracy = accuracy_calculate(yval_true, yval_pred)\n",
    "        # Append the value to the list created at the top for storage\n",
    "        results.append(final_scores)\n",
    "        accuracy_rate.append(accuracy)\n",
    "    # Convert list 'results' to array for easy further calculation\n",
    "    cv_results = np.asarray(results)\n",
    "    # Calculate the average precision score after k-fold time running\n",
    "    avg_precision = np.mean(cv_results[:, 0], axis = 0)\n",
    "    # Calculate the average recall score after k-fold time running\n",
    "    avg_recall = np.mean(cv_results[:, 1], axis = 0)\n",
    "    # Calculate the average f1 score after k-fold time running\n",
    "    avg_fscore = np.mean(cv_results[:, 2], axis = 0)\n",
    "    # Calculate the average accuracy after k-fold time running\n",
    "    avg_accuracy = np.mean(accuracy_rate)\n",
    "    print('\\n')\n",
    "    print('After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: ')\n",
    "    return avg_precision, avg_recall, avg_fscore, avg_accuracy #return all the values needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f25fc170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b729501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model accuracy given a classifier\n",
    "def accuracy_calculate(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for a, b in zip(y_true, y_pred):\n",
    "        if a==b: correct += 1\n",
    "        else: pass\n",
    "    return correct/len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20671a8e",
   "metadata": {},
   "source": [
    "# Test different tokenizing method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb8bea",
   "metadata": {},
   "source": [
    "## Original method from previous questions (used for benchmarking): use string split (same func and operations performed in question 1 - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aaf01f8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 10240 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 10240 rawData, 8192 trainData, 2048 testData\n",
      "Training Samples: \n",
      "8192\n",
      "Features: \n",
      "21678\n",
      "\n",
      "\n",
      "Cross validation result  with new tokenizing method using word_tokenize from nltk package\n",
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5684319703304088, 0.5683827946653851, 0.5681037779509728, 0.568382794665385)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = text.split() # string split method used for tokenizing\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    return tokens_list # Return the token_list for further use\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n",
    "print('\\n')\n",
    "print('Cross validation result  with new tokenizing method using word_tokenize from nltk package')\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca26992",
   "metadata": {},
   "source": [
    "## Modify pre_process function: use word_tokenize method from nltk package, instead of str split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff093b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5646281013006186,\n",
       " 0.5641241139012376,\n",
       " 0.5641779103549835,\n",
       " 0.5641241139012376)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []   \n",
    "\n",
    "# Use a word_tokenize method to tokenize the text into a list of tokens\n",
    "# Import word_tokenize method from nltk package\n",
    "from nltk.tokenize import word_tokenize\n",
    "def pre_process(text):\n",
    "    tokens_list = [] # Create an empty token list\n",
    "    token = word_tokenize(text) # Tokenize the input text\n",
    "    for i in token: # Loop each value throughout the token \n",
    "        tokens_list.append(i) # Append the value into the token list created at the beginning\n",
    "    return tokens_list # Return the token_list for further use\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4e294e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "434091b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " 'The',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Veterans',\n",
       " 'Affairs',\n",
       " 'has',\n",
       " 'a',\n",
       " 'manual',\n",
       " 'out',\n",
       " 'there',\n",
       " 'telling',\n",
       " 'our',\n",
       " 'veterans',\n",
       " 'stuff',\n",
       " 'like',\n",
       " '',\n",
       " '',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'really',\n",
       " 'of',\n",
       " 'value',\n",
       " 'to',\n",
       " 'your',\n",
       " 'community',\n",
       " '',\n",
       " '',\n",
       " 'You',\n",
       " 'know',\n",
       " '',\n",
       " 'encouraging',\n",
       " 'them',\n",
       " 'to',\n",
       " 'commit',\n",
       " 'suicide',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'FAKE',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list = []\n",
    "for i in raw_data:\n",
    "    token = re.compile(r'\\W').split(str(i))\n",
    "#     a = re.split(token, str(i))\n",
    "#     for a in token:\n",
    "#         if a == '': \n",
    "#             continue\n",
    "#         else:\n",
    "#             tokens_list.append(a)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "02f64634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6076188402394452, 0.6128210981617206, 0.604708478073426, 0.6128210981617206)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []   \n",
    "\n",
    "# Use a word_tokenize method to tokenize the text into a list of tokens\n",
    "# Import word_tokenize method from nltk package\n",
    "from nltk.tokenize import word_tokenize\n",
    "def pre_process(text):\n",
    "    tokens_list = [] # Create an empty token list\n",
    "    token = re.compile(r'\\W').split(str(text)) # Tokenize the input text\n",
    "    for i in token: # Loop each value throughout the token \n",
    "        if i == '':\n",
    "            continue\n",
    "        else:\n",
    "            tokens_list.append(i) # Append the value into the token list created at the beginning\n",
    "    return tokens_list # Return the token_list for further use\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0965961",
   "metadata": {},
   "source": [
    "**With word_token method used, the performance is not improved, so we skip this modification, still go with the traditional str.split() and proceed next one**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe410d93",
   "metadata": {},
   "source": [
    "## Tokenizing by str split and remove non-text feature (punctuation, special characters, digits, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fc762a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.560150056869122, 0.5609365613360567, 0.5600316778462293, 0.5609365613360567)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "def pre_process(text):\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        if i.isalpha(): # check if a string only contain texts\n",
    "            tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac3d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91116140",
   "metadata": {},
   "source": [
    "**Removing non-text feature does not improve score, so we will skip this**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68420f",
   "metadata": {},
   "source": [
    "## Tokenizing by str split and remove stopwords (too common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a3df7b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5643471719646004,\n",
       " 0.5641048900636789,\n",
       " 0.5639305289758509,\n",
       " 0.5641048900636789)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    stopword = set(stopwords.words('english')) # Store stopwords for further check\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        if i.lower() not in stopword: # check if token is within stopword, if not, then add as features\n",
    "            tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b1418",
   "metadata": {},
   "source": [
    "**With stopwords removal, the performance does not remove, so we will not keep this**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782c459",
   "metadata": {},
   "source": [
    "## Tokenizing by str split + normalise words (lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "461cd848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5696076277893829, 0.569471945212063, 0.569173100600496, 0.569471945212063)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23e3c4",
   "metadata": {},
   "source": [
    "**Normalization improves the model performance, incorporate this into pre-processing stage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb0067",
   "metadata": {},
   "source": [
    "## Tokenizing by str split and normalization + lemmatise words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1b59195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5680802033679234,\n",
       " 0.5682836717529736,\n",
       " 0.5678002929891937,\n",
       " 0.5682836717529737)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    lemmatizer = WordNetLemmatizer() # Initialize lemmatizer\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        i = lemmatizer.lemmatize(i) # Lemmatize\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a803c5e",
   "metadata": {},
   "source": [
    "**Lemmatization does not improve score, hence not including it in the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88429648",
   "metadata": {},
   "source": [
    "## Tokenizing by str split and normalization + stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abea9fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5658962877881699,\n",
       " 0.5645914934518803,\n",
       " 0.5649028886590568,\n",
       " 0.5645914934518803)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    stem = PorterStemmer() # Initialize stemmer\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        i = stem.stem(i) # Stem\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51c3a0",
   "metadata": {},
   "source": [
    "**Stemming does not improve score, hence not including it in the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50c2c5",
   "metadata": {},
   "source": [
    "## Try to incorporate everything method above: Tokenizing by str split and remove stopwords (too common words) + normalise/lemmatise/stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9cbbdc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5616198497963472,\n",
       " 0.5606902559173376,\n",
       " 0.5607683748550507,\n",
       " 0.5606902559173376)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    stem = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        if i.lower() not in stop_word:\n",
    "            i = i.lower()\n",
    "            i = stem.stem(i)\n",
    "            i = lemmatizer.lemmatize(i)\n",
    "            tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e516d2",
   "metadata": {},
   "source": [
    "**With all method included, the performance of the model still does not improve, hence, the most optimal preprocessing method up to now is:**\n",
    "- *Tokenizing by the traditional str.split(), using any whitespace as separator (default setting)*\n",
    "- *Implementing normalization, converting all tokens into lowercase form*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069993d",
   "metadata": {},
   "source": [
    "# LinearSVC tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87d253",
   "metadata": {},
   "source": [
    "## Go with optimal tokenizing method + Tune the regularizing cost model hyperparameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b7ac796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter setting: LinearSVC(C=0.01, max_iter=100000).\n",
      "Average accuracy across folds of best hyperparameter setting: 0.6058343012001549.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-10, 0.25, 0.5, 1, 2, 2.5, 5, 10, 20, 25, 30, 50, 100]}\n",
    "svc =  LinearSVC(max_iter=100000) #set max_iter to avoid converge warning issue\n",
    "svc_cv = GridSearchCV(svc, parameters, cv = 10) #run GridSearchCV with cv = 10\n",
    "# train_data_X = [t[0] for t in train_data] # Extract feature from train_data\n",
    "# train_data_y = [t[1] for t in train_data] # Extract true labels from train_data\n",
    "# svc_cv.fit(train_data_X, train_data_y)\n",
    "SklearnClassifier(svc_cv).train(train_data)\n",
    "print('Best hyperparameter setting: {0}.'.format(svc_cv.best_estimator_))\n",
    "print('Average accuracy across folds of best hyperparameter setting: {0}.'.format(svc_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45c0fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter setting: LinearSVC(C=0.007964824120603023, max_iter=100000).\n",
      "Average accuracy across folds of best hyperparameter setting: 0.608519461568242.\n"
     ]
    }
   ],
   "source": [
    "# Try again with more detailed C within a suitable range\n",
    "\n",
    "parameters = {'C': np.linspace(1e-1, 1e-3, 200).tolist()} # using numpy.linspace to create value list with smaller, detailed values\n",
    "svc =  LinearSVC(max_iter=100000)\n",
    "svc_cv = GridSearchCV(svc, parameters, cv = 10)\n",
    "# train_data_X = [t[0] for t in train_data] # Extract feature from train_data\n",
    "# train_data_y = [t[1] for t in train_data] # Extract true labels from train_data\n",
    "# svc_cv.fit(train_data_X, train_data_y)\n",
    "SklearnClassifier(svc_cv).train(train_data)\n",
    "print('Best hyperparameter setting: {0}.'.format(svc_cv.best_estimator_))\n",
    "print('Average accuracy across folds of best hyperparameter setting: {0}.'.format(svc_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19908983",
   "metadata": {},
   "source": [
    "**With optimal C hyperparameter, the model has been significant improved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3605d0f",
   "metadata": {},
   "source": [
    "## Go with optimal tokenizing method + Use the optimal model hyperparameter C + set class_weight hyperparameter to 'balanced' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a11f2a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter setting: LinearSVC(C=0.007964824120603023, class_weight='balanced', max_iter=100000).\n",
      "Average accuracy across folds of best hyperparameter setting: 0.5996091307066916.\n"
     ]
    }
   ],
   "source": [
    "# Try with balanced class _weight\n",
    "\n",
    "parameters = {'C': [0.007964824120603023], 'max_iter': [100000]}\n",
    "svc =  LinearSVC(class_weight='balanced')\n",
    "svc_cv = GridSearchCV(svc, parameters, cv = 10)\n",
    "# train_data_X = [t[0] for t in train_data] # Extract feature from train_data\n",
    "# train_data_y = [t[1] for t in train_data] # Extract true labels from train_data\n",
    "# svc_cv.fit(train_data_X, train_data_y)\n",
    "SklearnClassifier(svc_cv).train(train_data)\n",
    "print('Best hyperparameter setting: {0}.'.format(svc_cv.best_estimator_))\n",
    "print('Average accuracy across folds of best hyperparameter setting: {0}.'.format(svc_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbe88e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5994161458195213,\n",
       " 0.6057419199807762,\n",
       " 0.5959581834212833,\n",
       " 0.6057419199807762)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applied best parameter gained from GridSearchCV above: C = 0.007964824120603023, with optimal preprocessing method\n",
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "def train_classifier(data):\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.007964824120603023))])\n",
    "    return SklearnClassifier(pipeline).train(data)\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cd11d",
   "metadata": {},
   "source": [
    "**Balanced class_weight seems not to be a good choice**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa5afd",
   "metadata": {},
   "source": [
    "# Feature extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48647936",
   "metadata": {},
   "source": [
    "## Try adding controls on feature selection: minimum token frequency > 2, with most updated pre_process and optimal model hyperparameter C\n",
    "The reason is the features currently are presented by unigrams, which has a long tail distribution; hence, setting minimum token frequency could be a good idea to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1eefd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set min frequency \n",
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    feature_dict = {} # Created an empty dictionary (local feature dict) to store the vocab\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    for i in tokens: # Loop the input tokens. \n",
    "        #For each token, look up in the global dictionary to see if it exists\n",
    "        if i not in global_feature_dict: # If no, then assign value 1 for that token and add it to the global dictionary\n",
    "            global_feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the global dictionary\n",
    "            global_feature_dict[i] = float(global_feature_dict[i] + 1)\n",
    "        #For each token, look up in the feature dictionary to see if it exists (same approach as above)\n",
    "        if i not in feature_dict:  # If no, then assign value 1 for that token and add it to the feature dictionary\n",
    "            feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the feature dictionary\n",
    "            feature_dict[i] = float(feature_dict[i] + 1)\n",
    "    for a in feature_dict.copy():\n",
    "        if global_feature_dict[a] < 2:\n",
    "                del feature_dict[a]\n",
    "    return feature_dict # Return the feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "add53d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.601818669157328, 0.6078114862429412, 0.5980890168376389, 0.6078114862429412)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applied best parameter gained from GridSearchCV above\n",
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "def train_classifier(data):\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.007964824120603023))])\n",
    "    return SklearnClassifier(pipeline).train(data)\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd257ca",
   "metadata": {},
   "source": [
    "**Score is lifted, agree to add min frequency > 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100be4b3",
   "metadata": {},
   "source": [
    "## Try adding length of the sentence, with most updated preprocess/optimal model hyperparameter C/min.frequency >2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80fdefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    feature_dict = {} # Created an empty dictionary (local feature dict) to store the vocab\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    for i in tokens: # Loop the input tokens. \n",
    "        #For each token, look up in the global dictionary to see if it exists\n",
    "        if i not in global_feature_dict: # If no, then assign value 1 for that token and add it to the global dictionary\n",
    "            global_feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the global dictionary\n",
    "            global_feature_dict[i] = float(global_feature_dict[i] + 1)\n",
    "        #For each token, look up in the feature dictionary to see if it exists (same approach as above)\n",
    "        if i not in feature_dict:  # If no, then assign value 1 for that token and add it to the feature dictionary\n",
    "            feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the feature dictionary\n",
    "            feature_dict[i] = float(feature_dict[i] + 1)\n",
    "    for a in feature_dict.copy():\n",
    "        if global_feature_dict[a] < 2:\n",
    "                del feature_dict[a]\n",
    "    feature_dict['sent_len'] = float(len(tokens)) # Add sentence length as new feature\n",
    "    return feature_dict # Return the feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6395cc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5997576306801301, 0.6059870239096479, 0.596319947846663, 0.6059870239096479)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applied best parameter gained from GridSearchCV above\n",
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "def train_classifier(data):\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.007964824120603023, max_iter=100000))])\n",
    "    return SklearnClassifier(pipeline).train(data)\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3b01e",
   "metadata": {},
   "source": [
    "**Result is not lifted after add text length as a new feature, dont consider adding length of sentence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c696c",
   "metadata": {},
   "source": [
    "## Try adding count of non-text word, with most updated preprocess/optimal model hyperparameter C/min.frequency >2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "65f88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    feature_dict = {} # Created an empty dictionary (local feature dict) to store the vocab\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    nontext_count = 0\n",
    "    for i in tokens: # Loop the input tokens. \n",
    "        #For each token, look up in the global dictionary to see if it exists\n",
    "        if i not in global_feature_dict: # If no, then assign value 1 for that token and add it to the global dictionary\n",
    "            global_feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the global dictionary\n",
    "            global_feature_dict[i] = float(global_feature_dict[i] + 1)\n",
    "        #For each token, look up in the feature dictionary to see if it exists (same approach as above)\n",
    "        if i not in feature_dict:  # If no, then assign value 1 for that token and add it to the feature dictionary\n",
    "            feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the feature dictionary\n",
    "            feature_dict[i] = float(feature_dict[i] + 1)\n",
    "        if not i.isalpha():\n",
    "            nontext_count += 1\n",
    "    for a in feature_dict.copy():\n",
    "        if global_feature_dict[a] < 2:\n",
    "                del feature_dict[a]\n",
    "    feature_dict['nontext_count'] = nontext_count # Add non-text count as new feature\n",
    "    return feature_dict # Return the feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b333e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6020411143379807, 0.6080517842124233, 0.598107828827725, 0.6080517842124233)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applied best parameter gained from GridSearchCV above\n",
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "def train_classifier(data):\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.007964824120603023, max_iter=100000))])\n",
    "    return SklearnClassifier(pipeline).train(data)\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af90d7",
   "metadata": {},
   "source": [
    "**After adding non-text count, the model is better than previous a bit, so I decide to keep this feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80207244",
   "metadata": {},
   "source": [
    "## After testing around, below is what has been successfully tested and has helped to improve the score\n",
    "1. Tokenizing by the tradition str.split(), with separator = any whitespace\n",
    "2. Token normalization\n",
    "3. Adding minimum frequency for token: min = 2\n",
    "4. In model hyperparameter, setting a regularizing cost hyperparameter C = 0.007964824120603023 (optimal C after tunning with GridSearch CV)\n",
    "5. Adding non-text count as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc8314",
   "metadata": {},
   "source": [
    "# Final model with optimal preprocessing method after testing different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5849aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After folding throughout cross-val process, the average score of precision - recall - f1score - accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6020411143379807, 0.6080517842124233, 0.598107828827725, 0.6080517842124233)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applied best parameter gained from GridSearchCV above\n",
    "raw_data = []         \n",
    "train_data = []        \n",
    "test_data = []        \n",
    "\n",
    "\n",
    "def pre_process(text): # optimal preprocess: str.split() + token normalization\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    tokens_list = []\n",
    "    token = text.split()\n",
    "    for i in token:\n",
    "        i = i.lower()\n",
    "        tokens_list.append(i)\n",
    "    return tokens_list\n",
    "\n",
    "def train_classifier(data): # optimal version: set C ~ 0.008\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.007964824120603023, max_iter=100000))])\n",
    "    return SklearnClassifier(pipeline).train(data)\n",
    "\n",
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens): # optimal feature extract: set min token frequency and add non-text count as new feature\n",
    "    feature_dict = {} # Created an empty dictionary (local feature dict) to store the vocab\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    nontext_count = 0\n",
    "    for i in tokens: # Loop the input tokens. \n",
    "        #For each token, look up in the global dictionary to see if it exists\n",
    "        if i not in global_feature_dict: # If no, then assign value 1 for that token and add it to the global dictionary\n",
    "            global_feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the global dictionary\n",
    "            global_feature_dict[i] = float(global_feature_dict[i] + 1)\n",
    "        #For each token, look up in the feature dictionary to see if it exists (same approach as above)\n",
    "        if i not in feature_dict:  # If no, then assign value 1 for that token and add it to the feature dictionary\n",
    "            feature_dict[i] = 1\n",
    "        else: # If yes, then take the values (weights) to add 1 more (values + 1) and update it into the feature dictionary\n",
    "            feature_dict[i] = float(feature_dict[i] + 1)\n",
    "        if not i.isalpha():\n",
    "            nontext_count += 1\n",
    "    for a in feature_dict.copy():\n",
    "        if global_feature_dict[a] < 2:\n",
    "                del feature_dict[a]\n",
    "    feature_dict['nontext_count'] = nontext_count\n",
    "    return feature_dict # Return the feature dictionary\n",
    "\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c68d97",
   "metadata": {},
   "source": [
    "# Apply the model on test set to evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34c44683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'we': 1, 'have': 1, 'invested': 1, 'record': 1, 'funding': 1, 'in': 1, 'protecting': 1, 'our': 1, 'environment.': 1, 'nontext_count': 1}, 'FAKE')\n",
      "Done training!\n",
      "Precision: 0.601904\n",
      "Recall: 0.605469\n",
      "F Score:0.596231\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  #Â classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
